# Scripts de D√©ploiement - Scraper-Pro

Ce r√©pertoire contient tous les scripts n√©cessaires pour installer, configurer et maintenir Scraper-Pro en production.

## üìã Vue d'Ensemble des Scripts

| Script | Description | Dur√©e | Quand l'utiliser |
|--------|-------------|-------|------------------|
| `quick_install.sh` | Installation automatique compl√®te | 10-15 min | Premi√®re installation |
| `validate-installation.sh` | Validation de l'installation | 1 min | Apr√®s installation ou mise √† jour |
| `setup-nginx.sh` | Configuration Nginx + SSL | 5-10 min | Apr√®s quick_install.sh |
| **`monitor_job.sh`** | **Surveillance temps r√©el d'un job** | Continu | Pendant un scraping |
| **`monitor_job.py`** | **Surveillance temps r√©el (Python)** | Continu | Pendant un scraping |

---

## üìä monitor_job.sh / monitor_job.py

**Surveillance en temps r√©el des jobs de scraping**

### Fonctionnalit√©s

- ‚úÖ Affichage du status en temps r√©el
- ‚úÖ Barre de progression visuelle
- ‚úÖ Compteurs (pages, contacts, erreurs)
- ‚úÖ D√©tection automatique de fin
- ‚úÖ Lien vers logs en cas d'erreur
- ‚úÖ Support multi-plateforme

### Usage

**Version Bash (Linux/Mac/WSL) :**
```bash
# Surveiller un job (rafra√Æchir toutes les 10s)
./scripts/monitor_job.sh 123

# Intervalle personnalis√© (5 secondes)
./scripts/monitor_job.sh 123 5

# API distante
API_URL=http://prod-server:8000 ./scripts/monitor_job.sh 123
```

**Version Python (Multi-plateforme) :**
```bash
# Installer les d√©pendances (optionnelles pour affichage enrichi)
pip install requests rich

# Surveiller un job
python scripts/monitor_job.py 123

# Intervalle personnalis√©
python scripts/monitor_job.py 123 --interval 5

# API distante
python scripts/monitor_job.py 123 --api-url http://prod-server:8000
```

### Exemple de sortie (Bash avec jq)

```
[2026-02-13 14:30:45] Rafra√Æchissement #12
  Status      : ‚öôÔ∏è  RUNNING
  Progress    : [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 60.5%
  Type        : google_search
  Pages       : 61
  Contacts    : 23
  Errors      : 0

üèÅ Job termin√© avec status: completed

üìä R√©sum√© final:
  ‚Ä¢ Pages scrap√©es    : 100
  ‚Ä¢ Contacts extraits : 42
  ‚Ä¢ Erreurs           : 2
```

### Quand l'utiliser

- ‚úÖ Pendant un long scraping (suivre la progression)
- ‚úÖ Debugging (d√©tecter rapidement les erreurs)
- ‚úÖ D√©monstration client (montrer l'avancement)
- ‚úÖ Tests de performance

### Workflow avec monitoring

```bash
# 1. Cr√©er un job
JOB_ID=$(curl -s -X POST http://localhost:8000/api/v1/scraping/jobs/simple \
  -H "Content-Type: application/json" \
  -d '{
    "source_type": "custom_urls",
    "name": "Test",
    "config": {"urls": ["https://example.com"]}
  }' | jq -r '.job_id')

# 2. Surveiller en temps r√©el
./scripts/monitor_job.sh $JOB_ID

# OU en arri√®re-plan avec logs
nohup python scripts/monitor_job.py $JOB_ID > job_$JOB_ID.log 2>&1 &
```

---

## üöÄ quick_install.sh

**Installation automatique compl√®te de Scraper-Pro**

### Fonctionnalit√©s

- ‚úÖ Met √† jour le syst√®me Ubuntu/Debian
- ‚úÖ Installe Docker + Docker Compose
- ‚úÖ Configure le firewall UFW
- ‚úÖ Clone le repository (si besoin)
- ‚úÖ **Auto-g√©n√®re des secrets forts** (PostgreSQL, Redis, API, Dashboard, Grafana)
- ‚úÖ **Sauvegarde les secrets** dans `~/.scraper-pro-secrets-YYYYMMDD_HHMMSS.txt`
- ‚úÖ Build les images Docker
- ‚úÖ D√©marre tous les services
- ‚úÖ V√©rifie la sant√© de l'API

### Usage

```bash
# Installation compl√®te (recommand√©)
bash scripts/quick_install.sh

# Skiper l'installation Docker (si d√©j√† install√©)
bash scripts/quick_install.sh --skip-docker

# Nettoyage complet (DESTRUCTIF - supprime toutes les donn√©es)
bash scripts/quick_install.sh --cleanup
```

### Ce qui est auto-g√©n√©r√©

Le script g√©n√®re automatiquement :

- **POSTGRES_PASSWORD** : 32 caract√®res alphanum√©riques
- **REDIS_PASSWORD** : 32 caract√®res alphanum√©riques
- **API_HMAC_SECRET** : 64 caract√®res hexad√©cimaux
- **DASHBOARD_PASSWORD** : 16 caract√®res alphanum√©riques
- **GRAFANA_PASSWORD** : 16 caract√®res alphanum√©riques

Tous ces secrets sont sauvegard√©s dans :
- `~/.scraper-pro-secrets-YYYYMMDD_HHMMSS.txt` (fichier s√©curis√© avec permissions 600)
- `/opt/scraper-pro/.env` (fichier de configuration)

### Apr√®s l'installation

1. **Sauvegarder vos secrets** :
   ```bash
   # Copier le fichier vers un endroit s√ªr (password manager, etc.)
   cat ~/.scraper-pro-secrets-*.txt

   # Puis supprimer du serveur
   rm ~/.scraper-pro-secrets-*.txt
   ```

2. **Configurer les secrets optionnels** (MailWizz, Webhooks) :
   ```bash
   nano /opt/scraper-pro/.env
   # Modifier MAILWIZZ_*_API_KEY et WEBHOOK_*_SECRET
   ```

3. **Red√©marrer les services** :
   ```bash
   cd /opt/scraper-pro
   docker-compose -f docker-compose.production.yml restart
   ```

4. **Valider l'installation** :
   ```bash
   bash scripts/validate-installation.sh
   ```

---

## ‚úÖ validate-installation.sh

**Validation automatique de tous les composants**

### Fonctionnalit√©s

- ‚úÖ V√©rifie que Docker fonctionne
- ‚úÖ V√©rifie que tous les containers sont "Up"
- ‚úÖ Teste les health checks (API, PostgreSQL, Redis)
- ‚úÖ V√©rifie l'accessibilit√© (Dashboard, Grafana, Prometheus)
- ‚úÖ V√©rifie le sch√©ma de la base de donn√©es
- ‚úÖ Teste la connectivit√© r√©seau inter-containers
- ‚úÖ V√©rifie l'espace disque et la m√©moire
- ‚úÖ V√©rifie les permissions du fichier `.env`
- ‚úÖ D√©tecte les mots de passe par d√©faut (CHANGE_ME)

### Usage

```bash
# Validation standard
bash scripts/validate-installation.sh

# Validation avec sortie d√©taill√©e (verbose)
bash scripts/validate-installation.sh --verbose
bash scripts/validate-installation.sh -v
```

### Sortie

**Si tout est OK** :
```
=========================================
  Scraper-Pro Installation Validator
=========================================

=== Docker Daemon ===
Checking Docker Daemon Running... ‚úÖ PASS

=== Docker Containers ===
Checking PostgreSQL Container... ‚úÖ PASS
Checking Redis Container... ‚úÖ PASS
Checking API Container... ‚úÖ PASS
...

=========================================
  Validation Summary
=========================================
Total Checks: 20
Passed: 20

‚úÖ All checks passed! Installation is healthy.
```

**Si des probl√®mes sont d√©tect√©s** :
```
Checking Redis Ping... ‚ùå FAIL

=========================================
  Validation Summary
=========================================
Total Checks: 20
Passed: 18
Failed: 2

‚ö†Ô∏è Some checks failed. Review the output above.

Troubleshooting tips:
  1. Check Docker logs:
     docker-compose -f docker-compose.production.yml logs
  ...
```

### Quand l'utiliser

- ‚úÖ Apr√®s l'installation initiale
- ‚úÖ Apr√®s une mise √† jour du code
- ‚úÖ Apr√®s un red√©marrage du serveur
- ‚úÖ Pour diagnostiquer un probl√®me
- ‚úÖ Dans un script de monitoring (cron)

### Automatisation (Monitoring)

Vous pouvez automatiser ce script pour surveiller votre installation :

```bash
# Ajouter un cron job (toutes les heures)
crontab -e

# Ajouter cette ligne :
0 * * * * /opt/scraper-pro/scripts/validate-installation.sh >> /var/log/scraper-health.log 2>&1
```

---

## üåê setup-nginx.sh

**Configuration automatique de Nginx + SSL (Let's Encrypt)**

### Fonctionnalit√©s

- ‚úÖ Installe Nginx et Certbot
- ‚úÖ V√©rifie la configuration DNS
- ‚úÖ Cr√©e 3 configurations Nginx :
  - **dashboard.$DOMAIN** ‚Üí Streamlit (port 8501)
  - **api.$DOMAIN** ‚Üí FastAPI (port 8000)
  - **grafana.$DOMAIN** ‚Üí Grafana (port 3000)
- ‚úÖ Active WebSocket pour Streamlit et Grafana
- ‚úÖ Installe les certificats SSL Let's Encrypt
- ‚úÖ Configure le renouvellement automatique SSL
- ‚úÖ Configure le firewall UFW

### Usage

```bash
# Avec votre domaine (email auto-g√©n√©r√©)
bash scripts/setup-nginx.sh yourdomain.com

# Avec email personnalis√©
bash scripts/setup-nginx.sh yourdomain.com admin@yourdomain.com
```

### Pr√©requis DNS

**AVANT** de lancer le script, configurez 3 enregistrements DNS de type A :

| Nom | Type | Valeur |
|-----|------|--------|
| dashboard.yourdomain.com | A | Votre IP serveur |
| api.yourdomain.com | A | Votre IP serveur |
| grafana.yourdomain.com | A | Votre IP serveur |

**V√©rifier la propagation DNS** :
```bash
# Depuis votre ordinateur local
dig dashboard.yourdomain.com +short
dig api.yourdomain.com +short
dig grafana.yourdomain.com +short

# R√©sultat attendu : votre IP serveur
```

### Exemple de sortie

```
=========================================
  Nginx + SSL Setup for Scraper-Pro
=========================================

Domain: example.com
Email: admin@example.com
Subdomains:
  - dashboard.example.com
  - api.example.com
  - grafana.example.com

[INFO] Step 1/7: Installing Nginx and Certbot...
[SUCCESS] Nginx installed
[SUCCESS] Certbot installed

[INFO] Step 2/7: Checking DNS configuration...
...

=========================================
  ‚úÖ Setup Complete!
=========================================

Access your services:

  üìä Dashboard (Streamlit):
     https://dashboard.example.com

  üîå API (FastAPI):
     https://api.example.com
     https://api.example.com/docs

  üìà Grafana (Monitoring):
     https://grafana.example.com
```

### Certificats SSL

- ‚úÖ Certificats Let's Encrypt (gratuits, reconnus par tous les navigateurs)
- ‚úÖ Renouvellement automatique tous les 60 jours (cron job)
- ‚úÖ HTTP ‚Üí HTTPS redirect automatique
- ‚úÖ Note de s√©curit√© A+ (SSL Labs)

### Commandes de maintenance

```bash
# V√©rifier le statut des certificats
sudo certbot certificates

# Renouveler manuellement
sudo certbot renew

# Tester le renouvellement
sudo certbot renew --dry-run

# Tester la configuration Nginx
sudo nginx -t

# Recharger Nginx
sudo systemctl reload nginx

# Voir les logs Nginx
sudo tail -f /var/log/nginx/scraper-*-error.log
```

---

## üîß Workflow Recommand√©

### Installation Initiale (Serveur Vierge)

```bash
# 1. Se connecter au serveur
ssh root@VOTRE_IP

# 2. Cloner le repository
git clone https://github.com/VOTRE_ORG/scraper-pro.git /opt/scraper-pro
cd /opt/scraper-pro

# 3. Lancer l'installation automatique
bash scripts/quick_install.sh

# 4. Sauvegarder les secrets g√©n√©r√©s
cat ~/.scraper-pro-secrets-*.txt
# Copier dans un password manager, puis :
rm ~/.scraper-pro-secrets-*.txt

# 5. Valider l'installation
bash scripts/validate-installation.sh

# 6. Configurer Nginx + SSL (si vous avez un domaine)
bash scripts/setup-nginx.sh yourdomain.com admin@yourdomain.com

# 7. Tester l'acc√®s
# Ouvrir https://dashboard.yourdomain.com dans votre navigateur
```

### Mise √† Jour du Code

```bash
# 1. Se connecter
ssh root@VOTRE_IP
cd /opt/scraper-pro

# 2. Sauvegarder .env
cp .env .env.backup

# 3. Pull les changements
git pull origin main

# 4. Rebuild et red√©marrer
docker-compose -f docker-compose.production.yml build
docker-compose -f docker-compose.production.yml up -d

# 5. Valider
bash scripts/validate-installation.sh
```

### Diagnostic de Probl√®me

```bash
# 1. Valider l'installation
bash scripts/validate-installation.sh --verbose

# 2. Voir les logs
docker-compose -f docker-compose.production.yml logs -f

# 3. V√©rifier un service sp√©cifique
docker logs scraper-app --tail 100
docker logs scraper-postgres --tail 100

# 4. Red√©marrer un service
docker restart scraper-app
```

---

## üìö Documentation Compl√®te

Pour plus d'informations, consultez :

- **DEPLOYMENT_PRODUCTION.md** : Guide de d√©ploiement complet (√©tape par √©tape)
- **QUICK_START.md** : Guide de d√©marrage rapide
- **docs/ARCHITECTURE.md** : Architecture technique
- **docs/API.md** : Documentation de l'API REST
- **docs/DEDUPLICATION_SYSTEM.md** : Syst√®me de d√©duplication

---

## üõ°Ô∏è S√©curit√©

### Bonnes pratiques

1. ‚úÖ **Sauvegarder les secrets** dans un password manager (1Password, LastPass, Bitwarden)
2. ‚úÖ **Supprimer les fichiers de secrets** du serveur apr√®s sauvegarde
3. ‚úÖ **Permissions strictes** sur `.env` (chmod 600)
4. ‚úÖ **Firewall activ√©** (UFW)
5. ‚úÖ **SSL/TLS** sur tous les endpoints
6. ‚úÖ **Backups automatiques** de PostgreSQL (cron)
7. ‚úÖ **Monitoring actif** avec Grafana + Prometheus

### Checklist de s√©curit√©

```bash
# V√©rifier les permissions .env
ls -la /opt/scraper-pro/.env
# Attendu : -rw------- (600)

# V√©rifier le firewall
sudo ufw status
# Attendu : 22, 80, 443 autoris√©s

# V√©rifier les certificats SSL
sudo certbot certificates
# Attendu : Expiry date dans 60-90 jours

# V√©rifier les mots de passe par d√©faut
grep "CHANGE_ME" /opt/scraper-pro/.env
# Attendu : aucune correspondance

# V√©rifier les backups
ls -lh /home/scraper/backups/
# Attendu : backups r√©cents (< 24h)
```

---

## üÜò Support

En cas de probl√®me :

1. Consulter **DEPLOYMENT_PRODUCTION.md** section "Troubleshooting"
2. V√©rifier les logs : `docker-compose logs -f`
3. Valider l'installation : `bash scripts/validate-installation.sh --verbose`
4. Ouvrir une issue sur GitHub

---

**Scraper-Pro v2.0.0 - Scripts de D√©ploiement**
Made with ‚ù§Ô∏è by the SOS-Expat Tech Team
