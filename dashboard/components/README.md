# Dashboard Components - Scraper-Pro

## ğŸ“¦ Vue d'ensemble

Ce package contient des composants rÃ©utilisables pour le dashboard Scraper-Pro, conÃ§us pour Ãªtre modulaires, performants et faciles Ã  intÃ©grer.

## ğŸ¯ Composants disponibles

### 1. Article Filters (`article_filters.py`)

Composant ultra-premium de filtres dynamiques pour explorer les articles scrapÃ©s.

#### âœ¨ FonctionnalitÃ©s

- **Filtres auto-populate** : Chargement automatique des valeurs uniques depuis la DB
- **Multi-critÃ¨res** : Langue, pays, rÃ©gion, catÃ©gorie, ville, domaine, dates, recherche textuelle
- **Statistiques visuelles** : Graphiques Plotly interactifs (pie charts, bar charts)
- **Export CSV** : Compatible Excel avec UTF-8 BOM
- **Performance** : Cache Streamlit (5min pour filtres, 1min pour comptage)
- **Pagination** : Navigation fluide avec pages personnalisables
- **UX Premium** : Loading states, error handling, formatage des nombres

#### ğŸ“– Usage de base

```python
from dashboard.components import render_article_filters, get_filtered_articles
from sqlalchemy import create_engine

# 1. CrÃ©er un engine SQLAlchemy
engine = create_engine("postgresql://user:pass@localhost:5432/scraper_db")

# 2. Afficher les filtres (retourne un dict de valeurs)
filters = render_article_filters(engine)

# 3. RÃ©cupÃ©rer les articles filtrÃ©s
articles_df = get_filtered_articles(engine, filters, limit=50, offset=0)

# 4. Utiliser le DataFrame
st.dataframe(articles_df)
```

#### ğŸš€ Usage avancÃ© (Dashboard complet)

```python
from dashboard.components import render_full_articles_dashboard
from sqlalchemy import create_engine

engine = create_engine("postgresql://user:pass@localhost:5432/scraper_db")

# Affiche tout: filtres + stats + tableau + export
render_full_articles_dashboard(engine)
```

#### ğŸ“Š Filtres retournÃ©s

La fonction `render_article_filters()` retourne un dictionnaire:

```python
{
    "language": "fr" | "all",
    "country": "france" | "all",
    "region": "europe" | "all",
    "category": "guide" | "all",
    "city": "paris" | "all",
    "domain": "example.com" | "all",
    "date_from": date(2024, 1, 1) | None,
    "date_to": date(2024, 12, 31) | None,
    "search": "mot-clÃ©" | "",
    "sort_by": "date_published DESC" | "word_count DESC" | ...,
}
```

#### ğŸ¨ Fonctions disponibles

##### 1. `render_article_filters(engine: Engine) -> Dict[str, Any]`

Affiche l'interface de filtres et retourne les valeurs sÃ©lectionnÃ©es.

**ParamÃ¨tres:**
- `engine`: SQLAlchemy Engine connectÃ© Ã  la DB

**Retour:**
- Dictionnaire de filtres

**Exemple:**
```python
filters = render_article_filters(engine)
# User interacts with UI, filters are updated
```

---

##### 2. `get_filtered_articles(engine, filters, limit=50, offset=0) -> pd.DataFrame`

RÃ©cupÃ¨re les articles filtrÃ©s sous forme de DataFrame Pandas.

**ParamÃ¨tres:**
- `engine`: SQLAlchemy Engine
- `filters`: Dict retournÃ© par `render_article_filters()`
- `limit`: Nombre max d'articles (dÃ©faut: 50)
- `offset`: Offset pour pagination (dÃ©faut: 0)

**Retour:**
- DataFrame Pandas avec colonnes:
  - `id`, `title`, `url`, `domain`, `language`, `country`, `region`, `city`
  - `category_expat`, `word_count`, `author`, `date_published`, `scraped_at`
  - `excerpt`, `tags`, `categories`

**Exemple:**
```python
# Page 1 (0-49)
articles = get_filtered_articles(engine, filters, limit=50, offset=0)

# Page 2 (50-99)
articles = get_filtered_articles(engine, filters, limit=50, offset=50)
```

---

##### 3. `render_article_stats(engine: Engine, filters: Dict[str, Any])`

Affiche des statistiques visuelles sur les articles filtrÃ©s.

**FonctionnalitÃ©s:**
- Cartes mÃ©triques : Total, langues uniques, pays uniques, mots moyens
- Graphiques Plotly : Distribution par langue (pie), distribution par pays (bar)

**Exemple:**
```python
filters = render_article_filters(engine)
render_article_stats(engine, filters)
```

---

##### 4. `export_filtered_articles(engine: Engine, filters: Dict[str, Any])`

Affiche un bouton d'export CSV avec support Excel (UTF-8 BOM).

**FonctionnalitÃ©s:**
- Export jusqu'Ã  100,000 articles
- UTF-8 avec BOM pour compatibilitÃ© Excel
- Nom de fichier avec timestamp
- Formatage des dates lisibles

**Exemple:**
```python
filters = render_article_filters(engine)
export_filtered_articles(engine, filters)
# User clicks button â†’ CSV download
```

---

##### 5. `render_full_articles_dashboard(engine: Engine)`

Dashboard complet clÃ©-en-main combinant tous les composants.

**Inclut:**
1. Filtres dynamiques
2. Statistiques visuelles
3. Tableau paginÃ© des rÃ©sultats
4. Export CSV

**Exemple:**
```python
render_full_articles_dashboard(engine)
# Everything is rendered automatically
```

---

#### ğŸ—„ï¸ SchÃ©ma de la table `scraped_articles`

Le composant s'attend Ã  une table PostgreSQL avec cette structure:

```sql
CREATE TABLE scraped_articles (
    id SERIAL PRIMARY KEY,
    job_id INTEGER,
    url TEXT UNIQUE NOT NULL,
    title TEXT,
    content_text TEXT,
    content_html TEXT,
    excerpt TEXT,
    author TEXT,
    date_published TIMESTAMPTZ,
    categories TEXT[],
    tags TEXT[],
    external_links JSONB,
    internal_links JSONB,
    featured_image_url TEXT,
    meta_description TEXT,
    word_count INTEGER,
    language VARCHAR(10),
    domain TEXT,
    country VARCHAR(100),      -- Migration 005
    region VARCHAR(100),       -- Migration 005
    city VARCHAR(100),         -- Migration 005
    category_expat VARCHAR(100), -- Migration 005
    scraped_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index recommandÃ©s
CREATE INDEX idx_articles_language ON scraped_articles(language);
CREATE INDEX idx_articles_country ON scraped_articles(country);
CREATE INDEX idx_articles_region ON scraped_articles(region);
CREATE INDEX idx_articles_category_expat ON scraped_articles(category_expat);
CREATE INDEX idx_articles_date_published ON scraped_articles(date_published DESC);
```

#### âš™ï¸ Performance & Optimisation

**Cache Streamlit:**
- Valeurs uniques (filtres) : **5 minutes**
- Comptage articles : **1 minute**
- RequÃªtes optimisÃ©es avec index DB

**Recommandations:**
- CrÃ©er des index sur `language`, `country`, `region`, `category_expat`, `date_published`
- Limiter les rÃ©sultats Ã  100-200 par page max
- Utiliser la pagination pour grands volumes

**Queries SQL:**
- Utilise des paramÃ¨tres bindÃ©s (protection SQL injection)
- Pagination avec `LIMIT` et `OFFSET`
- Tri personnalisable (date, mots, titre, pays)

#### ğŸ¨ DÃ©pendances

**Required:**
- `streamlit >= 1.30.0`
- `sqlalchemy >= 2.0.0`
- `pandas >= 2.1.0`
- `psycopg2-binary >= 2.9.9`

**Optional (pour graphiques):**
- `plotly >= 5.18.0` (recommandÃ© pour statistiques visuelles)

Si Plotly n'est pas installÃ©, les statistiques fonctionnent sans graphiques.

#### ğŸ“ Exemples d'intÃ©gration

##### Exemple 1 : IntÃ©gration dans un onglet Streamlit

```python
import streamlit as st
from dashboard.components import render_full_articles_dashboard
from sqlalchemy import create_engine

# Dans votre dashboard principal
tab1, tab2, tab3 = st.tabs(["Jobs", "Contacts", "Articles"])

with tab3:
    st.header("ğŸ“° Articles ScrapÃ©s")
    engine = create_engine(get_db_url())
    render_full_articles_dashboard(engine)
```

##### Exemple 2 : Filtres sÃ©parÃ©s des rÃ©sultats

```python
import streamlit as st
from dashboard.components import (
    render_article_filters,
    get_filtered_articles,
    export_filtered_articles
)
from sqlalchemy import create_engine

engine = create_engine(get_db_url())

# Section filtres
with st.expander("ğŸ” Filtres", expanded=True):
    filters = render_article_filters(engine)

# Section rÃ©sultats
st.subheader("ğŸ“‹ RÃ©sultats")
articles = get_filtered_articles(engine, filters, limit=50)
st.dataframe(articles)

# Section export
export_filtered_articles(engine, filters)
```

##### Exemple 3 : Customisation avancÃ©e

```python
from dashboard.components import render_article_filters, get_filtered_articles

engine = create_engine(get_db_url())

# Filtres
filters = render_article_filters(engine)

# Custom logic
if filters["language"] == "fr" and filters["country"] == "france":
    st.info("ğŸ‡«ğŸ‡· Articles franÃ§ais de France")

# RÃ©cupÃ©rer articles
articles = get_filtered_articles(engine, filters, limit=100)

# Custom display
for _, article in articles.iterrows():
    with st.expander(f"ğŸ“„ {article['title']}"):
        st.write(f"**Domaine:** {article['domain']}")
        st.write(f"**Mots:** {article['word_count']:,}")
        st.write(f"**URL:** {article['url']}")
```

#### ğŸ› Troubleshooting

**ProblÃ¨me: "Aucun article trouvÃ©"**
- VÃ©rifier que la table `scraped_articles` contient des donnÃ©es
- VÃ©rifier les migrations (migration 005 pour champs Expat.com)
- Assouplir les filtres (mettre "Toutes" partout)

**ProblÃ¨me: "Erreur de connexion DB"**
- VÃ©rifier les variables d'env `POSTGRES_HOST`, `POSTGRES_USER`, etc.
- Tester avec `psql` en ligne de commande

**ProblÃ¨me: "Graphiques non affichÃ©s"**
- Installer Plotly : `pip install plotly`
- Le composant fonctionne sans Plotly (stats sans graphiques)

**ProblÃ¨me: "Export CSV vide dans Excel"**
- Le BOM UTF-8 (`\ufeff`) est inclus pour Excel
- Si problÃ¨me persiste, ouvrir avec "UTF-8" explicite dans Excel

#### ğŸ§ª Tests

ExÃ©cuter les tests unitaires:

```bash
cd scraper-pro
python dashboard/test_article_filters.py
```

Les tests vÃ©rifient:
- âœ… Connexion Ã  la base de donnÃ©es
- âœ… RÃ©cupÃ©ration des valeurs uniques
- âœ… Construction des requÃªtes SQL
- âœ… Comptage des articles

#### ğŸ“„ Licence

Â© 2025 Scraper-Pro. Usage interne uniquement.

---

## ğŸš€ Roadmap

**Prochains composants:**

- [ ] `contact_filters.py` - Filtres pour contacts validÃ©s
- [ ] `job_monitor.py` - Monitoring temps-rÃ©el des jobs
- [ ] `proxy_dashboard.py` - Dashboard santÃ© des proxies
- [ ] `mailwizz_sync.py` - Stats sync MailWizz

---

## ğŸ’¬ Support

Pour toute question ou suggestion sur les composants:
- Consulter la doc principale : `scraper-pro/README.md`
- Consulter le guide dashboard : `dashboard/README_FINAL.md`
