# üöÄ Guide de D√©ploiement SCRAPER-PRO

## üìã Table des Mati√®res

1. [Introduction](#introduction)
2. [Pr√©-requis](#pr√©-requis)
3. [MODE 1 : Scraping Massif (avec proxies)](#mode-1-scraping-massif-avec-proxies)
4. [MODE 2 : Scraping Simple (sans proxies)](#mode-2-scraping-simple-sans-proxies)
5. [Configuration](#configuration)
6. [Lancement](#lancement)
7. [V√©rifications](#v√©rifications)
8. [Backup & Monitoring](#backup--monitoring)
9. [Troubleshooting](#troubleshooting)

---

## üéØ Introduction

SCRAPER-PRO offre **DEUX modes de d√©ploiement** selon vos besoins :

### üîµ MODE 1 : SCRAPING MASSIF (Recommand√©)
- ‚úÖ **Avec proxies** rotatifs (r√©sidentiels + datacenter)
- ‚úÖ **9 sources** : Google Search, Google Maps, LinkedIn, Facebook, Instagram, YouTube, Forums, URLs custom
- ‚úÖ **10K-20K contacts/mois**
- ‚úÖ **Anti-d√©tection** avanc√©e
- üí∞ **Budget** : ~280‚Ç¨/mois
- üéØ **Pour** : Scraping professionnel, multi-sources, volumes √©lev√©s

### üü¢ MODE 2 : SCRAPING SIMPLE (√âconomique)
- ‚úÖ **Sans proxies** (IP fixe VPS)
- ‚úÖ **1 source** : URLs personnalis√©es uniquement (generic_url_spider)
- ‚úÖ **2K-5K contacts/mois**
- ‚úÖ **Configuration** minimale
- üí∞ **Budget** : ~80‚Ç¨/mois
- üéØ **Pour** : Scraping ponctuel, sites simples, budget limit√©

---

## üõ†Ô∏è Pr√©-requis

### Serveur VPS

**Configuration minimale** :
- **CPU** : 4 vCPU
- **RAM** : 8 GB
- **Disque** : 50 GB SSD
- **OS** : Ubuntu 22.04 LTS (recommand√©) ou Debian 11+
- **R√©seau** : IP publique fixe

**Providers recommand√©s** :
- ü•á **Hetzner** (30‚Ç¨/mois) - CPX31
- ü•à **DigitalOcean** (48$/mois) - 8GB Droplet
- ü•â **OVH** (35‚Ç¨/mois) - VPS Elite

### Logiciels requis

```bash
# Installer Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Installer Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# V√©rifier installation
docker --version
docker-compose --version
```

### Cl√©s API requises

**OBLIGATOIRES** (les deux modes) :
- ‚úÖ **MailWizz SOS-Expat** API Key
- ‚úÖ **MailWizz Ulixai** API Key

**MODE 1 UNIQUEMENT** :
- ‚úÖ **Oxylabs** (ou SmartProxy, BrightData) credentials
- ‚úÖ **SerpAPI** key (pour Google Search)
- üìå **2Captcha** key (optionnel, pour LinkedIn/Facebook)

---

## üîµ MODE 1 : Scraping Massif (avec proxies)

### √âtape 1 : Connexion VPS

```bash
# Connexion SSH
ssh root@VOTRE_IP_VPS

# Cr√©er utilisateur scraper
adduser scraper
usermod -aG sudo,docker scraper

# Passer en utilisateur scraper
su - scraper
```

### √âtape 2 : Clone du projet

```bash
# Cr√©er dossier projets
mkdir -p ~/projets
cd ~/projets

# Clone repository (ajustez l'URL selon votre repo)
git clone https://github.com/votre-username/scraper-pro.git
cd scraper-pro
```

### √âtape 3 : Configuration MODE 1

```bash
# Copier template MODE 1
cp .env.mode1.example .env

# √âditer .env
nano .env
```

**Variables CRITIQUES √† remplir** :

```bash
# ====== S√âCURIT√â ======
POSTGRES_PASSWORD=VotreMotDePasseSecurit√©Postgres123!
REDIS_PASSWORD=VotreMotDePasseRedis456!
API_HMAC_SECRET=$(openssl rand -hex 32)
DASHBOARD_PASSWORD=VotreMotDePasseAdmin789!

# ====== PROXIES ======
# Oxylabs
OXYLABS_USER=votre_username_oxylabs
OXYLABS_PASS=votre_password_oxylabs

# SmartProxy
SMARTPROXY_USER=votre_username_smartproxy
SMARTPROXY_PASS=votre_password_smartproxy

# ====== MAILWIZZ ======
MAILWIZZ_SOS_EXPAT_API_KEY=votre_cle_api_sos_expat_ici
MAILWIZZ_ULIXAI_API_KEY=votre_cle_api_ulixai_ici

# ====== GOOGLE SEARCH ======
SERPAPI_KEY=votre_cle_serpapi
```

### √âtape 4 : Lancement MODE 1

```bash
# Build containers
docker-compose build

# Lancer services
docker-compose up -d

# V√©rifier logs
docker-compose logs -f scraper
```

### √âtape 5 : Acc√®s dashboard

```
URL : http://VOTRE_IP_VPS:8501
Password : (celui d√©fini dans DASHBOARD_PASSWORD)
```

---

## üü¢ MODE 2 : Scraping Simple (sans proxies)

### √âtape 1 : Connexion VPS

```bash
# Connexion SSH (m√™me que MODE 1)
ssh root@VOTRE_IP_VPS

# Cr√©er utilisateur scraper
adduser scraper
usermod -aG sudo,docker scraper

# Passer en utilisateur scraper
su - scraper
```

### √âtape 2 : Clone du projet

```bash
# Cr√©er dossier projets
mkdir -p ~/projets
cd ~/projets

# Clone repository
git clone https://github.com/votre-username/scraper-pro.git
cd scraper-pro
```

### √âtape 3 : Configuration MODE 2

```bash
# Copier template MODE 2
cp .env.mode2.example .env

# √âditer .env
nano .env
```

**Variables CRITIQUES √† remplir** :

```bash
# ====== MODE ======
SCRAPER_MODE=simple
ENABLE_PROXIES=false

# ====== S√âCURIT√â ======
POSTGRES_PASSWORD=VotreMotDePasseSecurit√©Postgres123!
REDIS_PASSWORD=VotreMotDePasseRedis456!
API_HMAC_SECRET=$(openssl rand -hex 32)
DASHBOARD_PASSWORD=VotreMotDePasseAdmin789!

# ====== MAILWIZZ ======
MAILWIZZ_SOS_EXPAT_API_KEY=votre_cle_api_sos_expat_ici
MAILWIZZ_ULIXAI_API_KEY=votre_cle_api_ulixai_ici
```

**IMPORTANT** : Pas besoin de remplir les proxies, SerpAPI, ni 2Captcha en MODE 2.

### √âtape 4 : Lancement MODE 2

```bash
# Utiliser docker-compose MODE simple
docker-compose -f docker-compose-mode-simple.yml build

# Lancer services
docker-compose -f docker-compose-mode-simple.yml up -d

# V√©rifier logs
docker-compose -f docker-compose-mode-simple.yml logs -f scraper
```

### √âtape 5 : Acc√®s dashboard

```
URL : http://VOTRE_IP_VPS:8501
Password : (celui d√©fini dans DASHBOARD_PASSWORD)
```

---

## ‚öôÔ∏è Configuration

### Listes MailWizz (√† cr√©er avant)

#### SOS-Expat (10 listes)

| ID | Nom Liste | Cat√©gorie |
|----|-----------|-----------|
| 1 | Avocats Internationaux | avocat |
| 2 | Assureurs Expatri√©s | assureur |
| 3 | Notaires Internationaux | notaire |
| 4 | M√©decins Francophones | medecin |
| 5 | Comptables et Fiscalistes | comptable |
| 6 | Traducteurs et Interpr√®tes | traducteur |
| 7 | Agents Immobiliers | agent_immo |
| 8 | D√©m√©nageurs Internationaux | demenageur |
| 9 | Banquiers et Conseillers | banquier |
| 11 | Consultants Expatriation | consultant |
| 10 | Contacts Divers | default |

#### Ulixai (4 listes)

| ID | Nom Liste | Cat√©gorie |
|----|-----------|-----------|
| 1 | Blogueurs Voyage | blogueur |
| 2 | Influenceurs R√©seaux Sociaux | influenceur |
| 3 | Admins Groupes Facebook | admin_groupe |
| 4 | YouTubeurs Voyage | youtubeur |
| 10 | Contacts Divers | default |

**Ajuster les IDs dans** : `config/mailwizz_routing.json`

### Firewall

```bash
# UFW (Ubuntu Firewall)
sudo ufw allow 22/tcp    # SSH
sudo ufw allow 8000/tcp  # API
sudo ufw allow 8501/tcp  # Dashboard
sudo ufw enable
```

### Reverse Proxy (Optionnel mais recommand√©)

**Nginx pour HTTPS** :

```bash
sudo apt install nginx certbot python3-certbot-nginx

# Cr√©er config Nginx
sudo nano /etc/nginx/sites-available/scraper-pro
```

```nginx
server {
    listen 80;
    server_name scraper.votredomaine.com;

    location / {
        proxy_pass http://localhost:8501;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }

    location /api {
        proxy_pass http://localhost:8000;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

```bash
# Activer site
sudo ln -s /etc/nginx/sites-available/scraper-pro /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx

# Obtenir certificat SSL
sudo certbot --nginx -d scraper.votredomaine.com
```

---

## üöÄ Lancement

### Commandes Docker Compose

**MODE 1** :
```bash
# D√©marrer
docker-compose up -d

# Arr√™ter
docker-compose down

# Red√©marrer
docker-compose restart

# Rebuild apr√®s changement code
docker-compose up -d --build

# Logs
docker-compose logs -f scraper
docker-compose logs -f dashboard
```

**MODE 2** :
```bash
# D√©marrer
docker-compose -f docker-compose-mode-simple.yml up -d

# Arr√™ter
docker-compose -f docker-compose-mode-simple.yml down

# Logs
docker-compose -f docker-compose-mode-simple.yml logs -f
```

### D√©marrage automatique au boot

```bash
# Cr√©er service systemd
sudo nano /etc/systemd/system/scraper-pro.service
```

**MODE 1** :
```ini
[Unit]
Description=Scraper-Pro MODE 1
Requires=docker.service
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/scraper/projets/scraper-pro
ExecStart=/usr/local/bin/docker-compose up -d
ExecStop=/usr/local/bin/docker-compose down
User=scraper

[Install]
WantedBy=multi-user.target
```

**MODE 2** :
```ini
[Unit]
Description=Scraper-Pro MODE 2
Requires=docker.service
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/scraper/projets/scraper-pro
ExecStart=/usr/local/bin/docker-compose -f docker-compose-mode-simple.yml up -d
ExecStop=/usr/local/bin/docker-compose -f docker-compose-mode-simple.yml down
User=scraper

[Install]
WantedBy=multi-user.target
```

```bash
# Activer service
sudo systemctl enable scraper-pro
sudo systemctl start scraper-pro

# V√©rifier status
sudo systemctl status scraper-pro
```

---

## ‚úÖ V√©rifications

### 1. Health checks

```bash
# PostgreSQL
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT 1"

# Redis
docker exec scraper_redis redis-cli -a VotreMotDePasseRedis ping

# API
curl http://localhost:8000/health

# Dashboard
curl http://localhost:8501
```

### 2. Test scraping job

1. **Acc√©der dashboard** : `http://VOTRE_IP:8501`
2. **Cr√©er job test** : Onglet "üìù Cr√©er Job"
   - Source : URLs personnalis√©es
   - URLs : 5-10 sites test
   - Cat√©gorie : avocat
   - Platform : SOS-Expat
3. **Lancer** et surveiller progression
4. **V√©rifier** : Onglet "üìä Jobs Actifs"

### 3. V√©rifier pipeline complet

```bash
# 1. Contacts scrap√©s
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT COUNT(*) FROM scraped_contacts WHERE status='pending_validation'"

# 2. Contacts valid√©s
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT COUNT(*) FROM validated_contacts WHERE status='ready_for_mailwizz'"

# 3. Contacts envoy√©s MailWizz
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT COUNT(*) FROM validated_contacts WHERE status='sent_to_mailwizz'"

# 4. Logs sync MailWizz
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT status, COUNT(*) FROM mailwizz_sync_log GROUP BY status"
```

---

## üíæ Backup & Monitoring

### Backup PostgreSQL automatique

```bash
# Cr√©er script backup
mkdir -p ~/backups
nano ~/backups/backup-postgres.sh
```

```bash
#!/bin/bash
BACKUP_DIR="/home/scraper/backups/postgres"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
CONTAINER="scraper_postgres"
DB_NAME="scraper_db"
DB_USER="scraper_admin"

mkdir -p $BACKUP_DIR

# Backup
docker exec $CONTAINER pg_dump -U $DB_USER $DB_NAME | gzip > "$BACKUP_DIR/scraper_$TIMESTAMP.sql.gz"

# Garder seulement 7 derniers jours
find $BACKUP_DIR -name "scraper_*.sql.gz" -mtime +7 -delete

echo "Backup completed: scraper_$TIMESTAMP.sql.gz"
```

```bash
chmod +x ~/backups/backup-postgres.sh

# Cron job daily backup
crontab -e
```

Ajouter :
```
0 3 * * * /home/scraper/backups/backup-postgres.sh >> /home/scraper/backups/backup.log 2>&1
```

### Monitoring (optionnel)

**Prometheus + Grafana** :

```bash
# Ajouter au docker-compose.yml
prometheus:
  image: prom/prometheus:latest
  volumes:
    - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
  ports:
    - "9090:9090"

grafana:
  image: grafana/grafana:latest
  ports:
    - "3000:3000"
```

---

## üîß Troubleshooting

### Probl√®me : Container scraper ne d√©marre pas

```bash
# V√©rifier logs
docker-compose logs scraper

# Probl√®mes courants :
# 1. PostgreSQL pas pr√™t ‚Üí Attendre 30sec et retry
# 2. .env mal configur√© ‚Üí V√©rifier variables
# 3. Port 8000 d√©j√† utilis√© ‚Üí Changer port dans docker-compose.yml
```

### Probl√®me : Dashboard ne charge pas

```bash
# V√©rifier dashboard logs
docker-compose logs dashboard

# V√©rifier API accessible
curl http://localhost:8000/health

# Red√©marrer dashboard
docker-compose restart dashboard
```

### Probl√®me : Proxies bloqu√©s (MODE 1)

```bash
# V√©rifier stats proxies
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT status, COUNT(*) FROM proxy_stats GROUP BY status"

# R√©initialiser proxy en cooldown
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "UPDATE proxy_stats SET status='ACTIVE', consecutive_failures=0, cooldown_until=NULL WHERE status='COOLDOWN'"
```

### Probl√®me : MailWizz sync √©choue

```bash
# V√©rifier logs sync
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT * FROM mailwizz_sync_log WHERE status='failed' ORDER BY synced_at DESC LIMIT 10"

# Tester connexion MailWizz
docker exec scraper_app python -c "
from scraper.integrations.mailwizz_client import get_client
client = get_client('sos-expat')
print(client.api_url, client.api_key[:10])
"

# Re-envoyer contacts failed
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "UPDATE validated_contacts SET status='ready_for_mailwizz', retry_count=0 WHERE status='failed'"
```

### Probl√®me : Job scraping bloqu√©

```bash
# V√©rifier jobs en cours
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "SELECT id, name, status, progress FROM scraping_jobs WHERE status='running'"

# Reset job bloqu√©
docker exec scraper_postgres psql -U scraper_admin -d scraper_db -c "UPDATE scraping_jobs SET status='failed' WHERE id=X"  # Remplacer X par l'ID
```

---

## üìû Support

**Probl√®mes techniques** :
- üìß Email : support@sos-expat.com
- üìù GitHub Issues : https://github.com/votre-repo/scraper-pro/issues

**Documentation** :
- üìñ User Guide : `USER_GUIDE.md`
- üîß API Docs : `http://VOTRE_IP:8000/docs` (FastAPI auto-doc)

---

## ‚úÖ Checklist post-d√©ploiement

- [ ] VPS configur√© (Docker + Docker Compose install√©s)
- [ ] `.env` rempli avec toutes les cl√©s API
- [ ] Firewall configur√© (ports 22, 8000, 8501)
- [ ] Services d√©marr√©s (`docker-compose up -d`)
- [ ] Health checks OK (Postgres, Redis, API)
- [ ] Dashboard accessible (http://IP:8501)
- [ ] Job test cr√©√© et ex√©cut√© avec succ√®s
- [ ] Contacts valid√©s et envoy√©s vers MailWizz
- [ ] Backup PostgreSQL automatique configur√©
- [ ] Service systemd activ√© pour auto-start
- [ ] (Optionnel) Reverse proxy Nginx + SSL
- [ ] (Optionnel) Monitoring Prometheus + Grafana

---

**Version** : 1.0.0
**Date** : F√©vrier 2026
**Auteur** : Williams - SOS-Expat.com / Ulixai.com
