# ğŸ” Scraper-Pro

**SystÃ¨me professionnel de scraping B2B avec pipeline complet de validation, catÃ©gorisation et injection MailWizz.**

[![Production Ready](https://img.shields.io/badge/production-ready-green.svg)](https://github.com)
[![Docker](https://img.shields.io/badge/docker-compose-blue.svg)](https://docs.docker.com/compose/)
[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-Proprietary-red.svg)](LICENSE)

---

## ğŸ‰ NouveautÃ©s v1.1.0

**Mode Dev API** - DÃ©marrez en 5 minutes sans configuration HMAC !

```bash
# CrÃ©er un job de scraping en une seule commande
curl -X POST http://localhost:8000/api/v1/scraping/jobs/simple \
  -H "Content-Type: application/json" \
  -d '{
    "source_type": "custom_urls",
    "name": "Mon Premier Job",
    "config": {"urls": ["https://example.com"]}
  }'

# Surveiller en temps rÃ©el
./scripts/monitor_job.sh 123
```

**NouveautÃ©s :**
- ğŸ†• Endpoint `/jobs/simple` sans authentification (localhost uniquement)
- ğŸ†• Endpoint `/logs` pour consulter les erreurs dÃ©taillÃ©es
- ğŸ†• Scripts de monitoring (Bash + Python)
- ğŸ“– Guide Quick Start - Premier job en 5 minutes
- ğŸ“– Documentation enrichie avec 45+ exemples

**Liens rapides :**
- ğŸ“– [Quick Start Guide](docs/API_QUICKSTART.md) - DÃ©marrage en 5 minutes
- ğŸ› ï¸ [Mode Dev API](docs/API_DEV_MODE.md) - Guide complet
- ğŸ†• [Release Notes v1.1](RELEASE_NOTES_v1.1.md) - DÃ©tails complets

---

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#-vue-densemble)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [API Reference](#-api-reference)
- [Monitoring](#-monitoring)
- [Backups](#-backups)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

---

## ğŸ¯ Vue d'ensemble

**Scraper-Pro** automatise l'acquisition de contacts professionnels B2B via 4 sources :
- ğŸ” **Google Search** : Recherche par mots-clÃ©s avec pagination
- ğŸ“ **Google Maps** : Extraction de business listings
- ğŸ”— **URLs personnalisÃ©es** : Scraping de listes ciblÃ©es
- ğŸ“ **Blog Content** : Extraction d'articles de blog

Le systÃ¨me inclut un **pipeline complet** :
```
Scraping â†’ Validation â†’ CatÃ©gorisation â†’ Enrichissement â†’ Injection MailWizz
```

### Technologies utilisÃ©es

| Composant | Stack |
|-----------|-------|
| **Scraping** | Scrapy 2.11, SerpAPI (fallback anti-CAPTCHA) |
| **API** | FastAPI 0.115, Uvicorn |
| **Database** | PostgreSQL 16, Redis 7 |
| **Dashboard** | Streamlit 1.41 |
| **Monitoring** | Prometheus, Grafana, Loki |
| **Orchestration** | Docker Compose |
| **Tests** | Pytest, pytest-asyncio |

---

## âœ¨ FonctionnalitÃ©s

### ğŸ•·ï¸ Scraping Multi-Sources
- âœ… 4 spiders opÃ©rationnels avec checkpoint/resume
- âœ… Proxies rotatifs (Oxylabs, BrightData, SmartProxy)
- âœ… Anti-ban : User-agent rotation, delays, rate limiting
- âœ… SerpAPI fallback pour contourner CAPTCHA Google
- âœ… Extraction : emails, tÃ©lÃ©phones, rÃ©seaux sociaux, WHOIS

### âœ… Validation Robuste
- âœ… Email : Regex + DNS MX check + blacklist domaines jetables
- âœ… TÃ©lÃ©phone : Validation internationale avec `phonenumbers`
- âœ… DÃ©duplication : Redis atomic + PostgreSQL UNIQUE
- âœ… Blacklist domaines : Bounce rate > 10%

### ğŸ¯ CatÃ©gorisation Automatique
- âœ… **14 catÃ©gories** : avocat, assureur, mÃ©decin, blogueur, etc.
- âœ… Scoring intelligent : Keywords + source_type
- âœ… Routing multi-plateforme : SOS-Expat vs Ulixai
- âœ… Tags auto-gÃ©nÃ©rÃ©s

### ğŸ“§ IntÃ©gration MailWizz
- âœ… Client API complet (add/update/search)
- âœ… 21 listes configurÃ©es (catÃ©gories professionnelles)
- âœ… Warmup Guard : Protection quotas email
- âœ… Retry logic : 3 tentatives avec backoff
- âœ… Webhooks : Bounce/open/click notifications

### ğŸ“Š Dashboard Administrateur
- âœ… 6 onglets : Jobs, Contacts, Articles, Stats, WHOIS, Config
- âœ… CrÃ©ation de jobs en 1 clic
- âœ… Monitoring temps rÃ©el
- âœ… Exports CSV/JSON
- âœ… Recherche et filtres avancÃ©s

### ğŸ” SÃ©curitÃ©
- âœ… HMAC-SHA256 authentication (API)
- âœ… Rate limiting (protection DDoS)
- âœ… Environment variables pour secrets
- âœ… Docker network isolation
- âœ… Port binding `127.0.0.1` uniquement

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SCRAPER-PRO                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Scrapy     â”‚â”€â”€â”€>â”‚  PostgreSQL  â”‚<â”€â”€â”‚  Dashboard   â”‚  â”‚
â”‚  â”‚   Spiders    â”‚    â”‚   (16 GB)    â”‚   â”‚  Streamlit   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚                   â”‚          â”‚
â”‚         â”‚                    â”‚                   â”‚          â”‚
â”‚         v                    v                   v          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    Redis     â”‚    â”‚   FastAPI    â”‚   â”‚ Prometheus   â”‚  â”‚
â”‚  â”‚   Cache      â”‚<â”€â”€â”€â”‚     API      â”‚â”€â”€â”€â”‚   Grafana    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚                              â”‚
â”‚         â”‚                    â”‚                              â”‚
â”‚         v                    v                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Cron Jobs (process + sync)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    External Services                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Proxies (Oxylabs) â”‚ MailWizz API â”‚ SerpAPI â”‚ WHOIS       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline de traitement

```
1. SCRAPING
   â”œâ”€ Spider scrape les URLs
   â”œâ”€ Extraction contacts (email, phone, social)
   â”œâ”€ Checkpoint sauvegardÃ© (reprise auto)
   â””â”€ Stockage dans scraped_contacts (status: pending_validation)

2. VALIDATION (Cron: toutes les heures)
   â”œâ”€ Email: Regex + MX check + blacklist
   â”œâ”€ TÃ©lÃ©phone: phonenumbers validation
   â”œâ”€ DÃ©duplication: Redis + DB
   â””â”€ Stockage dans validated_contacts (status: ready_for_mailwizz)

3. CATÃ‰GORISATION
   â”œâ”€ Analyse keywords + source_type
   â”œâ”€ Scoring (14 catÃ©gories)
   â”œâ”€ Routing plateforme (SOS-Expat/Ulixai)
   â””â”€ Assignation MailWizz list_id

4. SYNC MAILWIZZ (Cron: toutes les heures, +30min)
   â”œâ”€ Batch de 100 contacts
   â”œâ”€ Warmup guard check
   â”œâ”€ API call avec retry logic
   â””â”€ Update status: sent_to_mailwizz
```

---

## ğŸš€ Installation

### PrÃ©requis

- Docker 20.10+ et Docker Compose 2.0+
- Git
- 4 GB RAM minimum (8 GB recommandÃ©)
- 20 GB espace disque

### Installation rapide

```bash
# 1. Cloner le repository
git clone https://github.com/votre-org/scraper-pro.git
cd scraper-pro

# 2. Copier et configurer .env
cp .env.example .env
nano .env  # Remplir avec vos vraies valeurs

# 3. DÃ©marrer les services
docker-compose up -d

# 4. VÃ©rifier le statut
docker-compose ps

# 5. Voir les logs
docker-compose logs -f scraper

# 6. AccÃ©der au dashboard
# http://localhost:8501
# Mot de passe : celui dÃ©fini dans DASHBOARD_PASSWORD
```

### Installation dÃ©taillÃ©e

Voir [INSTALLATION.md](docs/INSTALLATION.md) pour :
- Configuration des proxies
- Configuration MailWizz
- Configuration monitoring
- Configuration backups

---

## âš™ï¸ Configuration

### Variables d'environnement essentielles

```bash
# PostgreSQL
POSTGRES_DB=scraper_db
POSTGRES_USER=scraper_admin
POSTGRES_PASSWORD=VOTRE_MOT_DE_PASSE_SECURISE

# Redis
REDIS_PASSWORD=VOTRE_MOT_DE_PASSE_REDIS

# Proxies (au moins un provider)
PROXY_PROVIDER=oxylabs
PROXY_USER=votre_username
PROXY_PASS=votre_password

# MailWizz - SOS-Expat
MAILWIZZ_SOS_EXPAT_API_URL=https://mail.sos-expat.com/api
MAILWIZZ_SOS_EXPAT_API_KEY=votre_api_key

# MailWizz - Ulixai
MAILWIZZ_ULIXAI_API_URL=https://mail.ulixai.com/api
MAILWIZZ_ULIXAI_API_KEY=votre_api_key

# Webhooks
WEBHOOK_SOS_EXPAT_URL=https://us-central1-sos-urgently-ac307.cloudfunctions.net/emailEventsWebhook
WEBHOOK_SOS_EXPAT_SECRET=votre_hmac_secret

# API Authentication
API_HMAC_SECRET=VOTRE_SECRET_HMAC_256_BITS

# Dashboard
DASHBOARD_PASSWORD=admin_password_securise

# Optionnel : SerpAPI (fallback anti-CAPTCHA)
SERPAPI_KEY=votre_serpapi_key
```

### Configuration des proxies

Ã‰diter `config/proxy_config.json` :

```json
{
  "providers": {
    "oxylabs": {
      "endpoint": "pr.oxylabs.io:7777",
      "auth_type": "user_pass",
      "pool_size": 20,
      "type": "datacenter"
    }
  },
  "rotation": {
    "mode": "weighted_random",
    "sticky_session_seconds": 180,
    "max_consecutive_failures": 5
  }
}
```

### Configuration MailWizz routing

Ã‰diter `config/mailwizz_routing.json` pour mapper catÃ©gories â†’ listes :

```json
{
  "platforms": {
    "sos-expat": {
      "lists": {
        "avocat": {
          "list_id": 1,
          "list_name": "Avocats Internationaux",
          "auto_tags": ["avocat", "professionnel", "juridique"],
          "template_default": "partenariat_avocat"
        }
      }
    }
  }
}
```

---

## ğŸ“– Utilisation

### 1. Via Dashboard (RecommandÃ©)

1. AccÃ©der Ã  `http://localhost:8501`
2. Se connecter avec `DASHBOARD_PASSWORD`
3. Onglet **Jobs** â†’ **Launch New Job**
4. SÃ©lectionner source type (Google Search, Google Maps, etc.)
5. Configurer paramÃ¨tres (query, max_results, category, platform)
6. Cliquer **Launch Job**
7. Suivre le progrÃ¨s en temps rÃ©el

### 2. Via API

#### CrÃ©er un job

```bash
# GÃ©nÃ©rer signature HMAC
TIMESTAMP=$(date +%s)
BODY='{"source_type":"google_search","name":"Test Job","config":{"query":"avocat Paris","max_results":100},"category":"avocat","platform":"sos-expat","tags":[],"auto_inject_mailwizz":true}'
SIGNATURE=$(echo -n "${TIMESTAMP}.${BODY}" | openssl dgst -sha256 -hmac "VOTRE_API_HMAC_SECRET" | awk '{print $2}')

# Appeler l'API
curl -X POST http://localhost:8000/api/v1/scraping/jobs \
  -H "Content-Type: application/json" \
  -H "X-Timestamp: ${TIMESTAMP}" \
  -H "X-Signature: ${SIGNATURE}" \
  -d "${BODY}"
```

#### Reprendre un job Ã©chouÃ©

```bash
curl -X POST http://localhost:8000/api/v1/scraping/jobs/123/resume \
  -H "X-Timestamp: ${TIMESTAMP}" \
  -H "X-Signature: ${SIGNATURE}"
```

#### Voir le statut d'un job

```bash
curl http://localhost:8000/api/v1/scraping/jobs/123/status \
  -H "X-Timestamp: ${TIMESTAMP}" \
  -H "X-Signature: ${SIGNATURE}"
```

### 3. Via CLI (Manuel)

```bash
# Lancer un spider manuellement
docker-compose exec scraper scrapy crawl google_search \
  -a job_id=999 \
  -a query="mÃ©decin francophone" \
  -a max_results=50 \
  -a country=fr

# Lancer process_contacts manuellement
docker-compose exec scraper python -m scraper.jobs.process_contacts

# Lancer sync_to_mailwizz manuellement
docker-compose exec scraper python -m scraper.jobs.sync_to_mailwizz
```

---

## ğŸ“Š Monitoring

### Dashboards disponibles

| Service | URL | Credentials |
|---------|-----|-------------|
| **Dashboard Scraper** | http://localhost:8501 | `DASHBOARD_PASSWORD` |
| **Grafana** | http://localhost:3000 | admin / `GRAFANA_PASSWORD` |
| **Prometheus** | http://localhost:9090 | - |
| **API Health** | http://localhost:8000/health | - |

### MÃ©triques clÃ©s

```bash
# VÃ©rifier la santÃ© du systÃ¨me
curl http://localhost:8000/health

# RÃ©ponse attendue :
{
  "status": "ok",
  "service": "scraper-pro",
  "postgres": true,
  "redis": true
}
```

### Alertes configurÃ©es

- âœ… Job Ã©chouÃ© 3 fois consÃ©cutivement
- âœ… PostgreSQL down > 2 minutes
- âœ… Redis down > 1 minute
- âœ… Espace disque < 10%
- âœ… Taux d'erreur scraping > 50%
- âœ… Bounce rate MailWizz > 5%

---

## ğŸ’¾ Backups

### Backup automatique

Les backups PostgreSQL sont automatiques **tous les jours Ã  3h00 AM** :

```bash
# Voir les backups disponibles
ls -lh /var/backups/scraper-pro/

# Exemple de sortie :
-rw-r--r-- 1 root root 45M Feb 13 03:00 scraper_db_2026-02-13_030000.sql.gz
-rw-r--r-- 1 root root 43M Feb 12 03:00 scraper_db_2026-02-12_030000.sql.gz
```

### Restauration manuelle

```bash
# ArrÃªter les services
docker-compose down

# Restaurer depuis un backup
gunzip < /var/backups/scraper-pro/scraper_db_2026-02-13_030000.sql.gz | \
  docker-compose exec -T postgres psql -U scraper_admin -d scraper_db

# RedÃ©marrer
docker-compose up -d
```

### Backup manuel

```bash
# CrÃ©er un backup immÃ©diat
docker-compose exec postgres pg_dump -U scraper_admin scraper_db | \
  gzip > backup_manual_$(date +%Y%m%d_%H%M%S).sql.gz
```

---

## ğŸ”§ Troubleshooting

### ProblÃ¨mes courants

#### 1. Container PostgreSQL ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker-compose logs postgres

# Solution : RÃ©initialiser le volume
docker-compose down -v
docker-compose up -d
```

#### 2. Spider bloquÃ© par CAPTCHA

```bash
# VÃ©rifier si SerpAPI est configurÃ©
grep SERPAPI_KEY .env

# Solution : Ajouter SERPAPI_KEY dans .env
SERPAPI_KEY=votre_cle_serpapi
```

#### 3. MailWizz sync Ã©choue

```bash
# VÃ©rifier la configuration
docker-compose exec scraper python -c "
from scraper.integrations.mailwizz_client import get_client
client = get_client('sos-expat')
print('âœ“ MailWizz configured')
"

# VÃ©rifier les logs de sync
docker-compose exec scraper tail -f /app/logs/mailwizz_sync.log
```

#### 4. Dashboard inaccessible

```bash
# VÃ©rifier que le service tourne
docker-compose ps dashboard

# RedÃ©marrer le dashboard
docker-compose restart dashboard
```

#### 5. Proxies ne fonctionnent pas

```bash
# Tester la connexion proxy
docker-compose exec scraper python -c "
import os
import httpx
proxy_url = f'http://{os.getenv(\"PROXY_USER\")}:{os.getenv(\"PROXY_PASS\")}@pr.oxylabs.io:7777'
r = httpx.get('http://ip-api.com/json', proxies={'http://': proxy_url}, timeout=10)
print(r.json())
"
```

### Logs utiles

```bash
# Logs du scraper
docker-compose logs -f scraper

# Logs PostgreSQL
docker-compose logs -f postgres

# Logs Redis
docker-compose logs -f redis

# Logs du dashboard
docker-compose logs -f dashboard

# Logs de tous les services
docker-compose logs -f
```

---

## ğŸ§ª Tests

### Lancer les tests

```bash
# Tous les tests
docker-compose exec scraper pytest

# Avec coverage
docker-compose exec scraper pytest --cov=scraper --cov-report=html

# Tests spÃ©cifiques
docker-compose exec scraper pytest tests/test_validator.py -v
```

### Tests disponibles

- `test_api.py` : Tests API REST
- `test_validator.py` : Validation email/phone
- `test_categorizer.py` : CatÃ©gorisation
- `test_mailwizz_client.py` : Client MailWizz
- `test_spiders.py` : Spiders Scrapy
- `test_pipelines.py` : Pipelines de traitement
- Et 10 autres fichiers...

---

## ğŸ“š Documentation complÃ¨te

### DÃ©marrage Rapide
- ğŸš€ **[API Quick Start](docs/API_QUICKSTART.md)** - Premier job en 5 minutes
- ğŸ› ï¸ **[Mode Dev API](docs/API_DEV_MODE.md)** - DÃ©veloppement sans HMAC
- ğŸ†• **[Release Notes v1.1](RELEASE_NOTES_v1.1.md)** - NouveautÃ©s et changements

### Documentation Technique
- ğŸ“– [Guide d'installation dÃ©taillÃ©](docs/INSTALLATION.md)
- ğŸ—ï¸ [Architecture technique](docs/ARCHITECTURE.md)
- ğŸ”Œ [API Reference complÃ¨te](docs/API.md)
- ğŸ”§ [Configuration avancÃ©e](docs/CONFIGURATION.md)
- ğŸš€ [Guide de dÃ©ploiement](docs/DEPLOYMENT.md)
- ğŸ› [Guide de debugging](docs/DEBUGGING.md)

### Changelogs
- ğŸ“ [Changelog API](CHANGELOG_API.md) - Historique API
- ğŸ“ [Changelog Deployment](CHANGELOG_DEPLOYMENT.md) - DÃ©ploiement
- ğŸ“ [Changelog Metadata](CHANGELOG_METADATA.md) - MÃ©tadonnÃ©es

---

## ğŸ“„ License

Proprietary - SOS-Expat / Ulixai Â© 2024-2026

---

## ğŸ‘¥ Support

- ğŸ“§ Email : support@sos-expat.com
- ğŸ’¬ Slack : #scraper-pro
- ğŸ› Issues : GitHub Issues

---

**Made with â¤ï¸ by the SOS-Expat Tech Team**
