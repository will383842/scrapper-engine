# üöÄ GUIDE DE D√âPLOIEMENT PRODUCTION - SCRAPER-PRO

**Le guide ultime, √©tape par √©tape, pour d√©ployer Scraper-Pro en production sans erreur possible.**

> Ce guide est con√ßu pour √™tre utilisable par un **d√©butant complet**. Chaque commande est expliqu√©e, test√©e, et copy-paste ready.

---

## üìã TABLE DES MATI√àRES

1. [Vue d'ensemble](#-1-vue-densemble)
2. [Pr√©requis et Budget](#-2-pr√©requis-et-budget)
3. [Achat et Configuration VPS Hetzner](#-3-achat-et-configuration-vps-hetzner)
4. [Installation Automatique (RECOMMAND√â)](#-4-installation-automatique-recommand√©)
5. [Installation Manuelle (Step by Step)](#-5-installation-manuelle-step-by-step)
6. [Configuration Initiale](#-6-configuration-initiale)
7. [Acc√®s et Tests](#-7-acc√®s-et-tests)
8. [Monitoring et Grafana](#-8-monitoring-et-grafana)
9. [Maintenance et Backups](#-9-maintenance-et-backups)
10. [Migration vers Google (Plus tard)](#-10-migration-vers-google-plus-tard)
11. [Rollback en Cas de Probl√®me](#-11-rollback-en-cas-de-probl√®me)
12. [Troubleshooting](#-12-troubleshooting)
13. [Checklist Finale](#-13-checklist-finale)

---

## üéØ 1. VUE D'ENSEMBLE

### Ce que vous allez d√©ployer

**Scraper-Pro** est un syst√®me professionnel de scraping B2B avec :
- üîó **Scraping d'URLs personnalis√©es** (mode URLs Only - SANS proxies, SANS Google)
- üìß **Validation automatique** des emails et t√©l√©phones
- üéØ **Cat√©gorisation intelligente** (14 cat√©gories professionnelles)
- üìä **Dashboard Premium** Streamlit pour g√©rer tout en 1 clic
- üîÑ **Injection MailWizz** automatique vers vos listes emails
- üìà **Monitoring complet** Grafana + Prometheus

### Architecture finale

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Serveur Hetzner CPX31 (8GB RAM, 4 vCPU)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üåê Nginx (Reverse Proxy + SSL)                        ‚îÇ
‚îÇ    ‚îú‚îÄ dashboard.votredomaine.com ‚Üí Streamlit (8501)   ‚îÇ
‚îÇ    ‚îú‚îÄ api.votredomaine.com ‚Üí FastAPI (8000)           ‚îÇ
‚îÇ    ‚îî‚îÄ grafana.votredomaine.com ‚Üí Grafana (3000)       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üê≥ Docker Containers                                   ‚îÇ
‚îÇ    ‚îú‚îÄ scraper-postgres (PostgreSQL 16)                ‚îÇ
‚îÇ    ‚îú‚îÄ scraper-redis (Redis 7)                         ‚îÇ
‚îÇ    ‚îú‚îÄ scraper-app (FastAPI + Scrapy)                  ‚îÇ
‚îÇ    ‚îú‚îÄ scraper-dashboard (Streamlit)                   ‚îÇ
‚îÇ    ‚îú‚îÄ scraper-prometheus (Metrics)                    ‚îÇ
‚îÇ    ‚îú‚îÄ scraper-grafana (Monitoring)                    ‚îÇ
‚îÇ    ‚îú‚îÄ scraper-loki (Logs)                             ‚îÇ
‚îÇ    ‚îî‚îÄ scraper-promtail (Log shipping)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Timeline compl√®te

| √âtape | Dur√©e | Difficult√© |
|-------|-------|------------|
| Achat VPS Hetzner | 5 min | ‚≠ê Facile |
| Premier acc√®s SSH | 5 min | ‚≠ê Facile |
| Installation automatique | 10 min | ‚≠ê Facile |
| Configuration .env | 10 min | ‚≠ê‚≠ê Moyen |
| D√©marrage services | 5 min | ‚≠ê Facile |
| Configuration Nginx + SSL | 15 min | ‚≠ê‚≠ê Moyen |
| Tests et validation | 10 min | ‚≠ê Facile |
| **TOTAL** | **1h environ** | |

---

## üí∞ 2. PR√âREQUIS ET BUDGET

### Comp√©tences requises

- ‚úÖ Utiliser un terminal SSH (on vous guide)
- ‚úÖ Copier-coller des commandes
- ‚úÖ √âditer un fichier texte
- ‚ùå PAS besoin de conna√Ætre Docker, Linux, ou le code

### Budget mensuel

#### Mode URLs Only (recommand√© pour d√©marrer)

| Service | Co√ªt/mois | Obligatoire |
|---------|-----------|-------------|
| üñ•Ô∏è Hetzner CPX31 (4 vCPU, 8GB RAM) | ~13‚Ç¨ | ‚úÖ OUI |
| üåê Nom de domaine (optionnel) | ~10‚Ç¨/an | ‚ùå Optionnel |
| üìß MailWizz (si vous l'avez d√©j√†) | Variable | ‚ö†Ô∏è Optionnel |
| **TOTAL** | **~13‚Ç¨/mois** | |

#### Mode Full (Google Search + Maps)

| Service | Co√ªt/mois | Obligatoire |
|---------|-----------|-------------|
| üñ•Ô∏è Hetzner CPX31 | ~13‚Ç¨ | ‚úÖ OUI |
| üîê Proxies (SmartProxy, Oxylabs, BrightData) | 500-2000‚Ç¨ | ‚úÖ OUI |
| üîé SerpAPI (fallback anti-CAPTCHA) | 50-200‚Ç¨ | ‚úÖ OUI |
| üåê Nom de domaine | ~10‚Ç¨/an | ‚ùå Optionnel |
| **TOTAL** | **~563-2213‚Ç¨/mois** | |

> **üí° Recommandation** : Commencez avec le **Mode URLs Only** (13‚Ç¨/mois) pour tester le syst√®me sans risque. Vous pourrez activer Google plus tard en 5 minutes.

### Ce dont vous avez besoin

- ‚úÖ Une carte bancaire (pour Hetzner)
- ‚úÖ Une adresse email
- ‚úÖ Un ordinateur avec acc√®s SSH (Windows, Mac, Linux)
- ‚úÖ 1 heure de temps devant vous

---

## üõí 3. ACHAT ET CONFIGURATION VPS HETZNER

### √âtape 1 : Cr√©er un compte Hetzner

1. Aller sur **https://www.hetzner.com/**
2. Cliquer **"Sign Up"** (en haut √† droite)
3. Remplir le formulaire :
   - Email
   - Mot de passe
   - Adresse (obligatoire pour facturation)
4. Confirmer l'email re√ßu
5. Ajouter un moyen de paiement (CB ou PayPal)

### √âtape 2 : Commander le serveur CPX31

1. Se connecter √† **https://console.hetzner.cloud/**
2. Cliquer **"New Project"**
   - Nom : `scraper-pro-production`
3. Cliquer **"Add Server"**
4. S√©lectionner :
   - **Location** : `Nuremberg` (Allemagne) ou `Helsinki` (Finlande)
   - **Image** : `Ubuntu 22.04`
   - **Type** : `CPX31` (4 vCPU, 8GB RAM, 160GB SSD)
   - **SSH Key** : Cliquer "Add SSH Key" (voir ci-dessous)
   - **Hostname** : `scraper-pro`
5. Cliquer **"Create & Buy Now"**

> **üí° Astuce** : Le serveur CPX31 co√ªte environ 13‚Ç¨/mois. Hetzner facture √† l'heure (environ 0,018‚Ç¨/heure), vous pouvez donc le supprimer quand vous voulez sans engagement.

### √âtape 3 : G√©n√©rer une cl√© SSH (si vous n'en avez pas)

#### Sur Windows (avec Git Bash ou PowerShell)

```bash
# Ouvrir Git Bash ou PowerShell
ssh-keygen -t ed25519 -C "votre-email@example.com"

# Appuyer sur Entr√©e 3 fois (pas de passphrase pour simplifier)
# La cl√© est cr√©√©e dans : C:\Users\VotreNom\.ssh\id_ed25519.pub

# Afficher la cl√© publique
cat ~/.ssh/id_ed25519.pub
```

#### Sur Mac / Linux

```bash
# Ouvrir Terminal
ssh-keygen -t ed25519 -C "votre-email@example.com"

# Appuyer sur Entr√©e 3 fois
# La cl√© est cr√©√©e dans : ~/.ssh/id_ed25519.pub

# Afficher la cl√© publique
cat ~/.ssh/id_ed25519.pub
```

**Copier toute la ligne affich√©e** (commence par `ssh-ed25519 ...`) et la coller dans Hetzner ‚Üí Add SSH Key.

### √âtape 4 : Configurer le Firewall Hetzner

1. Dans le Cloud Console, aller dans **"Firewalls"**
2. Cliquer **"Create Firewall"**
3. Nom : `scraper-firewall`
4. R√®gles entrantes :
   - ‚úÖ SSH (port 22) : `0.0.0.0/0` (tout le monde)
   - ‚úÖ HTTP (port 80) : `0.0.0.0/0`
   - ‚úÖ HTTPS (port 443) : `0.0.0.0/0`
5. R√®gles sortantes :
   - ‚úÖ Tout autoriser
6. Cliquer **"Create"**
7. Attacher le firewall au serveur `scraper-pro`

### √âtape 5 : Premier acc√®s SSH

Une fois le serveur cr√©√© (environ 30 secondes), vous verrez son **adresse IP publique** (exemple : `95.217.123.45`).

```bash
# Remplacer par VOTRE adresse IP
ssh root@95.217.123.45
```

**√Ä la premi√®re connexion**, vous verrez ce message :

```
The authenticity of host '95.217.123.45' can't be established.
Are you sure you want to continue connecting (yes/no)?
```

Tapez **`yes`** puis Entr√©e.

‚úÖ **Vous √™tes maintenant connect√© au serveur !**

---

## ‚ö° 4. INSTALLATION AUTOMATIQUE (RECOMMAND√â)

### Option A : Script d'installation one-liner

Cette m√©thode installe **tout automatiquement** en une seule commande.

```bash
# Copier-coller cette commande (une seule ligne)
curl -fsSL https://raw.githubusercontent.com/VOTRE_ORG/scraper-pro/main/scripts/quick_install.sh | bash
```

> **‚ö†Ô∏è ATTENTION** : Si vous n'avez pas de script `quick_install.sh` dans votre repo, passez √† l'**Option B** ci-dessous.

### Ce que le script fait automatiquement

1. ‚úÖ Met √† jour Ubuntu
2. ‚úÖ Installe Docker + Docker Compose
3. ‚úÖ Installe Git, Nginx, Certbot
4. ‚úÖ Clone le repository scraper-pro
5. ‚úÖ Copie `.env.production` vers `.env`
6. ‚úÖ G√©n√®re des secrets forts automatiquement
7. ‚úÖ Build les images Docker
8. ‚úÖ D√©marre tous les services
9. ‚úÖ Configure le firewall UFW

### Validation

```bash
# V√©rifier que les containers tournent
docker ps

# Vous devriez voir 8 containers :
# - scraper-postgres
# - scraper-redis
# - scraper-app
# - scraper-dashboard
# - scraper-prometheus
# - scraper-grafana
# - scraper-loki
# - scraper-promtail
```

‚úÖ **Si vous voyez 8 containers "Up", passez directement √† [Configuration Initiale](#-6-configuration-initiale).**

---

## üõ†Ô∏è 5. INSTALLATION MANUELLE (STEP BY STEP)

Si le script automatique ne fonctionne pas, ou si vous pr√©f√©rez tout contr√¥ler, suivez ces √©tapes.

### √âtape 1 : Mise √† jour du syst√®me

```bash
# Se connecter en SSH (si pas d√©j√† fait)
ssh root@95.217.123.45

# Mettre √† jour tous les paquets
apt update && apt upgrade -y

# Rebooter (recommand√©)
reboot

# Attendre 1 minute, puis se reconnecter
ssh root@95.217.123.45
```

### √âtape 2 : Installer Docker

```bash
# Installer les d√©pendances
apt install -y apt-transport-https ca-certificates curl software-properties-common

# Ajouter la cl√© GPG Docker
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# Ajouter le repository Docker
echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

# Mettre √† jour et installer Docker
apt update
apt install -y docker-ce docker-ce-cli containerd.io

# V√©rifier l'installation
docker --version
# R√©sultat attendu : Docker version 24.0.x ou sup√©rieur
```

### √âtape 3 : Installer Docker Compose

```bash
# T√©l√©charger Docker Compose (derni√®re version)
curl -L "https://github.com/docker/compose/releases/download/v2.24.5/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

# Rendre ex√©cutable
chmod +x /usr/local/bin/docker-compose

# V√©rifier l'installation
docker-compose --version
# R√©sultat attendu : Docker Compose version v2.24.5 ou sup√©rieur
```

### √âtape 4 : Installer Git, Nginx, et outils

```bash
# Installer Git
apt install -y git

# Installer Nginx (reverse proxy)
apt install -y nginx

# Installer Certbot (SSL/TLS gratuit)
apt install -y certbot python3-certbot-nginx

# Installer des outils utiles
apt install -y htop curl wget nano

# V√©rifier
git --version
nginx -v
certbot --version
```

### √âtape 5 : Configurer le firewall UFW

```bash
# Installer UFW (si pas d√©j√† install√©)
apt install -y ufw

# Autoriser SSH (IMPORTANT : avant d'activer UFW)
ufw allow 22/tcp

# Autoriser HTTP/HTTPS
ufw allow 80/tcp
ufw allow 443/tcp

# Activer UFW
ufw enable

# V√©rifier le statut
ufw status

# R√©sultat attendu :
# Status: active
# To                         Action      From
# --                         ------      ----
# 22/tcp                     ALLOW       Anywhere
# 80/tcp                     ALLOW       Anywhere
# 443/tcp                    ALLOW       Anywhere
```

### √âtape 6 : Cloner le repository

```bash
# Cr√©er le r√©pertoire de d√©ploiement
mkdir -p /opt/scraper-pro
cd /opt/scraper-pro

# Cloner le projet (remplacer par VOTRE URL)
git clone https://github.com/VOTRE_ORG/scraper-pro.git .

# V√©rifier
ls -la
# Vous devriez voir : docker-compose.production.yml, .env.production, etc.
```

### √âtape 7 : Configurer les variables d'environnement

```bash
# Copier le fichier exemple
cp .env.production .env

# √âditer le fichier
nano .env
```

#### Variables OBLIGATOIRES √† modifier

Utilisez les fl√®ches du clavier pour naviguer, puis modifiez ces lignes :

```bash
# ========================================
# PASSWORDS (OBLIGATOIRE)
# ========================================

# PostgreSQL
POSTGRES_PASSWORD=CHANGE_ME_STRONG_PASSWORD_32_CHARS_MIN

# Redis
REDIS_PASSWORD=CHANGE_ME_STRONG_PASSWORD_32_CHARS_MIN

# Dashboard
DASHBOARD_PASSWORD=CHANGE_ME_STRONG_PASSWORD

# API HMAC Secret (64 caract√®res minimum)
API_HMAC_SECRET=CHANGE_ME_STRONG_SECRET_64_CHARS_HERE

# Grafana
GRAFANA_PASSWORD=CHANGE_ME_GRAFANA_PASSWORD

# ========================================
# MAILWIZZ (Optionnel si vous l'avez)
# ========================================

MAILWIZZ_SOS_EXPAT_API_KEY=your_api_key_here
MAILWIZZ_ULIXAI_API_KEY=your_api_key_here

# ========================================
# WEBHOOKS (Optionnel)
# ========================================

WEBHOOK_SOS_EXPAT_SECRET=your_hmac_secret_here
WEBHOOK_ULIXAI_SECRET=your_hmac_secret_here
```

#### G√©n√©rer des secrets forts

**Option 1 : Commandes rapides**

```bash
# PostgreSQL password (copier le r√©sultat dans .env)
openssl rand -base64 32

# Redis password
openssl rand -base64 32

# API HMAC secret (64 caract√®res)
openssl rand -hex 32

# Dashboard password
openssl rand -base64 24

# Grafana password
openssl rand -base64 24
```

**Option 2 : G√©n√©rateur en ligne**

Allez sur **https://passwordsgenerator.net/** et g√©n√©rez des mots de passe de 32+ caract√®res.

#### Sauvegarder et quitter nano

1. Appuyez sur **Ctrl + O** (pour sauvegarder)
2. Appuyez sur **Entr√©e** (confirmer le nom)
3. Appuyez sur **Ctrl + X** (pour quitter)

#### Prot√©ger le fichier .env

```bash
# Le fichier .env contient des secrets, personne d'autre ne doit le lire
chmod 600 .env

# V√©rifier
ls -la .env
# R√©sultat attendu : -rw------- (seulement vous pouvez lire/√©crire)
```

### √âtape 8 : Build et d√©marrer les services

```bash
# Build toutes les images Docker (peut prendre 5-10 minutes)
docker-compose -f docker-compose.production.yml build

# D√©marrer tous les services en arri√®re-plan
docker-compose -f docker-compose.production.yml up -d

# V√©rifier que les containers d√©marrent
docker-compose -f docker-compose.production.yml ps

# R√©sultat attendu : 8 containers avec "Up" dans la colonne STATE
```

### √âtape 9 : V√©rifier les logs

```bash
# Voir les logs de tous les services en temps r√©el
docker-compose -f docker-compose.production.yml logs -f

# Appuyez sur Ctrl+C pour arr√™ter de suivre les logs

# Voir les logs d'un service sp√©cifique
docker logs scraper-app --tail 50
docker logs scraper-postgres --tail 50
docker logs scraper-dashboard --tail 50
```

**‚ö†Ô∏è Erreurs courantes :**

- Si vous voyez `connection refused` : attendez 30 secondes que PostgreSQL d√©marre compl√®tement
- Si vous voyez `password authentication failed` : v√©rifiez que `POSTGRES_PASSWORD` est identique partout dans `.env`
- Si vous voyez `port already in use` : un autre service utilise le port, changez-le dans `docker-compose.production.yml`

‚úÖ **Si aucune erreur, passez √† la configuration initiale !**

---

## ‚öôÔ∏è 6. CONFIGURATION INITIALE

### √âtape 1 : Tester l'API

```bash
# Health check de l'API
curl http://localhost:8000/health

# R√©sultat attendu :
# {"status":"ok","service":"scraper-pro","postgres":true,"redis":true}
```

‚úÖ **Si vous voyez `"status":"ok"`, l'API fonctionne !**

### √âtape 2 : Tester PostgreSQL

```bash
# Se connecter √† PostgreSQL
docker exec -it scraper-postgres psql -U scraper_admin -d scraper_db

# Une fois connect√©, v√©rifier les tables
\dt

# R√©sultat attendu : vous devez voir ces tables :
# - scraping_jobs
# - scraped_contacts
# - validated_contacts
# - url_deduplication_cache
# - content_hash_cache
# - mailwizz_sync_log
# - error_logs
# - whois_cache
# - scraped_articles

# Quitter PostgreSQL
\q
```

### √âtape 3 : Tester Redis

```bash
# Se connecter √† Redis (remplacer YOUR_REDIS_PASSWORD par le vrai mot de passe depuis .env)
docker exec -it scraper-redis redis-cli -a YOUR_REDIS_PASSWORD

# Une fois connect√©, tester
PING

# R√©sultat attendu : PONG

# Voir toutes les cl√©s (vide au d√©but)
KEYS *

# Quitter Redis
exit
```

### √âtape 4 : Acc√©der au Dashboard (test local)

Pour l'instant, le Dashboard n'est accessible que depuis le serveur (localhost). Nous allons configurer Nginx pour y acc√©der depuis votre navigateur.

```bash
# Test rapide (depuis le serveur)
curl -I http://localhost:8501

# R√©sultat attendu : HTTP/1.1 200 OK
```

---

## üåê 7. ACC√àS ET TESTS

### Configuration Nginx (Reverse Proxy + SSL)

#### Option A : Avec un nom de domaine (RECOMMAND√â)

Si vous avez un nom de domaine (exemple : `example.com`), vous pouvez cr√©er 3 sous-domaines :

- **dashboard.example.com** ‚Üí Dashboard Streamlit
- **api.example.com** ‚Üí API FastAPI
- **grafana.example.com** ‚Üí Monitoring Grafana

**√âtape 1 : Configurer les DNS**

Chez votre registrar (OVH, Gandi, Namecheap, etc.), cr√©ez 3 enregistrements DNS de type **A** :

| Nom | Type | Valeur |
|-----|------|--------|
| dashboard | A | 95.217.123.45 (votre IP serveur) |
| api | A | 95.217.123.45 |
| grafana | A | 95.217.123.45 |

**Attendre 5-10 minutes** que les DNS se propagent.

**√âtape 2 : Configuration Nginx pour le Dashboard**

```bash
# Cr√©er le fichier de configuration
nano /etc/nginx/sites-available/scraper-dashboard
```

Copier-coller ce contenu (remplacer `dashboard.example.com` par VOTRE domaine) :

```nginx
server {
    listen 80;
    server_name dashboard.example.com;

    location / {
        proxy_pass http://localhost:8501;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_read_timeout 86400;
    }
}
```

Sauvegarder (**Ctrl+O**, **Entr√©e**, **Ctrl+X**).

**√âtape 3 : Configuration Nginx pour l'API**

```bash
nano /etc/nginx/sites-available/scraper-api
```

Copier-coller ce contenu (remplacer `api.example.com`) :

```nginx
server {
    listen 80;
    server_name api.example.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Sauvegarder.

**√âtape 4 : Configuration Nginx pour Grafana**

```bash
nano /etc/nginx/sites-available/scraper-grafana
```

Copier-coller ce contenu (remplacer `grafana.example.com`) :

```nginx
server {
    listen 80;
    server_name grafana.example.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Sauvegarder.

**√âtape 5 : Activer les sites**

```bash
# Cr√©er les liens symboliques
ln -s /etc/nginx/sites-available/scraper-dashboard /etc/nginx/sites-enabled/
ln -s /etc/nginx/sites-available/scraper-api /etc/nginx/sites-enabled/
ln -s /etc/nginx/sites-available/scraper-grafana /etc/nginx/sites-enabled/

# Tester la configuration
nginx -t

# R√©sultat attendu : syntax is ok, test is successful

# Recharger Nginx
systemctl reload nginx
```

**√âtape 6 : Installer les certificats SSL (Let's Encrypt)**

```bash
# Dashboard
certbot --nginx -d dashboard.example.com

# R√©pondre aux questions :
# - Email : votre email
# - Terms of service : A (agree)
# - Share email : N (no)
# - Redirect HTTP to HTTPS : 2 (yes)

# API
certbot --nginx -d api.example.com

# Grafana
certbot --nginx -d grafana.example.com
```

Les certificats SSL sont install√©s et se renouvellent automatiquement !

**√âtape 7 : Configuration du renouvellement automatique**

```bash
# Tester le renouvellement automatique
certbot renew --dry-run

# R√©sultat attendu : Congratulations, all simulated renewals succeeded

# Ajouter un cron job pour renouveler automatiquement
crontab -e

# Ajouter cette ligne √† la fin :
0 3 * * * certbot renew --quiet
```

Sauvegarder (**Ctrl+O**, **Entr√©e**, **Ctrl+X**).

‚úÖ **Maintenant, vous pouvez acc√©der √† :**
- **https://dashboard.example.com** (Dashboard Premium)
- **https://api.example.com** (API)
- **https://grafana.example.com** (Monitoring)

#### Option B : Sans nom de domaine (acc√®s par IP)

Si vous n'avez pas de domaine, vous pouvez acc√©der directement par IP, mais **SANS SSL** (non s√©curis√©).

```bash
# Modifier le firewall pour exposer les ports
ufw allow 8501/tcp  # Dashboard
ufw allow 8000/tcp  # API
ufw allow 3000/tcp  # Grafana
```

‚úÖ **Acc√®s :**
- **http://95.217.123.45:8501** (Dashboard)
- **http://95.217.123.45:8000** (API)
- **http://95.217.123.45:3000** (Grafana)

> **‚ö†Ô∏è ATTENTION** : Cette m√©thode est **NON S√âCURIS√âE** (pas de SSL). Utilisez-la uniquement pour les tests, jamais en production avec de vraies donn√©es.

### Acc√®s au Dashboard

1. Ouvrir **https://dashboard.example.com** (ou http://IP:8501)
2. Vous verrez un √©cran de login avec un champ **"Password"**
3. Entrer le mot de passe depuis `.env` ‚Üí `DASHBOARD_PASSWORD`
4. Cliquer **"Login"**

‚úÖ **Vous √™tes connect√© au Dashboard Premium !**

### Premier test : Cr√©er un job de scraping

1. Dans le Dashboard, aller dans l'onglet **"Scraping URLs (Actif)"**
2. Section **"Lancer un Nouveau Job"**
3. Remplir :
   - **Nom du job** : `Test Job 1`
   - **Plateforme** : `sos-expat`
   - **URLs** : Coller quelques URLs de test (une par ligne) :
     ```
     https://example.com
     https://www.wikipedia.org
     https://www.github.com
     ```
4. Cliquer **"Lancer le Job"**

**R√©sultat attendu** : Le job d√©marre, et vous voyez :
- Un ID de job (exemple : `#123`)
- Statut : `running`
- Progression en temps r√©el

5. Attendre 1-2 minutes que le job se termine
6. V√©rifier les r√©sultats dans l'onglet **"Contacts"**

‚úÖ **Si vous voyez des contacts extraits, le syst√®me fonctionne parfaitement !**

---

## üìä 8. MONITORING ET GRAFANA

### Acc√®s √† Grafana

1. Ouvrir **https://grafana.example.com** (ou http://IP:3000)
2. Login :
   - **Username** : `admin`
   - **Password** : Depuis `.env` ‚Üí `GRAFANA_PASSWORD`
3. Cliquer **"Log in"**

### Dashboards disponibles

Grafana a √©t√© configur√© avec plusieurs dashboards pr√©-install√©s :

1. **Scraper Overview** : Vue d'ensemble du syst√®me
   - Jobs en cours
   - Contacts scrap√©s (total)
   - Contacts valid√©s
   - Taux de succ√®s
   - Erreurs r√©centes

2. **Deduplication Stats** : Statistiques de d√©duplication
   - URLs d√©dupliqu√©es (exact + normalized)
   - Emails uniques
   - Content hash uniques
   - Taux de d√©duplication global

3. **PostgreSQL Performance** : M√©triques de la base de donn√©es
   - Connections actives
   - Slow queries
   - Index usage
   - Table sizes

4. **Redis Performance** : M√©triques Redis
   - Memory usage
   - Hit rate
   - Commands per second
   - Connected clients

5. **System Resources** : Ressources du serveur
   - CPU usage
   - RAM usage
   - Disk I/O
   - Network traffic

### Configuration des alertes email

1. Dans Grafana, aller dans **"Alerting"** ‚Üí **"Contact points"**
2. Cliquer **"New contact point"**
3. Remplir :
   - **Name** : `Email Alerts`
   - **Type** : `Email`
   - **Addresses** : `votre-email@example.com`
4. Cliquer **"Test"** pour v√©rifier que l'email arrive
5. Cliquer **"Save contact point"**

Les alertes configur√©es :
- ‚úÖ Service down (critical)
- ‚úÖ High job failure rate (warning)
- ‚úÖ PostgreSQL connection issues (warning)
- ‚úÖ Redis connection issues (warning)
- ‚úÖ Disk usage > 80% (warning)

---

## üîß 9. MAINTENANCE ET BACKUPS

### Backup automatique de PostgreSQL

**√âtape 1 : Cr√©er le script de backup**

```bash
# Cr√©er le r√©pertoire de backups
mkdir -p /home/scraper/backups

# Cr√©er le script
nano /home/scraper/backup-postgres.sh
```

Copier-coller ce contenu :

```bash
#!/bin/bash

# Configuration
BACKUP_DIR="/home/scraper/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
POSTGRES_PASSWORD="YOUR_POSTGRES_PASSWORD"  # Remplacer par le vrai mot de passe
CONTAINER_NAME="scraper-postgres"
DB_USER="scraper_admin"
DB_NAME="scraper_db"

# Cr√©er le r√©pertoire si n√©cessaire
mkdir -p $BACKUP_DIR

# Ex√©cuter le backup
docker exec $CONTAINER_NAME pg_dump -U $DB_USER $DB_NAME | gzip > $BACKUP_DIR/scraper_db_$TIMESTAMP.sql.gz

# V√©rifier que le backup a r√©ussi
if [ $? -eq 0 ]; then
    echo "‚úÖ Backup r√©ussi : scraper_db_$TIMESTAMP.sql.gz"
else
    echo "‚ùå Backup √©chou√©"
    exit 1
fi

# Garder seulement les 7 derniers backups (1 semaine)
find $BACKUP_DIR -name "scraper_db_*.sql.gz" -mtime +7 -delete

# Afficher l'espace utilis√©
du -sh $BACKUP_DIR

echo "‚úÖ Backup termin√© avec succ√®s"
```

**√âtape 2 : Rendre le script ex√©cutable**

```bash
chmod +x /home/scraper/backup-postgres.sh
```

**√âtape 3 : Tester le script**

```bash
# Lancer manuellement
/home/scraper/backup-postgres.sh

# V√©rifier que le backup a √©t√© cr√©√©
ls -lh /home/scraper/backups/

# Vous devriez voir un fichier .sql.gz
```

**√âtape 4 : Configurer le cron (backup quotidien √† 2h du matin)**

```bash
# √âditer le crontab
crontab -e

# Ajouter cette ligne √† la fin :
0 2 * * * /home/scraper/backup-postgres.sh >> /home/scraper/backup.log 2>&1
```

Sauvegarder (**Ctrl+O**, **Entr√©e**, **Ctrl+X**).

‚úÖ **Les backups se feront automatiquement tous les jours √† 2h du matin !**

### Restaurer un backup

Si vous avez besoin de restaurer un backup :

```bash
# Arr√™ter les services
docker-compose -f docker-compose.production.yml down

# Lister les backups disponibles
ls -lh /home/scraper/backups/

# Choisir un backup (exemple : scraper_db_20260213_020000.sql.gz)
gunzip < /home/scraper/backups/scraper_db_20260213_020000.sql.gz | docker-compose -f docker-compose.production.yml run --rm postgres psql -U scraper_admin -d scraper_db

# Red√©marrer les services
docker-compose -f docker-compose.production.yml up -d
```

### Rotation des logs Docker

Les logs Docker sont automatiquement limit√©s dans `docker-compose.production.yml` :

```yaml
logging:
  driver: "json-file"
  options:
    max-size: "50m"    # Maximum 50MB par fichier de log
    max-file: "3"      # Garder 3 fichiers (= 150MB max)
```

Pour nettoyer manuellement :

```bash
# Voir l'espace utilis√© par Docker
docker system df

# Nettoyer les logs et images non utilis√©es
docker system prune -a --volumes
```

### Mise √† jour du syst√®me

**Tous les mois**, pensez √† mettre √† jour le syst√®me :

```bash
# Se connecter au serveur
ssh root@95.217.123.45

# Mettre √† jour les paquets
apt update && apt upgrade -y

# Nettoyer
apt autoremove -y

# Rebooter (recommand√©)
reboot
```

### Mise √† jour de Scraper-Pro

Quand une nouvelle version est disponible :

```bash
# Aller dans le r√©pertoire du projet
cd /opt/scraper-pro

# Sauvegarder le .env actuel
cp .env .env.backup

# Pull les derni√®res modifications
git pull origin main

# Rebuild les images
docker-compose -f docker-compose.production.yml build

# Restart
docker-compose -f docker-compose.production.yml up -d

# V√©rifier les logs
docker-compose -f docker-compose.production.yml logs -f
```

---

## üöÄ 10. MIGRATION VERS GOOGLE (PLUS TARD)

Quand vous serez pr√™t √† activer **Google Search** et **Google Maps**, voici comment faire.

### √âtape 1 : Souscrire aux services

#### Proxies (choisir UN seul provider)

**Option A : SmartProxy** (recommand√©, bon rapport qualit√©/prix)
- Site : **https://smartproxy.com/**
- Prix : ~500-800‚Ç¨/mois (10GB de data)
- Type : Residential proxies
- Setup : Tr√®s simple

**Option B : Oxylabs** (qualit√© premium)
- Site : **https://oxylabs.io/**
- Prix : ~1000-2000‚Ç¨/mois
- Type : Datacenter ou Residential
- Setup : Moyen

**Option C : BrightData** (anciennement Luminati)
- Site : **https://brightdata.com/**
- Prix : ~600-1500‚Ç¨/mois
- Type : Residential proxies
- Setup : Complexe mais tr√®s puissant

#### SerpAPI (fallback anti-CAPTCHA Google)

- Site : **https://serpapi.com/**
- Prix : ~50-200‚Ç¨/mois (5000-50000 requ√™tes)
- Setup : Tr√®s simple (juste une cl√© API)

### √âtape 2 : Configurer .env

```bash
# Se connecter au serveur
ssh root@95.217.123.45

# √âditer .env
cd /opt/scraper-pro
nano .env
```

Modifier ces lignes :

```bash
# ========================================
# MODE DE SCRAPING
# ========================================

# Passer de "urls_only" √† "full"
SCRAPING_MODE=full

# ========================================
# PROXY PROVIDER
# ========================================

# Choisir votre provider (oxylabs, brightdata, smartproxy)
PROXY_PROVIDER=smartproxy

# Vos credentials proxy
PROXY_USER=your_username_here
PROXY_PASS=your_password_here

# ========================================
# SERPAPI (GOOGLE SEARCH FALLBACK)
# ========================================

SERPAPI_KEY=your_serpapi_key_here
```

Sauvegarder (**Ctrl+O**, **Entr√©e**, **Ctrl+X**).

### √âtape 3 : Red√©marrer les services

```bash
# Arr√™ter les services
docker-compose -f docker-compose.production.yml down

# Red√©marrer
docker-compose -f docker-compose.production.yml up -d

# V√©rifier les logs
docker-compose -f docker-compose.production.yml logs -f scraper-app
```

### √âtape 4 : Tester Google Search

1. Aller sur le **Dashboard**
2. Vous verrez maintenant un nouvel onglet : **"Scraping Google"**
3. Cr√©er un job **"Google Search"** :
   - **Query** : `avocat Paris`
   - **Max results** : `50`
   - **Country** : `fr`
   - **Category** : `avocat`
   - **Platform** : `sos-expat`
4. Cliquer **"Lancer le Job"**
5. Surveiller les logs :
   ```bash
   docker logs scraper-app -f
   ```

**Si vous voyez des r√©sultats Google dans l'onglet Contacts, c'est r√©ussi !**

---

## üîÑ 11. ROLLBACK EN CAS DE PROBL√àME

Cette section d√©crit comment revenir en arri√®re si quelque chose ne fonctionne pas.

### Arr√™ter les Services (R√©versible)

Cette m√©thode arr√™te tous les containers mais **pr√©serve vos donn√©es** (PostgreSQL, Redis).

```bash
# Se connecter au serveur
ssh root@95.217.123.45

# Aller dans le r√©pertoire du projet
cd /opt/scraper-pro

# Arr√™ter tous les services
docker-compose -f docker-compose.production.yml down

# R√©sultat : Les containers sont arr√™t√©s, mais les volumes persistent
```

**Pour red√©marrer apr√®s :**

```bash
# Red√©marrer tous les services
docker-compose -f docker-compose.production.yml up -d

# V√©rifier
docker ps
```

‚úÖ **Vos donn√©es (jobs, contacts, cache) sont toujours l√† !**

---

### Red√©marrer un Service Sp√©cifique

Si un seul service pose probl√®me :

```bash
# Red√©marrer l'API
docker restart scraper-app

# Red√©marrer PostgreSQL
docker restart scraper-postgres

# Red√©marrer Redis
docker restart scraper-redis

# Red√©marrer le Dashboard
docker restart scraper-dashboard

# Red√©marrer Grafana
docker restart scraper-grafana
```

---

### Supprimer Compl√®tement (DESTRUCTIF)

‚ö†Ô∏è **ATTENTION : Cette m√©thode supprime TOUTES les donn√©es (base de donn√©es, cache, logs).**

```bash
# Arr√™ter et supprimer TOUS les containers + volumes
docker-compose -f docker-compose.production.yml down -v

# Nettoyer tout Docker (images, volumes orphelins, etc.)
docker system prune -a --volumes -f

# R√©sultat : Le syst√®me est compl√®tement r√©initialis√©
```

**Utilisez cette m√©thode UNIQUEMENT si :**
- Vous voulez recommencer de z√©ro
- Vous avez un backup r√©cent
- Vous √™tes s√ªr de ne pas avoir besoin des donn√©es

---

### Restaurer depuis un Backup

Si vous avez un backup PostgreSQL (cr√©√© avec le script de backup) :

```bash
# Lister les backups disponibles
ls -lh /home/scraper/backups/

# Vous verrez des fichiers comme : scraper_db_20260213_020000.sql.gz

# √âtape 1 : Arr√™ter les services
cd /opt/scraper-pro
docker-compose -f docker-compose.production.yml down

# √âtape 2 : Supprimer les anciennes donn√©es PostgreSQL
docker volume rm scraper-pro_postgres_data

# √âtape 3 : Red√©marrer PostgreSQL seul
docker-compose -f docker-compose.production.yml up -d postgres

# Attendre 10 secondes que PostgreSQL soit pr√™t
sleep 10

# √âtape 4 : Restaurer le backup (remplacer par le vrai nom de fichier)
gunzip < /home/scraper/backups/scraper_db_20260213_020000.sql.gz | \
    docker exec -i scraper-postgres psql -U scraper_admin -d scraper_db

# √âtape 5 : Red√©marrer tous les services
docker-compose -f docker-compose.production.yml up -d

# √âtape 6 : V√©rifier
docker ps
curl http://localhost:8000/health
```

‚úÖ **Vos donn√©es sont restaur√©es √† l'√©tat du backup !**

---

### Revenir √† une Version Pr√©c√©dente du Code

Si une mise √† jour du code a cass√© quelque chose :

```bash
# Aller dans le r√©pertoire
cd /opt/scraper-pro

# Voir l'historique des commits
git log --oneline -10

# Exemple de sortie :
# a1b2c3d (HEAD -> main) Update: improved deduplication
# d4e5f6g Fix: dashboard crash on large jobs
# g7h8i9j Add: new validation system

# Revenir au commit pr√©c√©dent (remplacer par le vrai ID)
git checkout d4e5f6g

# Rebuild les images
docker-compose -f docker-compose.production.yml build

# Red√©marrer
docker-compose -f docker-compose.production.yml up -d

# V√©rifier les logs
docker-compose -f docker-compose.production.yml logs -f
```

**Pour revenir √† la derni√®re version :**

```bash
git checkout main
docker-compose -f docker-compose.production.yml build
docker-compose -f docker-compose.production.yml up -d
```

---

### Recommencer l'Installation Compl√®te

Si tout est cass√© et que vous voulez repartir de z√©ro :

```bash
# √âtape 1 : Faire un backup de .env (contient vos secrets)
cp /opt/scraper-pro/.env ~/scraper-backup.env

# √âtape 2 : Faire un backup des secrets (si vous les avez sauvegard√©s)
cp ~/.scraper-pro-secrets-*.txt ~/ 2>/dev/null || echo "No secrets file found"

# √âtape 3 : Arr√™ter et supprimer tout
cd /opt/scraper-pro
docker-compose -f docker-compose.production.yml down -v
docker system prune -a --volumes -f

# √âtape 4 : Supprimer le r√©pertoire
cd ~
rm -rf /opt/scraper-pro

# √âtape 5 : R√©installer (cloner le repo)
git clone https://github.com/VOTRE_ORG/scraper-pro.git /opt/scraper-pro
cd /opt/scraper-pro

# √âtape 6 : Restaurer .env
cp ~/scraper-backup.env .env
chmod 600 .env

# √âtape 7 : R√©installer avec le script automatique
bash scripts/quick_install.sh --skip-docker

# √âtape 8 : D√©marrer les services
docker-compose -f docker-compose.production.yml build
docker-compose -f docker-compose.production.yml up -d

# √âtape 9 : V√©rifier
bash scripts/validate-installation.sh
```

‚úÖ **Installation fra√Æche avec vos anciens secrets !**

---

### Sauvegarder Avant une Op√©ration Risqu√©e

**Bonne pratique** : Toujours sauvegarder avant de faire des changements importants.

```bash
# Script rapide de sauvegarde compl√®te
BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/home/scraper/emergency-backup-$BACKUP_DATE"

# Cr√©er le r√©pertoire de backup
mkdir -p $BACKUP_DIR

# Sauvegarder .env
cp /opt/scraper-pro/.env $BACKUP_DIR/

# Sauvegarder PostgreSQL
docker exec scraper-postgres pg_dump -U scraper_admin scraper_db | \
    gzip > $BACKUP_DIR/postgres.sql.gz

# Sauvegarder Redis (snapshot)
docker exec scraper-redis redis-cli -a $(grep REDIS_PASSWORD /opt/scraper-pro/.env | cut -d'=' -f2) \
    --rdb /data/backup.rdb 2>/dev/null || true
docker cp scraper-redis:/data/backup.rdb $BACKUP_DIR/redis.rdb

# Afficher le r√©sultat
ls -lh $BACKUP_DIR/
du -sh $BACKUP_DIR/

echo "‚úÖ Backup complet sauvegard√© dans : $BACKUP_DIR"
```

**Pour restaurer ce backup :**

```bash
# Remplacer BACKUP_DATE par la vraie date
BACKUP_DIR="/home/scraper/emergency-backup-20260213_143000"

# Restaurer .env
cp $BACKUP_DIR/.env /opt/scraper-pro/.env

# Restaurer PostgreSQL
cd /opt/scraper-pro
docker-compose -f docker-compose.production.yml down postgres
docker volume rm scraper-pro_postgres_data
docker-compose -f docker-compose.production.yml up -d postgres
sleep 10

gunzip < $BACKUP_DIR/postgres.sql.gz | \
    docker exec -i scraper-postgres psql -U scraper_admin -d scraper_db

# Red√©marrer tout
docker-compose -f docker-compose.production.yml up -d
```

---

## üêõ 12. TROUBLESHOOTING

### Probl√®me : Container ne d√©marre pas

**Sympt√¥mes** : `docker ps` montre un container "Exited" ou "Restarting"

**Solution** :

```bash
# Voir les logs du container probl√©matique
docker logs scraper-app --tail 100

# Causes courantes :
# 1. Mauvais mot de passe dans .env
# 2. Port d√©j√† utilis√©
# 3. Erreur de syntax dans .env

# V√©rifier la configuration
cat .env | grep -v "^#" | grep -v "^$"

# Restart complet
docker-compose -f docker-compose.production.yml down
docker-compose -f docker-compose.production.yml up -d
```

### Probl√®me : PostgreSQL "connection refused"

**Sympt√¥mes** : L'API ne peut pas se connecter √† PostgreSQL

**Solution** :

```bash
# V√©rifier que PostgreSQL tourne
docker ps | grep postgres

# V√©rifier les logs
docker logs scraper-postgres --tail 50

# Tester la connexion
docker exec scraper-postgres pg_isready -U scraper_admin

# Si "accepting connections" ‚Üí OK
# Si "no response" ‚Üí attendre 30 secondes de plus

# V√©rifier le mot de passe dans .env
grep POSTGRES_PASSWORD .env

# Recr√©er PostgreSQL si n√©cessaire
docker-compose -f docker-compose.production.yml down postgres
docker volume rm scraper-pro_postgres_data
docker-compose -f docker-compose.production.yml up -d postgres
```

### Probl√®me : Redis "connection refused"

**Sympt√¥mes** : L'API ne peut pas se connecter √† Redis

**Solution** :

```bash
# V√©rifier que Redis tourne
docker ps | grep redis

# V√©rifier les logs
docker logs scraper-redis --tail 50

# Tester la connexion (remplacer par le vrai mot de passe)
docker exec scraper-redis redis-cli -a YOUR_REDIS_PASSWORD ping

# R√©sultat attendu : PONG

# Recr√©er Redis si n√©cessaire
docker-compose -f docker-compose.production.yml down redis
docker volume rm scraper-pro_redis_data
docker-compose -f docker-compose.production.yml up -d redis
```

### Probl√®me : Dashboard inaccessible

**Sympt√¥mes** : Erreur 502 Bad Gateway ou page blanche

**Solution** :

```bash
# V√©rifier que le container tourne
docker ps | grep dashboard

# V√©rifier les logs
docker logs scraper-dashboard --tail 50

# Attendre 1-2 minutes (Streamlit prend du temps √† d√©marrer)

# V√©rifier Nginx
nginx -t
systemctl status nginx

# V√©rifier les logs Nginx
tail -f /var/log/nginx/error.log

# Restart le dashboard
docker restart scraper-dashboard

# Attendre 1 minute puis tester
curl -I http://localhost:8501
```

### Probl√®me : D√©duplication ne fonctionne pas

**Sympt√¥mes** : Les m√™mes URLs sont scrap√©es plusieurs fois

**Solution** :

```bash
# V√©rifier que les tables existent
docker exec scraper-postgres psql -U scraper_admin -d scraper_db -c "\dt"

# Vous devez voir :
# - url_deduplication_cache
# - content_hash_cache

# V√©rifier Redis
docker exec scraper-redis redis-cli -a YOUR_PASSWORD KEYS "dedup:*"

# V√©rifier les variables d'environnement
docker exec scraper-app env | grep DEDUP

# R√©sultat attendu :
# DEDUP_URL_TTL_DAYS=30
# DEDUP_EMAIL_GLOBAL=true
# DEDUP_CONTENT_HASH_ENABLED=true
# DEDUP_URL_NORMALIZE=true
```

### Probl√®me : Espace disque plein

**Sympt√¥mes** : Erreur "no space left on device"

**Solution** :

```bash
# V√©rifier l'espace disque
df -h

# Voir quel r√©pertoire utilise le plus d'espace
du -sh /var/lib/docker/*

# Nettoyer les images Docker non utilis√©es
docker system prune -a --volumes

# Nettoyer les backups anciens (> 30 jours)
find /home/scraper/backups -name "*.sql.gz" -mtime +30 -delete

# Nettoyer les logs
docker-compose -f docker-compose.production.yml logs --tail=0 -f
```

### Probl√®me : SSL certificate error

**Sympt√¥mes** : Erreur "certificate expired" ou "invalid certificate"

**Solution** :

```bash
# V√©rifier les certificats
certbot certificates

# Renouveler manuellement
certbot renew --force-renewal

# Red√©marrer Nginx
systemctl restart nginx
```

### Probl√®me : Job de scraping √©choue

**Sympt√¥mes** : Job passe en status "failed"

**Solution** :

```bash
# Voir les logs du job
docker logs scraper-app | grep "job_id=123"

# Causes courantes :
# 1. URL invalide
# 2. Timeout (augmenter DOWNLOAD_TIMEOUT dans .env)
# 3. Site bloque le scraping (ajouter un proxy)

# Relancer le job via l'API
curl -X POST http://localhost:8000/api/v1/scraping/jobs/123/resume
```

---

## ‚úÖ 13. CHECKLIST FINALE

Avant de mettre en production, v√©rifiez que tout est OK :

### Infrastructure

- [ ] Serveur Hetzner CPX31 provisionn√© et accessible via SSH
- [ ] Firewall configur√© (ports 22, 80, 443 ouverts)
- [ ] Nom de domaine configur√© avec DNS (ou acc√®s par IP si pas de domaine)
- [ ] Docker et Docker Compose install√©s

### D√©ploiement

- [ ] Repository clon√© dans `/opt/scraper-pro`
- [ ] Fichier `.env` cr√©√© avec des secrets forts (32+ caract√®res)
- [ ] Permissions `.env` configur√©es (`chmod 600`)
- [ ] 8 containers d√©marr√©s et "Up" (`docker ps`)
- [ ] API accessible et r√©pond "ok" (`curl http://localhost:8000/health`)
- [ ] PostgreSQL accessible (`docker exec scraper-postgres pg_isready`)
- [ ] Redis accessible (`docker exec scraper-redis redis-cli ping`)

### Web Access

- [ ] Nginx install√© et configur√©
- [ ] Certificats SSL install√©s (Let's Encrypt)
- [ ] Dashboard accessible via HTTPS (`https://dashboard.example.com`)
- [ ] API accessible via HTTPS (`https://api.example.com`)
- [ ] Grafana accessible via HTTPS (`https://grafana.example.com`)
- [ ] Login Dashboard fonctionne (password depuis `.env`)

### Fonctionnalit√©s

- [ ] Job de test lanc√© et termin√© avec succ√®s
- [ ] Contacts extraits visibles dans le Dashboard
- [ ] D√©duplication active (stats visibles dans Dashboard)
- [ ] Grafana montre les m√©triques en temps r√©el
- [ ] Pas d'erreurs dans les logs (`docker-compose logs`)

### Maintenance

- [ ] Backup automatique configur√© (cron √† 2h du matin)
- [ ] Backup test√© et fichier `.sql.gz` cr√©√©
- [ ] Renouvellement SSL automatique configur√© (cron certbot)
- [ ] Rotation des logs Docker configur√©e
- [ ] Alertes email Grafana configur√©es (optionnel)

### S√©curit√©

- [ ] Tous les mots de passe sont forts (32+ caract√®res)
- [ ] Fichier `.env` en mode 600 (lisible seulement par vous)
- [ ] Firewall UFW activ√©
- [ ] PostgreSQL et Redis accessibles UNIQUEMENT depuis localhost
- [ ] Dashboard prot√©g√© par mot de passe
- [ ] API prot√©g√©e par HMAC signature

---

## üéâ F√âLICITATIONS !

**Votre syst√®me Scraper-Pro est maintenant 100% op√©rationnel en production !**

### Prochaines √©tapes recommand√©es

1. **Configurer MailWizz** : Ajouter vos cl√©s API dans `.env` pour l'injection automatique
2. **Tester plusieurs jobs** : V√©rifier la stabilit√© sur 100+ URLs
3. **Configurer les alertes email** : √ätre notifi√© en cas de probl√®me
4. **Documenter vos listes MailWizz** : Mapper les cat√©gories ‚Üí listes dans `config/mailwizz_routing.json`
5. **Planifier la migration Google** : Quand vous serez pr√™t (budget proxies + SerpAPI)

### Support et Documentation

- üìñ **README.md** : Vue d'ensemble du projet
- üèóÔ∏è **docs/ARCHITECTURE.md** : Architecture technique compl√®te
- üîå **docs/API.md** : Documentation API REST
- üîí **docs/DEDUPLICATION_SYSTEM.md** : Syst√®me de d√©duplication (700+ lignes)
- üöÄ **QUICK_START.md** : Guide de d√©marrage rapide

### Besoin d'aide ?

- üêõ **Issues GitHub** : Signaler un bug
- üìß **Email** : support@sos-expat.com
- üí¨ **Slack** : #scraper-pro

---

**Bon scraping !** üöÄ

---

**Scraper-Pro v2.0.0 - Ultra-Professional System**
Made with ‚ù§Ô∏è by the SOS-Expat Tech Team
