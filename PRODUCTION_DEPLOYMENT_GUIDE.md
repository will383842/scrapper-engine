# Scraper-Pro - Guide de DÃ©ploiement Production

**Version:** 2.0.0
**Date:** 2026-02-13
**Serveur:** Hetzner CPX31 (4 vCPU, 8GB RAM, 160GB SSD)
**Mode:** URLs Only (PAS de proxies, PAS de Google)

---

## Table des MatiÃ¨res

1. [Vue d'Ensemble](#vue-densemble)
2. [PrÃ©requis](#prÃ©requis)
3. [Installation Automatique](#installation-automatique)
4. [Configuration Manuelle](#configuration-manuelle)
5. [SÃ©curitÃ©](#sÃ©curitÃ©)
6. [Monitoring](#monitoring)
7. [Maintenance](#maintenance)
8. [Troubleshooting](#troubleshooting)

---

## Vue d'Ensemble

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NGINX (Reverse Proxy)                    â”‚
â”‚                  SSL/TLS (Let's Encrypt)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard    â”‚                         â”‚   Grafana       â”‚
â”‚  (Streamlit)   â”‚                         â”‚  (Monitoring)   â”‚
â”‚   Port: 8501   â”‚                         â”‚   Port: 3000    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â”‚                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Scraper API (FastAPI)                   â”‚
â”‚                        Port: 8000                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   PostgreSQL 16  â”‚  â”‚    Redis 7       â”‚
          â”‚   Port: 5432     â”‚  â”‚   Port: 6379     â”‚
          â”‚   (Dedup Cache)  â”‚  â”‚   (Job Queue)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Prometheus + Loki Stack   â”‚
          â”‚   (Metrics + Logs)          â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Services Docker

| Service | Container | Ports | RAM | CPU | Description |
|---------|-----------|-------|-----|-----|-------------|
| PostgreSQL | scraper-postgres | 5432 | 2GB | 1.5 | Base de donnÃ©es principale |
| Redis | scraper-redis | 6379 | 1GB | 0.5 | Cache + Deduplication |
| Scraper API | scraper-app | 8000 | 3GB | 2.0 | FastAPI + Scrapy |
| Dashboard | scraper-dashboard | 8501 | 1GB | 0.5 | Streamlit Premium UI |
| Prometheus | scraper-prometheus | 9090 | 512MB | 0.5 | MÃ©triques |
| Grafana | scraper-grafana | 3000 | 512MB | 0.5 | Visualisation |
| Loki | scraper-loki | 3100 | 512MB | 0.5 | Logs agrÃ©gÃ©s |
| Promtail | scraper-promtail | - | 256MB | 0.25 | Log collector |
| Alertmanager | scraper-alertmanager | 9093 | 256MB | 0.25 | Alertes |

**Total RAM utilisÃ©e:** ~7.5GB sur 8GB disponibles (6% rÃ©servÃ© systÃ¨me)

---

## PrÃ©requis

### Serveur

- **OS:** Ubuntu 22.04 LTS ou Debian 12+
- **CPU:** 4 vCPU minimum
- **RAM:** 8GB minimum
- **Stockage:** 160GB SSD (NVMe recommandÃ©)
- **RÃ©seau:** 1 Gbit/s (20TB/mois inclus)

### AccÃ¨s

```bash
# SSH avec clÃ© publique (PAS de mot de passe)
ssh root@your-server-ip

# CrÃ©er un utilisateur non-root
adduser scraper
usermod -aG sudo scraper
su - scraper
```

### Logiciels requis

- Docker Engine 24+
- Docker Compose v2.24+
- Git
- UFW (firewall)
- Nginx (pour reverse proxy)
- Certbot (pour SSL Let's Encrypt)

---

## Installation Automatique

### Ã‰tape 1: Cloner le projet

```bash
cd /home/scraper
git clone https://github.com/YOUR_REPO/scraper-pro.git
cd scraper-pro
```

### Ã‰tape 2: Lancer le script d'initialisation

```bash
# Installation complÃ¨te automatique (RECOMMANDÃ‰)
bash scripts/init-production.sh

# Options disponibles:
bash scripts/init-production.sh --skip-secrets  # Utiliser .env existant
bash scripts/init-production.sh --no-firewall   # Skip UFW config
bash scripts/init-production.sh --dry-run       # Check only
```

Le script effectue:

1. âœ… VÃ©rification des prÃ©requis (Docker, Docker Compose)
2. âœ… GÃ©nÃ©ration des secrets sÃ©curisÃ©s (PostgreSQL, Redis, API, Grafana)
3. âœ… CrÃ©ation du fichier `.env` depuis `.env.production`
4. âœ… Configuration du firewall UFW (ports 22, 80, 443)
5. âœ… Pull des images Docker
6. âœ… Build des images d'application
7. âœ… DÃ©marrage des services
8. âœ… Health checks automatiques
9. âœ… Sauvegarde des secrets dans `~/.scraper-pro-secrets-*.txt`

**DurÃ©e estimÃ©e:** 5-10 minutes

### Ã‰tape 3: Sauvegarder les secrets

```bash
# Les secrets sont sauvegardÃ©s dans un fichier temporaire
cat ~/.scraper-pro-secrets-*.txt

# IMPORTANT: Copier dans un gestionnaire de mots de passe
# Puis supprimer le fichier
rm ~/.scraper-pro-secrets-*.txt
```

### Ã‰tape 4: Configurer MailWizz et Webhooks

```bash
nano .env

# Mettre Ã  jour ces variables:
MAILWIZZ_SOS_EXPAT_API_KEY=your_real_api_key
MAILWIZZ_ULIXAI_API_KEY=your_real_api_key
WEBHOOK_SOS_EXPAT_SECRET=shared_secret_with_sos_expat
WEBHOOK_ULIXAI_SECRET=shared_secret_with_ulixai

# RedÃ©marrer les services
docker-compose -f docker-compose.production.yml restart
```

---

## Configuration Manuelle

### 1. GÃ©nÃ©rer les secrets

```bash
# PostgreSQL password (32 chars)
openssl rand -base64 32 | tr -d "=+/" | cut -c1-32

# Redis password (32 chars)
openssl rand -base64 32 | tr -d "=+/" | cut -c1-32

# API HMAC secret (64 chars)
openssl rand -base64 64 | tr -d "=+/" | cut -c1-64

# Dashboard password (24 chars)
openssl rand -base64 24 | tr -d "=+/" | cut -c1-24

# Grafana password (24 chars)
openssl rand -base64 24 | tr -d "=+/" | cut -c1-24
```

### 2. CrÃ©er le fichier .env

```bash
cp .env.production .env
chmod 600 .env
nano .env

# Remplacer TOUS les "CHANGE_ME" par les secrets gÃ©nÃ©rÃ©s
```

### 3. DÃ©marrer les services

```bash
# Pull images
docker-compose -f docker-compose.production.yml pull

# Build images
docker-compose -f docker-compose.production.yml build --no-cache

# Start services
docker-compose -f docker-compose.production.yml up -d

# VÃ©rifier les logs
docker-compose -f docker-compose.production.yml logs -f
```

### 4. VÃ©rifier le statut

```bash
# Statut des containers
docker ps

# Health checks
curl http://localhost:8000/health          # API
curl http://localhost:8501                 # Dashboard
curl http://localhost:3000/api/health      # Grafana
curl http://localhost:9090/-/healthy       # Prometheus
```

---

## SÃ©curitÃ©

### Firewall UFW

```bash
# Activer UFW
sudo ufw enable

# Autoriser SSH (CRITIQUE - ne pas oublier!)
sudo ufw allow 22/tcp

# Autoriser HTTP/HTTPS (pour Nginx)
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# VÃ©rifier le statut
sudo ufw status verbose
```

**IMPORTANT:** Les services internes (8000, 8501, 3000, etc.) sont bindÃ©s sur `127.0.0.1` uniquement et **NE SONT PAS** exposÃ©s Ã  Internet.

### Nginx Reverse Proxy

```bash
# Installer Nginx
sudo apt install nginx

# CrÃ©er la configuration
sudo nano /etc/nginx/sites-available/scraper-pro
```

```nginx
# Dashboard (Streamlit)
server {
    listen 80;
    server_name dashboard.your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:8501;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_read_timeout 86400;
    }
}

# Grafana
server {
    listen 80;
    server_name monitoring.your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:3000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

```bash
# Activer la config
sudo ln -s /etc/nginx/sites-available/scraper-pro /etc/nginx/sites-enabled/

# Tester la config
sudo nginx -t

# Recharger Nginx
sudo systemctl reload nginx
```

### SSL avec Let's Encrypt

```bash
# Installer Certbot
sudo apt install certbot python3-certbot-nginx

# Obtenir les certificats SSL
sudo certbot --nginx -d dashboard.your-domain.com -d monitoring.your-domain.com

# Renouvellement automatique (cron job dÃ©jÃ  crÃ©Ã© par Certbot)
sudo certbot renew --dry-run
```

### Fail2ban (Protection SSH)

```bash
# Installer Fail2ban
sudo apt install fail2ban

# CrÃ©er la configuration locale
sudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local
sudo nano /etc/fail2ban/jail.local

# Activer la protection SSH
[sshd]
enabled = true
port = 22
maxretry = 3
bantime = 3600

# RedÃ©marrer Fail2ban
sudo systemctl restart fail2ban

# VÃ©rifier le statut
sudo fail2ban-client status sshd
```

### Permissions des fichiers

```bash
# .env doit Ãªtre lisible uniquement par le propriÃ©taire
chmod 600 .env

# VÃ©rifier
ls -la .env
# Doit afficher: -rw------- 1 scraper scraper ...
```

---

## Monitoring

### AccÃ¨s Grafana

1. **URL:** `https://monitoring.your-domain.com`
2. **Username:** `admin`
3. **Password:** Voir le fichier de secrets gÃ©nÃ©rÃ©

### Dashboards Disponibles

1. **Scraper-Pro Production Dashboard** (`scraper-production.json`)
   - URLs scrapÃ©es (total + taux)
   - Emails extraits (total + taux)
   - CPU/RAM usage
   - PostgreSQL/Redis stats
   - Deduplication stats
   - HTTP response codes
   - Request duration (p95, p99)
   - Service health

### Prometheus Queries

```promql
# URLs scrapÃ©es par minute
rate(scraper_total_urls_scraped[5m]) * 60

# Emails extraits par minute
rate(scraper_total_emails_extracted[5m]) * 60

# CPU usage
100 * (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])))

# Memory usage
100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))

# PostgreSQL connections
pg_stat_database_numbackends{datname="scraper_db"}

# Redis keys count
redis_db_keys{db="db0"}

# Deduplication stats
scraper_dedup_urls_blocked
scraper_dedup_emails_blocked
```

### Alertes Prometheus

Fichier: `monitoring/alertmanager/alertmanager.yml`

Alertes configurÃ©es:

- **HighCPUUsage:** CPU > 80% pendant 5 minutes
- **HighMemoryUsage:** RAM > 85% pendant 5 minutes
- **HighDiskUsage:** Disque > 85%
- **PostgreSQLDown:** PostgreSQL indisponible
- **RedisDown:** Redis indisponible
- **ScraperAPIDown:** API indisponible
- **SlowScrapingRate:** < 10 URLs/minute pendant 15 minutes

### Logs

```bash
# Logs de tous les services
docker-compose -f docker-compose.production.yml logs -f

# Logs d'un service spÃ©cifique
docker logs scraper-app -f
docker logs scraper-postgres -f
docker logs scraper-redis -f

# Logs dans Grafana Loki
# AccÃ©der Ã  Grafana > Explore > Loki
# Query: {container_name="scraper-app"}
```

---

## Maintenance

### Backups PostgreSQL

```bash
# Backup manuel
bash scripts/backup-postgres.sh

# Restauration
bash scripts/restore-postgres.sh /path/to/backup.sql.gz

# Backup automatique quotidien (cron)
crontab -e

# Ajouter cette ligne (backup Ã  2h du matin)
0 2 * * * /home/scraper/scraper-pro/scripts/backup-postgres.sh
```

### Backups Redis

```bash
# Redis sauvegarde automatiquement (RDB + AOF)
# Fichiers dans le volume redis_data

# Backup manuel
docker exec scraper-redis redis-cli -a "$REDIS_PASSWORD" BGSAVE

# VÃ©rifier le dernier backup
docker exec scraper-redis redis-cli -a "$REDIS_PASSWORD" LASTSAVE
```

### Mises Ã  jour

```bash
# ArrÃªter les services
docker-compose -f docker-compose.production.yml stop

# Mettre Ã  jour le code
git pull origin main

# Rebuild les images
docker-compose -f docker-compose.production.yml build --no-cache

# RedÃ©marrer
docker-compose -f docker-compose.production.yml up -d

# VÃ©rifier les logs
docker-compose -f docker-compose.production.yml logs -f
```

### Nettoyage

```bash
# Nettoyer les images Docker inutilisÃ©es
docker system prune -a --volumes

# Nettoyer les logs Docker
sudo sh -c 'truncate -s 0 /var/lib/docker/containers/*/*-json.log'

# VACUUM PostgreSQL (automatique mais peut Ãªtre forcÃ©)
docker exec scraper-postgres psql -U scraper_admin -d scraper_db -c "VACUUM ANALYZE;"

# Nettoyer Redis (supprimer les clÃ©s expirÃ©es)
docker exec scraper-redis redis-cli -a "$REDIS_PASSWORD" --scan --pattern "*" | xargs -L 1 redis-cli -a "$REDIS_PASSWORD" DEL
```

### Rotation des logs

Fichier: `/etc/logrotate.d/scraper-pro`

```logrotate
/home/scraper/scraper-pro/logs/*.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
    create 0644 scraper scraper
}
```

---

## Troubleshooting

### ProblÃ¨me: PostgreSQL ne dÃ©marre pas

**Cause:** Permissions incorrectes sur le volume

```bash
# VÃ©rifier les logs
docker logs scraper-postgres

# Supprimer le volume et recrÃ©er
docker-compose -f docker-compose.production.yml down -v
docker-compose -f docker-compose.production.yml up -d postgres
```

### ProblÃ¨me: Redis nÃ©cessite un mot de passe

**Cause:** REDIS_PASSWORD non dÃ©fini ou incorrect

```bash
# VÃ©rifier la variable
docker exec scraper-redis env | grep REDIS_PASSWORD

# Tester la connexion
docker exec scraper-redis redis-cli -a "$REDIS_PASSWORD" PING
# Doit retourner: PONG
```

### ProblÃ¨me: API health check Ã©choue

**Cause:** Service non dÃ©marrÃ© ou erreur de configuration

```bash
# VÃ©rifier les logs
docker logs scraper-app -f

# VÃ©rifier les variables d'environnement
docker exec scraper-app env | grep POSTGRES
docker exec scraper-app env | grep REDIS

# RedÃ©marrer le service
docker-compose -f docker-compose.production.yml restart scraper
```

### ProblÃ¨me: Dashboard Streamlit ne charge pas

**Cause:** Port 8501 non accessible ou service crashÃ©

```bash
# VÃ©rifier le statut
docker ps | grep dashboard

# VÃ©rifier les logs
docker logs scraper-dashboard -f

# RedÃ©marrer
docker-compose -f docker-compose.production.yml restart dashboard
```

### ProblÃ¨me: Grafana ne se connecte pas Ã  Prometheus

**Cause:** Data source non configurÃ©e

```bash
# AccÃ©der Ã  Grafana
# Configuration > Data Sources > Add data source > Prometheus

# URL: http://prometheus:9090
# Access: Server (default)
# Save & Test
```

### ProblÃ¨me: Scraping rate trop faible

**Cause:** Limites de rate limiting ou configuration sous-optimale

```bash
# Ajuster dans .env
CONCURRENT_REQUESTS=32              # Augmenter (16 -> 32)
CONCURRENT_REQUESTS_PER_DOMAIN=8    # Augmenter (4 -> 8)
DOWNLOAD_DELAY=0.5                  # RÃ©duire (1.0 -> 0.5)

# RedÃ©marrer
docker-compose -f docker-compose.production.yml restart scraper
```

### ProblÃ¨me: Out of Memory

**Cause:** Trop de services ou fuites mÃ©moire

```bash
# VÃ©rifier l'utilisation mÃ©moire
docker stats

# Identifier le coupable
docker stats --no-stream --format "table {{.Name}}\t{{.MemUsage}}"

# RÃ©duire les limites dans docker-compose.production.yml
# RedÃ©marrer les services gourmands
```

---

## Performance Attendue

### Mode URLs Only (CPX31, 4 vCPU, 8GB RAM)

| MÃ©trique | Valeur |
|----------|--------|
| **URLs/minute** | 50-100 |
| **URLs/heure** | 3,000-6,000 |
| **URLs/jour** | 70,000-150,000 |
| **Emails/jour** | 10,000-30,000 (dÃ©pend des sites) |
| **CPU moyen** | 40-60% |
| **RAM moyenne** | 6-7GB / 8GB |
| **Stockage/jour** | 500MB-1GB (logs + donnÃ©es) |
| **Bande passante/jour** | 5-10GB |

### CoÃ»ts Mensuels EstimÃ©s

| Composant | CoÃ»t |
|-----------|------|
| **Hetzner CPX31** | â‚¬10.49/mois (~$11.50) |
| **Domaine + SSL** | â‚¬10/an (~$1/mois) |
| **Backups offsite** | â‚¬5-10/mois (optionnel) |
| **Total** | **~$15-20/mois** |

**Remarque:** Aucun coÃ»t de proxies car mode URLs Only (Ã©conomie de $500-2000/mois!)

---

## Commandes Utiles

```bash
# â”€â”€â”€ Docker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Voir tous les containers
docker ps

# Logs en temps rÃ©el
docker-compose -f docker-compose.production.yml logs -f

# RedÃ©marrer un service
docker-compose -f docker-compose.production.yml restart scraper

# ArrÃªter tous les services
docker-compose -f docker-compose.production.yml stop

# DÃ©marrer tous les services
docker-compose -f docker-compose.production.yml start

# Reconstruire et redÃ©marrer
docker-compose -f docker-compose.production.yml up -d --build

# â”€â”€â”€ PostgreSQL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# AccÃ©der au shell PostgreSQL
docker exec -it scraper-postgres psql -U scraper_admin -d scraper_db

# VÃ©rifier la taille de la DB
docker exec scraper-postgres psql -U scraper_admin -d scraper_db -c "SELECT pg_size_pretty(pg_database_size('scraper_db'));"

# Lister les tables
docker exec scraper-postgres psql -U scraper_admin -d scraper_db -c "\dt"

# â”€â”€â”€ Redis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# AccÃ©der au CLI Redis
docker exec -it scraper-redis redis-cli -a "$REDIS_PASSWORD"

# VÃ©rifier le nombre de clÃ©s
docker exec scraper-redis redis-cli -a "$REDIS_PASSWORD" DBSIZE

# Info mÃ©moire
docker exec scraper-redis redis-cli -a "$REDIS_PASSWORD" INFO memory

# â”€â”€â”€ Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# CPU/RAM usage en temps rÃ©el
docker stats

# Espace disque
df -h

# Trafic rÃ©seau
docker exec scraper-app cat /proc/net/dev

# â”€â”€â”€ Logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Logs d'un service
docker logs scraper-app -f --tail 100

# Logs systÃ¨me
journalctl -u docker -f

# â”€â”€â”€ SÃ©curitÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Statut UFW
sudo ufw status verbose

# Bannissements Fail2ban
sudo fail2ban-client status sshd

# Connexions SSH actives
who
```

---

## Support et Documentation

### Documentation

- **README.md** - Vue d'ensemble du projet
- **ULTRA_PRO_SYSTEM_READY.md** - Architecture complÃ¨te
- **docs/DEDUPLICATION_SYSTEM.md** - SystÃ¨me de dÃ©duplication
- **FAQ_CRITIQUE.md** - Questions frÃ©quentes
- **DEPLOYMENT_PRODUCTION.md** - Guide de dÃ©ploiement (ancien)

### Logs de DÃ©ploiement

- **FINAL_STATUS.md** - Ã‰tat final du projet
- **PRODUCTION_READINESS_GAPS.md** - Ã‰carts de production

### Contact

Pour toute question ou problÃ¨me:

1. Consulter la FAQ: `FAQ_CRITIQUE.md`
2. VÃ©rifier les logs: `docker-compose logs -f`
3. Consulter Grafana: `https://monitoring.your-domain.com`

---

## Checklist de DÃ©ploiement

Avant de marquer le dÃ©ploiement comme terminÃ©, vÃ©rifier:

### SÃ©curitÃ©

- [ ] Firewall UFW activÃ© (ports 22, 80, 443)
- [ ] Services bindÃ©s sur 127.0.0.1 uniquement
- [ ] Nginx reverse proxy configurÃ© avec SSL
- [ ] Let's Encrypt configurÃ© et auto-renouvelable
- [ ] Fail2ban activÃ© pour SSH
- [ ] Permissions `.env` = 600
- [ ] Secrets sauvegardÃ©s dans gestionnaire de mots de passe
- [ ] Fichier de secrets temporaire supprimÃ©

### Configuration

- [ ] Toutes les variables `.env` configurÃ©es (pas de CHANGE_ME)
- [ ] MailWizz API keys configurÃ©es
- [ ] Webhook secrets partagÃ©s avec SOS-Expat/Ulixai
- [ ] Mode scraping = `urls_only`
- [ ] Deduplication activÃ©e (ULTRA mode)

### Services

- [ ] PostgreSQL: healthy
- [ ] Redis: healthy
- [ ] Scraper API: healthy
- [ ] Dashboard: accessible
- [ ] Prometheus: healthy
- [ ] Grafana: accessible
- [ ] Loki: healthy
- [ ] Alertmanager: configurÃ©

### Monitoring

- [ ] Dashboard Grafana accessible via HTTPS
- [ ] Data sources Prometheus et Loki configurÃ©es
- [ ] Dashboard "Scraper-Pro Production" importÃ©
- [ ] Alertes configurÃ©es et testÃ©es
- [ ] Logs visibles dans Loki

### Backups

- [ ] Backup PostgreSQL manuel testÃ©
- [ ] Restauration PostgreSQL testÃ©e
- [ ] Cron job backup quotidien configurÃ©
- [ ] Backups offsite configurÃ©s (optionnel)

### Performance

- [ ] Scraping rate conforme (50-100 URLs/min)
- [ ] CPU usage < 80%
- [ ] RAM usage < 85%
- [ ] Disk usage < 80%
- [ ] Pas d'erreurs dans les logs

### Documentation

- [ ] Guide de dÃ©ploiement lu et compris
- [ ] Commandes utiles mÃ©morisÃ©es
- [ ] Contact support disponible
- [ ] ProcÃ©dures de maintenance dÃ©finies

---

## Changelog

### Version 2.0.0 (2026-02-13)

- Configuration production optimale crÃ©Ã©e
- Script d'initialisation automatique (`init-production.sh`)
- Dashboard Grafana production (`scraper-production.json`)
- Configuration PostgreSQL optimisÃ©e (CPX31)
- Mode URLs Only activÃ© par dÃ©faut
- Deduplication ULTRA-PRO intÃ©grÃ©e
- Monitoring stack complet (Prometheus, Grafana, Loki)
- Guide de dÃ©ploiement exhaustif

---

**Fin du Guide de DÃ©ploiement Production**

Pour toute question, consulter `FAQ_CRITIQUE.md` ou les logs des services.

**Bon dÃ©ploiement! ðŸš€**
