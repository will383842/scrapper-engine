# üö® GAPS DE PRODUCTION - Scraper-Pro

## ‚ùå CE QUI MANQUE POUR √äTRE VRAIMENT PRODUCTION-READY

---

## 1Ô∏è‚É£ INFRASTRUCTURE √Ä ACHETER (OBLIGATOIRE)

### A. Serveur VPS/VDS (CRITIQUE - 0% fait)

**Statut** : ‚ùå **NON ACHET√â - BLOQUANT**

**Pourquoi obligatoire ?**
- Le syst√®me tourne actuellement uniquement en LOCAL
- Docker n√©cessite un serveur 24/7 pour tourner en continu
- Impossible de faire du scraping continu sans serveur d√©di√©

**Ce qu'il faut acheter :**

| Provider | Plan | Prix/mois | Specs | Recommandation |
|----------|------|-----------|-------|----------------|
| **Hetzner** | CPX31 | ~12‚Ç¨ | 4 vCPU, 8GB RAM, 160GB SSD | ‚≠ê **MEILLEUR RAPPORT QUALIT√â/PRIX** |
| **Contabo** | VPS M | ~8‚Ç¨ | 4 vCPU, 8GB RAM, 200GB SSD | ‚úÖ √âconomique |
| **DigitalOcean** | CPU-Optimized 8GB | ~48$ | 4 vCPU, 8GB RAM, 100GB SSD | ‚úÖ Fiable mais cher |
| **OVH** | VPS Elite | ~14‚Ç¨ | 4 vCPU, 8GB RAM, 160GB NVMe | ‚úÖ Bon support |
| **AWS Lightsail** | 8GB | ~40$ | 2 vCPU, 8GB RAM, 160GB SSD | ‚ö†Ô∏è Cher |

**Recommandation finale** : **Hetzner CPX31** (12‚Ç¨/mois)
- Excellent r√©seau (1 Gbit/s)
- 20 TB de trafic inclus
- Datacenter Allemagne (proche de la France)
- Fiabilit√© excellente
- Snapshots gratuits

**Actions requises :**
```bash
# 1. Acheter le VPS
# 2. Configurer SSH
# 3. Installer Docker + Docker Compose
# 4. Cloner le repo
# 5. Configurer .env
# 6. D√©ployer : docker-compose up -d
```

**Co√ªt annuel** : ~150‚Ç¨/an

---

### B. Proxies Rotatifs (CRITIQUE - 0% actifs)

**Statut** : ‚ùå **NON ACHET√âS - BLOQUANT POUR GOOGLE**

**Pourquoi OBLIGATOIRES ?**
- Google/Maps bloquent les IPs apr√®s 10-50 requ√™tes
- Sans proxies, blacklist garantie en 5 minutes
- Les proxies r√©sidentiels imitent de vrais utilisateurs

**Le code est pr√™t, mais AUCUN proxy actif !**

**Providers recommand√©s :**

#### Option 1 : **Oxylabs** (Premium - Recommand√©)
- **Plan** : Residential Proxies Starter
- **Prix** : ~300‚Ç¨/mois (50GB trafic)
- **Pool** : 100M+ IPs r√©sidentielles
- **Rotation** : Automatique par requ√™te
- **Success rate** : 99.9%
- **Support** : 24/7
- ‚≠ê **MEILLEUR POUR GOOGLE**
- üîó Code d√©j√† int√©gr√© dans `config/proxy_config.json`

#### Option 2 : **Bright Data** (Ex-Luminati)
- **Plan** : Pay-as-you-go
- **Prix** : ~500‚Ç¨/mois (40GB trafic)
- **Pool** : 72M+ IPs
- **Qualit√©** : Excellente
- ‚ö†Ô∏è Plus cher mais tr√®s fiable

#### Option 3 : **SmartProxy** (Budget)
- **Plan** : Residential 8GB
- **Prix** : ~75‚Ç¨/mois
- **Pool** : 40M+ IPs
- **Qualit√©** : Correcte pour du scraping l√©ger
- ‚ö†Ô∏è Success rate ~95% (moins bon que Oxylabs)

#### Option 4 : **Proxies GRATUITS** (‚ö†Ô∏è NON RECOMMAND√â)
- **Prix** : 0‚Ç¨
- **Qualit√©** : üí© TR√àS MAUVAISE
- **Probl√®mes** :
  - 90% ne marchent pas
  - Blacklist√©s par Google
  - Lents (>10s par requ√™te)
  - Risque de fuite d'IP
- ‚ùå **NE PAS UTILISER EN PRODUCTION**

**Recommandation finale** : **Oxylabs Residential Proxies**
- 300‚Ç¨/mois pour 50GB
- OU 1500‚Ç¨/an (√©conomie de 15%)
- Inclus : rotation auto, sticky sessions, geo-targeting

**Co√ªt annuel** : ~3600‚Ç¨/an (ou 1500‚Ç¨ si paiement annuel)

**Configuration requise :**
```bash
# Dans .env
PROXY_PROVIDER=oxylabs
PROXY_USER=customer-YOUR_USERNAME
PROXY_PASS=YOUR_PASSWORD

# Test de connexion
curl -x pr.oxylabs.io:7777 -U "customer-YOUR_USERNAME:YOUR_PASSWORD" https://ip-api.com/json
```

**Alternative √©conomique** :
- Commencer avec SmartProxy 8GB (75‚Ç¨/mois)
- Upgrader vers Oxylabs quand le volume augmente

---

### C. SerpAPI (Optionnel mais recommand√©)

**Statut** : ‚ö†Ô∏è **NON ACHET√â - FORTEMENT RECOMMAND√â**

**Pourquoi utile ?**
- Fallback quand Google d√©tecte un CAPTCHA
- API officielle de Google Search
- Pas de risque de blacklist
- Parsing d√©j√† fait

**Plan recommand√©** : **SerpAPI Starter**
- **Prix** : ~50$/mois (5000 recherches)
- **Inclus** : Google Search, Google Maps, Shopping, News
- **Rate limit** : 1 req/s
- üîó Code d√©j√† int√©gr√© dans `scraper/integrations/serpapi_client.py`

**Configuration :**
```bash
# Dans .env
SERPAPI_KEY=your_serpapi_key_here
```

**Co√ªt annuel** : ~600$/an

---

## 2Ô∏è‚É£ FONCTIONNALIT√âS MANQUANTES (CODE √Ä √âCRIRE)

### A. D√©tection de Blacklist (CRITIQUE - 0% impl√©ment√©)

**Statut** : ‚ùå **NON IMPL√âMENT√â - CRITIQUE**

**Probl√®me actuel :**
- Le syst√®me ne d√©tecte PAS si une IP/proxy est blacklist√©
- Continue √† envoyer des requ√™tes m√™me si bloqu√©
- Gaspille des cr√©dits proxy
- Pas d'alerte quand blacklist√©

**Ce qui doit √™tre impl√©ment√© :**

```python
# scraper/utils/blacklist_detector.py (√Ä CR√âER)

class BlacklistDetector:
    """D√©tecte si on est blacklist√© par Google/sites."""

    BLACKLIST_INDICATORS = [
        "captcha",
        "unusual traffic",
        "automated queries",
        "robot",
        "403 forbidden",
        "429 too many requests",
        "sorry, but your computer",
        "unusual activity",
        "recaptcha",
    ]

    def is_blacklisted(self, response: Response) -> bool:
        """V√©rifie si la r√©ponse indique un blacklist."""
        # Status codes suspects
        if response.status_code in [403, 429, 503]:
            return True

        # Mots-cl√©s dans le HTML
        html_lower = response.text.lower()
        for indicator in self.BLACKLIST_INDICATORS:
            if indicator in html_lower:
                logger.warning(f"Blacklist detected: {indicator}")
                return True

        # Redirection vers CAPTCHA
        if "google.com/sorry" in response.url:
            return True

        return False

    def trigger_fallback(self, proxy_url: str):
        """Actions quand blacklist d√©tect√©e."""
        # 1. Marquer le proxy comme blacklist√©
        self.mark_proxy_blacklisted(proxy_url)

        # 2. Passer au proxy suivant
        self.rotate_proxy()

        # 3. Fallback sur SerpAPI si disponible
        if os.getenv("SERPAPI_KEY"):
            self.use_serpapi_fallback()

        # 4. Alerte admin
        self.send_alert("Blacklist detected, switched to fallback")
```

**Int√©gration dans les spiders :**
```python
# Dans parse_search_results()

detector = BlacklistDetector()

if detector.is_blacklisted(response):
    logger.error("üö® BLACKLISTED! Switching to fallback...")
    detector.trigger_fallback(response.meta.get('proxy'))
    # Relancer la requ√™te avec nouveau proxy
    yield scrapy.Request(
        response.url,
        callback=self.parse_search_results,
        dont_filter=True,
        meta={'retry': True}
    )
    return
```

**Effort** : 4-6 heures de dev + tests

---

### B. Dashboard de Sant√© des Proxies (MANQUANT - 0% impl√©ment√©)

**Statut** : ‚ùå **NON IMPL√âMENT√â - IMPORTANT**

**Probl√®me actuel :**
- Aucune visibilit√© sur la sant√© des proxies
- Pas de stats de success rate par proxy
- Impossible de savoir quel proxy est blacklist√©

**Ce qui doit √™tre cr√©√© :**

**1. Table de tracking des proxies**
```sql
-- D√©j√† existe dans db/init.sql : proxy_stats
-- Mais pas utilis√©e !

SELECT proxy_url, success_rate, consecutive_failures, status
FROM proxy_stats
WHERE status = 'active'
ORDER BY success_rate DESC;
```

**2. Middleware de tracking**
```python
# scraper/utils/middlewares.py (√Ä AM√âLIORER)

class ProxyMiddleware:
    def process_response(self, request, response, spider):
        proxy = request.meta.get('proxy')

        # Enregistrer la statistique
        if response.status_code == 200:
            self.record_proxy_success(proxy)
        else:
            self.record_proxy_failure(proxy)

        # Auto-blacklist si trop d'√©checs
        if self.get_failure_rate(proxy) > 0.5:
            self.blacklist_proxy(proxy)
            logger.warning(f"Proxy auto-blacklisted: {proxy}")
```

**3. Onglet Dashboard Streamlit**
```python
# dashboard/app.py - Nouvel onglet "Proxies"

with tab_proxies:
    st.header("Proxy Health Monitor")

    proxies = query_df("""
        SELECT proxy_url, proxy_type, provider,
               total_requests, successful_requests, failed_requests,
               success_rate, avg_response_ms, status, last_used_at
        FROM proxy_stats
        ORDER BY success_rate DESC
    """)

    # M√©triques cl√©s
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Active Proxies", len([p for p in proxies if p['status'] == 'active']))
    col2.metric("Blacklisted", len([p for p in proxies if p['status'] == 'blacklisted']))
    col3.metric("Avg Success Rate", f"{proxies['success_rate'].mean():.1f}%")
    col4.metric("Avg Response", f"{proxies['avg_response_ms'].mean():.0f}ms")

    # Table
    st.dataframe(proxies)
```

**Effort** : 6-8 heures de dev

---

### C. Scraping d'Annuaires (MANQUANT - 0% impl√©ment√©)

**Statut** : ‚ùå **NON IMPL√âMENT√â - FONCTIONNALIT√â MANQUANTE**

**Probl√®me actuel :**
- Le syst√®me scrappe seulement :
  - ‚úÖ Google Search
  - ‚úÖ Google Maps
  - ‚úÖ URLs personnalis√©es
  - ‚úÖ Blogs
- **MAIS PAS d'annuaires professionnels !**

**Annuaires importants √† ajouter :**

1. **Pages Jaunes** (pagesjaunes.fr)
2. **118712** (annuaire invers√©)
3. **Yelp France**
4. **TripAdvisor** (restaurants, h√¥tels)
5. **Justacote** (professionnels)
6. **LinkedIn** (‚ö†Ô∏è difficile, n√©cessite authentification)

**Ce qui doit √™tre cr√©√© :**

```python
# scraper/spiders/pagesjaunes_spider.py (√Ä CR√âER)

class PagesJaunesSpider(scrapy.Spider):
    name = "pagesjaunes"

    def __init__(self, query="avocat", location="paris", max_results=100, **kwargs):
        super().__init__(**kwargs)
        self.query = query
        self.location = location
        self.max_results = int(max_results)

    def start_requests(self):
        # URL Pages Jaunes
        url = f"https://www.pagesjaunes.fr/annuaire/chercherlespros?quoiqui={self.query}&ou={self.location}&proximite=0"
        yield scrapy.Request(url, callback=self.parse)

    def parse(self, response):
        # Extraire les fiches
        for fiche in response.css('.bi-bloc'):
            item = ContactItem()
            item['name'] = fiche.css('.denomination-links::text').get()
            item['phone'] = fiche.css('.numero::text').get()
            item['address'] = fiche.css('.adresse::text').get()
            item['website'] = fiche.css('.bi-lien-site::attr(href)').get()
            item['source_type'] = 'pagesjaunes'
            item['source_url'] = response.url
            yield item

        # Pagination
        next_page = response.css('.pagination-next::attr(href)').get()
        if next_page and self.results_count < self.max_results:
            yield response.follow(next_page, callback=self.parse)
```

**Effort par annuaire** : 4-6 heures
**Total pour 5 annuaires** : 20-30 heures

---

### D. Auto-Throttling Intelligent (BASIQUE - 30% impl√©ment√©)

**Statut** : ‚ö†Ô∏è **PARTIELLEMENT IMPL√âMENT√â - √Ä AM√âLIORER**

**Ce qui existe d√©j√† :**
```python
# settings.py
DOWNLOAD_DELAY = 2.0  # D√©lai fixe entre requ√™tes
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_TARGET_CONCURRENCY = 2.0
```

**Probl√®me :**
- Le throttling est **STATIQUE**
- Pas d'adaptation selon le taux d'erreur
- Pas de ralentissement automatique si blacklist d√©tect√©e

**Ce qui doit √™tre ajout√© :**

```python
# scraper/utils/smart_throttle.py (√Ä CR√âER)

class SmartThrottleExtension:
    """Auto-ajuste la vitesse selon le taux d'erreur."""

    def __init__(self):
        self.error_rate_window = deque(maxlen=100)  # 100 derni√®res requ√™tes
        self.current_delay = 2.0  # D√©but √† 2s

    def adjust_delay(self):
        """Ajuste le d√©lai selon le taux d'erreur."""
        error_rate = sum(self.error_rate_window) / len(self.error_rate_window)

        if error_rate > 0.3:  # >30% d'erreurs
            # RALENTIR agressivement
            self.current_delay *= 2
            logger.warning(f"‚ö†Ô∏è High error rate ({error_rate:.0%}), slowing down to {self.current_delay}s")

        elif error_rate > 0.1:  # >10% d'erreurs
            # RALENTIR mod√©r√©ment
            self.current_delay *= 1.5
            logger.info(f"Moderate errors ({error_rate:.0%}), delay ‚Üí {self.current_delay}s")

        elif error_rate < 0.05:  # <5% d'erreurs
            # ACC√âL√âRER progressivement
            self.current_delay = max(1.0, self.current_delay * 0.9)
            logger.info(f"Low errors ({error_rate:.0%}), speeding up to {self.current_delay}s")

        # Limites
        self.current_delay = max(1.0, min(60.0, self.current_delay))

        return self.current_delay
```

**Int√©gration dans Scrapy :**
```python
# settings.py
EXTENSIONS = {
    'scraper.utils.smart_throttle.SmartThrottleExtension': 500,
}
```

**Effort** : 3-4 heures

---

### E. V√©rification du Syst√®me de Checkpoint (EXISTE - √Ä TESTER)

**Statut** : ‚úÖ **CODE EXISTE mais NON TEST√â EN PRODUCTION**

**Ce qui existe :**
```python
# scraper/utils/checkpoint.py
def save_checkpoint(job_id: int, data: dict):
    """Sauvegarde un checkpoint dans scraping_jobs.checkpoint_data."""

def load_checkpoint(job_id: int) -> dict:
    """Charge le checkpoint d'un job."""

# scraper/spiders/google_search_spider.py
def start_requests(self):
    start_offset = 0
    if self.resume and self.job_id:
        checkpoint = load_checkpoint(int(self.job_id))
        if checkpoint:
            start_offset = checkpoint.get("last_page", 0)
            logger.info(f"Resuming from page {start_offset}")
```

**Test √† faire :**
```bash
# 1. Lancer un job Google Search (max 100 r√©sultats)
curl -X POST http://localhost:8000/api/v1/scraping/jobs \
  -H "Content-Type: application/json" \
  -d '{"source_type":"google_search","name":"Test Checkpoint","config":{"query":"avocat paris","max_results":100}}'

# 2. Attendre qu'il scrape 50 r√©sultats
# 3. Arr√™ter manuellement : docker-compose stop scraper
# 4. V√©rifier le checkpoint dans la DB
docker-compose exec postgres psql -U scraper_admin -d scraper_db -c "SELECT id, checkpoint_data FROM scraping_jobs WHERE id = 1;"

# 5. Reprendre le job
curl -X POST http://localhost:8000/api/v1/scraping/jobs/1/resume

# 6. V√©rifier qu'il reprend √† partir de la page 50 (pas de 0)
```

**Si le test √©choue** : Corriger le code de checkpoint

**Effort** : 2 heures de tests

---

## 3Ô∏è‚É£ SYST√àME EN CONTINU (PARTIELLEMENT FAIT - 60%)

### A. Cron Jobs Automatiques (‚úÖ IMPL√âMENT√â)

**Statut** : ‚úÖ **CODE EXISTE - MAIS INACTIF SANS VPS**

**Ce qui existe :**
```bash
# crontab (dans le container scraper)
0 * * * * cd /app && python -m scraper.jobs.process_contacts  # Validation toutes les heures
30 * * * * cd /app && python -m scraper.jobs.sync_to_mailwizz  # Sync MailWizz +30min
```

**Probl√®me :**
- ‚ùå Ces crons tournent uniquement SI le serveur VPS tourne 24/7
- ‚ùå Actuellement INACTIFS (pas de serveur)

**Solution :**
- D√©ployer sur VPS ‚Üí crons s'activent automatiquement

---

### B. Contr√¥le de Cadence (‚ö†Ô∏è BASIQUE - √Ä AM√âLIORER)

**Statut** : ‚ö†Ô∏è **PARTIELLEMENT IMPL√âMENT√â**

**Ce qui existe :**
```python
# settings.py
DOWNLOAD_DELAY = 2.0  # 2 secondes entre requ√™tes
CONCURRENT_REQUESTS = 8  # 8 requ√™tes simultan√©es max
CONCURRENT_REQUESTS_PER_DOMAIN = 2  # 2 requ√™tes/domaine
AUTOTHROTTLE_ENABLED = True
```

**Ce qui manque :**
- ‚ùå Pas d'ajustement dynamique selon les erreurs
- ‚ùå Pas de "mode ralenti" automatique si blacklist d√©tect√©e
- ‚ùå Pas de limites par heure/jour

**√Ä impl√©menter :**
```python
# Nouvelle table : scraping_quotas
CREATE TABLE scraping_quotas (
    id SERIAL PRIMARY KEY,
    source_type VARCHAR(50),
    requests_per_hour INTEGER DEFAULT 500,
    requests_today INTEGER DEFAULT 0,
    last_reset TIMESTAMPTZ DEFAULT NOW(),
    throttle_mode VARCHAR(20) DEFAULT 'normal'  -- normal, slow, paused
);

# Middleware de contr√¥le
class QuotaMiddleware:
    def process_request(self, request, spider):
        quota = get_daily_quota(spider.source_type)

        if quota.requests_today >= quota.requests_per_hour * 24:
            raise IgnoreRequest("Daily quota exceeded")

        if quota.throttle_mode == 'slow':
            time.sleep(5)  # Ralentir √† 5s entre requ√™tes
        elif quota.throttle_mode == 'paused':
            raise IgnoreRequest("Scraping paused")
```

**Effort** : 4-6 heures

---

## 4Ô∏è‚É£ R√âCAPITULATIF DES GAPS

### Bloquants (CRITIQUE - syst√®me non fonctionnel)

| Gap | Statut | Impact | Co√ªt | Effort | Priorit√© |
|-----|--------|--------|------|--------|----------|
| **VPS/VDS** | ‚ùå 0% | üî¥ BLOQUANT | 12‚Ç¨/mois | 2h setup | P0 |
| **Proxies** | ‚ùå 0% | üî¥ BLOQUANT (Google) | 300‚Ç¨/mois | 1h config | P0 |
| **D√©tection Blacklist** | ‚ùå 0% | üî¥ CRITIQUE | 0‚Ç¨ | 6h dev | P0 |

### Importants (syst√®me fonctionne mais incomplet)

| Gap | Statut | Impact | Co√ªt | Effort | Priorit√© |
|-----|--------|--------|------|--------|----------|
| **SerpAPI** | ‚ùå 0% | üü† IMPORTANT | 50$/mois | 0h (d√©j√† int√©gr√©) | P1 |
| **Dashboard Proxies** | ‚ùå 0% | üü† IMPORTANT | 0‚Ç¨ | 8h dev | P1 |
| **Auto-Throttling** | ‚ö†Ô∏è 30% | üü† IMPORTANT | 0‚Ç¨ | 4h dev | P1 |
| **Annuaires** | ‚ùå 0% | üü° SOUHAITABLE | 0‚Ç¨ | 30h dev | P2 |
| **Test Checkpoints** | ‚ö†Ô∏è 80% | üü° SOUHAITABLE | 0‚Ç¨ | 2h tests | P2 |

---

## üí∞ BUDGET TOTAL N√âCESSAIRE

### Co√ªts d'Infrastructure (OBLIGATOIRES)

| Item | Provider | Prix/mois | Prix/an | Note |
|------|----------|-----------|---------|------|
| **VPS** | Hetzner CPX31 | 12‚Ç¨ | 144‚Ç¨ | OBLIGATOIRE |
| **Proxies** | Oxylabs Residential | 300‚Ç¨ | 3600‚Ç¨ | CRITIQUE pour Google |
| **SerpAPI** | Starter Plan | ~50‚Ç¨ | 600‚Ç¨ | Fallback recommand√© |
| **Domaine + SSL** | Namecheap + Let's Encrypt | 1‚Ç¨ | 12‚Ç¨ | Let's Encrypt gratuit |
| **Backups S3** | AWS S3 Standard-IA | ~5‚Ç¨ | 60‚Ç¨ | Optionnel (backup local OK) |
| **TOTAL** | | **~368‚Ç¨/mois** | **~4416‚Ç¨/an** | |

### Alternative Budget R√©duit

| Item | Provider | Prix/mois | Prix/an | Note |
|------|----------|-----------|---------|------|
| **VPS** | Contabo VPS M | 8‚Ç¨ | 96‚Ç¨ | Moins cher |
| **Proxies** | SmartProxy 8GB | 75‚Ç¨ | 900‚Ç¨ | Budget mais moins fiable |
| **SerpAPI** | - | 0‚Ç¨ | 0‚Ç¨ | Skipper au d√©but |
| **Domaine** | Namecheap | 1‚Ç¨ | 12‚Ç¨ | |
| **TOTAL** | | **~84‚Ç¨/mois** | **~1008‚Ç¨/an** | |

---

## ‚è±Ô∏è TEMPS DE D√âVELOPPEMENT MANQUANT

| T√¢che | Effort | Priorit√© |
|-------|--------|----------|
| D√©tection Blacklist | 6h | P0 |
| Dashboard Proxies | 8h | P1 |
| Auto-Throttling | 4h | P1 |
| Test Checkpoints | 2h | P2 |
| 5 Annuaires | 30h | P2 |
| **TOTAL P0-P1** | **20h** | |
| **TOTAL P0-P2** | **50h** | |

---

## üöÄ PLAN D'ACTION RECOMMAND√â

### Phase 1 : Infrastructure (Semaine 1)

```bash
# Jour 1 : Acheter infrastructure
- Acheter VPS Hetzner CPX31 (12‚Ç¨/mois)
- Acheter compte Oxylabs (300‚Ç¨/mois)
- Optionnel : SerpAPI Starter (50$/mois)

# Jour 2-3 : Setup VPS
- Configurer serveur (Docker, SSL)
- D√©ployer scraper-pro
- Configurer proxies dans .env

# Jour 4-5 : Tests infrastructure
- Test scraping Google avec proxies
- Test rotation des proxies
- Test checkpoint/resume

Co√ªt : ~368‚Ç¨ (premier mois)
Temps : 16h
```

### Phase 2 : Code Critique (Semaine 2)

```bash
# Jour 1-2 : D√©tection Blacklist (6h)
- Impl√©menter BlacklistDetector
- Int√©grer dans spiders
- Tests avec vraies requ√™tes Google

# Jour 3-4 : Dashboard Proxies (8h)
- Cr√©er onglet Proxies dans dashboard
- Tracking stats par proxy
- Auto-blacklist si failure rate > 50%

# Jour 5 : Auto-Throttling (4h)
- Impl√©menter SmartThrottleExtension
- Tests de ralentissement automatique

# Jour 6 : Tests Checkpoints (2h)
- Test resume apr√®s crash
- V√©rifier que √ßa ne recommence PAS de 0

Co√ªt : 0‚Ç¨ (d√©veloppement)
Temps : 20h
```

### Phase 3 : Fonctionnalit√©s (Semaines 3-4)

```bash
# Optionnel : Annuaires
- Spider Pages Jaunes (6h)
- Spider Yelp (6h)
- Spider 118712 (6h)
- Spider Justacote (6h)
- Spider TripAdvisor (6h)

Co√ªt : 0‚Ç¨
Temps : 30h
```

---

## ‚úÖ CHECKLIST FINALE PRODUCTION-READY

### Infrastructure (CRITIQUE)
- [ ] VPS/VDS achet√© et configur√©
- [ ] Proxies r√©sidentiels actifs (Oxylabs/SmartProxy)
- [ ] DNS configur√© (scraper.votre-domaine.com)
- [ ] SSL/TLS actif (Let's Encrypt)
- [ ] Backup automatique configur√©

### Code Critique
- [ ] D√©tection blacklist impl√©ment√©e
- [ ] Dashboard proxies fonctionnel
- [ ] Auto-throttling intelligent actif
- [ ] Checkpoints test√©s et valid√©s
- [ ] SerpAPI fallback configur√© (optionnel)

### Monitoring
- [x] Prometheus + Grafana configur√©s
- [x] Alertes blacklist configur√©es
- [ ] Dashboard proxies avec alertes
- [x] Logs centralis√©s (Loki)

### Tests End-to-End
- [ ] Test scraping Google (100 r√©sultats) avec proxies
- [ ] Test rotation automatique des proxies
- [ ] Test d√©tection + r√©cup√©ration blacklist
- [ ] Test checkpoint/resume apr√®s crash
- [ ] Test cron jobs automatiques

---

## üí° RECOMMANDATION FINALE

**Pour √™tre VRAIMENT production-ready :**

1. **ACHETER (obligatoire)** :
   - ‚úÖ VPS Hetzner CPX31 (12‚Ç¨/mois)
   - ‚úÖ Oxylabs Residential Proxies (300‚Ç¨/mois)
   - ‚ö†Ô∏è SerpAPI (optionnel, 50$/mois)

2. **D√âVELOPPER (20h critiques)** :
   - ‚úÖ D√©tection blacklist (6h)
   - ‚úÖ Dashboard proxies (8h)
   - ‚úÖ Auto-throttling (4h)
   - ‚úÖ Tests checkpoints (2h)

3. **BUDGET TOTAL** :
   - **Minimum** : 84‚Ç¨/mois (VPS + SmartProxy)
   - **Recommand√©** : 368‚Ç¨/mois (VPS + Oxylabs + SerpAPI)
   - **Annuel** : 1000‚Ç¨-4400‚Ç¨/an

**Sans VPS + Proxies = syst√®me NON FONCTIONNEL en production** ‚ùå

---

Voulez-vous que je vous aide √† impl√©menter ces fonctionnalit√©s manquantes ?
