# ========================================
# DOCKER COMPOSE OPTIMISÉ - 2 APPLICATIONS
# ========================================
# Configuration pour serveur 2 vCPU / 4 GB RAM
# Applications : Scraper-Pro + Backlink Engine
# Stratégie : Services partagés + Limites RAM
# ========================================

version: '3.8'

# ========================================
# RÉSEAUX
# ========================================
networks:
  shared-network:
    driver: bridge
    name: shared-apps-network

# ========================================
# VOLUMES
# ========================================
volumes:
  postgres_data:
    name: shared-postgres-data
  redis_data:
    name: shared-redis-data

# ========================================
# SERVICES PARTAGÉS
# ========================================
services:
  # ==========================================
  # PostgreSQL PARTAGÉ (2 bases distinctes)
  # ==========================================
  postgres:
    image: postgres:16-alpine
    container_name: shared-postgres
    restart: unless-stopped

    environment:
      POSTGRES_USER: ${POSTGRES_USER:-shared_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: scraper_db,backlink_db

    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/postgres-init.sh:/docker-entrypoint-initdb.d/init.sh:ro

    ports:
      - "5432:5432"

    networks:
      - shared-network

    # Limites RAM (max 500 MB)
    deploy:
      resources:
        limits:
          memory: 500M
        reservations:
          memory: 200M

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-shared_user}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================
  # Redis PARTAGÉ (namespaces séparés)
  # ==========================================
  redis:
    image: redis:7-alpine
    container_name: shared-redis
    restart: unless-stopped

    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 150mb
      --maxmemory-policy allkeys-lru
      --save 60 1000

    volumes:
      - redis_data:/data

    ports:
      - "6379:6379"

    networks:
      - shared-network

    # Limites RAM (max 150 MB)
    deploy:
      resources:
        limits:
          memory: 150M
        reservations:
          memory: 50M

    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

# ========================================
# SCRAPER-PRO (Application 1)
# ========================================

  # ==========================================
  # Scraper API (FastAPI)
  # ==========================================
  scraper-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraper-api
    restart: unless-stopped

    environment:
      # Database (PostgreSQL partagé, DB 1)
      DATABASE_URL: postgresql://${POSTGRES_USER:-shared_user}:${POSTGRES_PASSWORD}@postgres:5432/scraper_db

      # Redis (namespace 0)
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0

      # API
      API_HOST: 0.0.0.0
      API_PORT: 8000
      API_HMAC_SECRET: ${API_HMAC_SECRET}

      # Scraping (OPTIMISÉ pour 2 vCPU)
      PROXY_ENABLED: "false"
      DOWNLOAD_DELAY: "2.5"
      CONCURRENT_REQUESTS: "3"  # ⚠️ Réduit à 3
      CONCURRENT_REQUESTS_PER_DOMAIN: "1"

      # Throttling
      AUTOTHROTTLE_ENABLED: "true"
      AUTOTHROTTLE_START_DELAY: "2"
      AUTOTHROTTLE_MAX_DELAY: "30"
      SMART_THROTTLE_MIN_DELAY: "1.0"
      SMART_THROTTLE_MAX_DELAY: "60.0"

      # Logs
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    ports:
      - "8000:8000"

    networks:
      - shared-network

    # Limites RAM (max 400 MB)
    deploy:
      resources:
        limits:
          memory: 400M
          cpus: '0.8'
        reservations:
          memory: 200M
          cpus: '0.5'

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ==========================================
  # Scraper Worker (Scrapy)
  # ==========================================
  scraper-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraper-worker
    restart: unless-stopped

    command: ["python", "-m", "scraper.worker"]

    environment:
      # Database
      DATABASE_URL: postgresql://${POSTGRES_USER:-shared_user}:${POSTGRES_PASSWORD}@postgres:5432/scraper_db

      # Redis
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0

      # Scraping (OPTIMISÉ)
      PROXY_ENABLED: "false"
      DOWNLOAD_DELAY: "2.5"
      CONCURRENT_REQUESTS: "3"
      CONCURRENT_REQUESTS_PER_DOMAIN: "1"

      # Throttling
      AUTOTHROTTLE_ENABLED: "true"
      SMART_THROTTLE_MIN_DELAY: "1.0"
      SMART_THROTTLE_MAX_DELAY: "60.0"

      LOG_LEVEL: ${LOG_LEVEL:-INFO}

    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    networks:
      - shared-network

    # Limites RAM (max 700 MB)
    deploy:
      resources:
        limits:
          memory: 700M
          cpus: '1.0'
        reservations:
          memory: 400M
          cpus: '0.6'

  # ==========================================
  # Dashboard Streamlit
  # ==========================================
  scraper-dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: scraper-dashboard
    restart: unless-stopped

    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-shared_user}:${POSTGRES_PASSWORD}@postgres:5432/scraper_db
      DASHBOARD_PASSWORD: ${DASHBOARD_PASSWORD}

    depends_on:
      postgres:
        condition: service_healthy

    ports:
      - "8501:8501"

    networks:
      - shared-network

    # Limites RAM (max 350 MB)
    deploy:
      resources:
        limits:
          memory: 350M
          cpus: '0.5'
        reservations:
          memory: 150M
          cpus: '0.3'

# ========================================
# BACKLINK ENGINE (Application 2)
# ========================================

  # ==========================================
  # Backlink Engine App
  # ==========================================
  backlink-engine:
    # ⚠️ Modifier selon votre image Docker Backlink Engine
    image: your-backlink-engine:latest
    # OU si vous buildez depuis un Dockerfile :
    # build:
    #   context: ../backlink-engine
    #   dockerfile: Dockerfile

    container_name: backlink-engine-app
    restart: unless-stopped

    environment:
      # Database (PostgreSQL partagé, DB 2)
      DB_CONNECTION: pgsql
      DB_HOST: postgres
      DB_PORT: 5432
      DB_DATABASE: backlink_db
      DB_USERNAME: ${POSTGRES_USER:-shared_user}
      DB_PASSWORD: ${POSTGRES_PASSWORD}

      # Redis (namespace 1)
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      REDIS_DB: 1  # ⚠️ Namespace différent de Scraper-Pro

      # App config (adapter selon votre Backlink Engine)
      APP_ENV: production
      APP_DEBUG: "false"
      APP_URL: ${BACKLINK_URL:-http://localhost:8080}

    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    ports:
      - "8080:80"  # ⚠️ Adapter selon votre Backlink Engine

    networks:
      - shared-network

    # Limites RAM (max 800 MB)
    deploy:
      resources:
        limits:
          memory: 800M
          cpus: '0.8'
        reservations:
          memory: 400M
          cpus: '0.5'

    # ⚠️ Volumes à adapter selon votre Backlink Engine
    volumes:
      - ./backlink-engine/storage:/var/www/html/storage
      - ./backlink-engine/config:/var/www/html/config

# ========================================
# SERVICES OPTIONNELS (Commentés par défaut)
# Décommenter si vous upgradez vers 8 GB RAM
# ========================================

  # ==========================================
  # Nginx Reverse Proxy (OPTIONNEL)
  # ==========================================
  # nginx:
  #   image: nginx:alpine
  #   container_name: shared-nginx
  #   restart: unless-stopped
  #
  #   ports:
  #     - "80:80"
  #     - "443:443"
  #
  #   volumes:
  #     - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
  #     - ./nginx/ssl:/etc/nginx/ssl:ro
  #
  #   networks:
  #     - shared-network
  #
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 100M

  # ==========================================
  # Prometheus (OPTIONNEL - Monitoring)
  # ==========================================
  # prometheus:
  #   image: prom/prometheus:latest
  #   container_name: shared-prometheus
  #   restart: unless-stopped
  #
  #   volumes:
  #     - ./monitoring/prometheus:/etc/prometheus
  #
  #   ports:
  #     - "9090:9090"
  #
  #   networks:
  #     - shared-network
  #
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 300M

  # ==========================================
  # Grafana (OPTIONNEL - Dashboards)
  # ==========================================
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: shared-grafana
  #   restart: unless-stopped
  #
  #   ports:
  #     - "3001:3000"
  #
  #   networks:
  #     - shared-network
  #
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 200M
