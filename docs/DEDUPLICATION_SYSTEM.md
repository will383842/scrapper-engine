# ğŸ”’ SYSTÃˆME DE DÃ‰DUPLICATION ULTRA-PROFESSIONNEL

Documentation complÃ¨te du systÃ¨me de dÃ©duplication multicouche de Scraper-Pro.

---

## ğŸ“‹ TABLE DES MATIÃˆRES

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Couches de DÃ©duplication](#couches-de-dÃ©duplication)
4. [Configuration](#configuration)
5. [Utilisation](#utilisation)
6. [Statistiques](#statistiques)
7. [Performance](#performance)
8. [Maintenance](#maintenance)

---

## ğŸ¯ VUE D'ENSEMBLE

Le systÃ¨me de dÃ©duplication de Scraper-Pro garantit qu'**aucune donnÃ©e n'est jamais scrapÃ©e deux fois**, grÃ¢ce Ã  une approche multicouche sophistiquÃ©e.

### Objectifs

- **100% de prÃ©cision**: Aucun faux positif ni faux nÃ©gatif
- **Performance optimale**: Redis en cache primaire, PostgreSQL en fallback
- **FlexibilitÃ©**: Configuration fine par couche
- **Transparence**: Statistiques dÃ©taillÃ©es en temps rÃ©el

### BÃ©nÃ©fices

- **Ã‰conomie de bande passante**: Ã‰vite les requÃªtes HTTP inutiles
- **Ã‰conomie de temps**: Jobs plus rapides
- **Ã‰conomie de coÃ»ts**: Moins de proxies, moins de SerpAPI calls
- **QualitÃ© des donnÃ©es**: Base de contacts unique et propre

---

## ğŸ—ï¸ ARCHITECTURE

### Composants

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Scrapy Pipeline                      â”‚
â”‚                 UltraProDeduplicationPipeline            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DeduplicationManager                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Layer 1: URL Exact Match                         â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  Layer 2: URL Normalized                          â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  Layer 3: Email Deduplication                     â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  Layer 4: Content Hash                            â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  Layer 5: Temporal Deduplication                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Redis   â”‚          â”‚ PostgreSQL â”‚
        â”‚  (Cache)  â”‚          â”‚ (Fallback) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de DonnÃ©es

1. **Item arrive** dans le pipeline
2. **Layer 1-5** vÃ©rifient sÃ©quentiellement
3. **Si duplicate dÃ©tectÃ©**: `DropItem` exception
4. **Si unique**: MarquÃ© comme vu + passÃ© au pipeline suivant

---

## ğŸ›¡ï¸ COUCHES DE DÃ‰DUPLICATION

### Layer 1: URL Exact Match

**Objectif**: DÃ©tecter les URLs identiques (match exact)

**MÃ©thode**:
- Comparaison byte-Ã -byte de l'URL
- Case-sensitive
- Tous les caractÃ¨res comptent

**Exemple**:
```
âœ… DÃ©tecte:
https://example.com/page1 = https://example.com/page1

âŒ Ne dÃ©tecte PAS:
https://example.com/page1 â‰  http://example.com/page1
https://example.com/page1 â‰  https://example.com/page1/
```

### Layer 2: URL Normalized

**Objectif**: DÃ©tecter les URLs sÃ©mantiquement identiques mais syntaxiquement diffÃ©rentes

**Normalisation**:
- `http://` â†’ `https://`
- Suppression de `www.`
- Suppression du trailing slash
- Tri des query parameters
- Suppression des tracking parameters (utm_*, fbclid, gclid, etc.)

**Exemple**:
```
Ces URLs sont considÃ©rÃ©es IDENTIQUES:
http://www.example.com/page1/
https://example.com/page1
https://example.com/page1?utm_source=facebook
â†’ Toutes normalisÃ©es en: https://example.com/page1
```

**Configuration**:
```bash
DEDUP_URL_NORMALIZE=true  # Activer la normalisation
```

### Layer 3: Email Deduplication

**Objectif**: Garantir l'unicitÃ© des emails (un contact = un email)

**MÃ©thode**:
- Email normalisÃ© (lowercase, trim)
- PortÃ©e: globale OU par-job

**Exemple**:
```
âœ… DÃ©tecte:
john.doe@example.com = JOHN.DOE@example.com
```

**Configuration**:
```bash
# Global: un email unique dans TOUTE la base
DEDUP_EMAIL_GLOBAL=true

# Per-job: un email unique par job seulement
DEDUP_EMAIL_GLOBAL=false
```

**Recommandation**: `DEDUP_EMAIL_GLOBAL=true` (production)

### Layer 4: Content Hash

**Objectif**: DÃ©tecter les pages avec le mÃªme contenu mais URLs diffÃ©rentes

**MÃ©thode**:
- SHA256 hash du contenu normalisÃ©
- Normalisation: lowercase, whitespace collapse
- DÃ©tecte: duplications, miroirs, domaines parkÃ©s

**Exemple**:
```
Ces pages ont le MÃŠME contenu:
https://site1.com/contact
https://site2.com/nous-contacter
https://mirror.site1.com/contact

â†’ Hash identique â†’ Duplicate dÃ©tectÃ©
```

**Configuration**:
```bash
DEDUP_CONTENT_HASH_ENABLED=true
```

**Use case**: Ã‰viter de scraper 1000 domaines parkÃ©s identiques

### Layer 5: Temporal Deduplication

**Objectif**: Ne pas re-scraper une URL trop rÃ©cemment

**MÃ©thode**:
- VÃ©rifier la date du dernier scrape
- Si < X jours: skip
- Si > X jours: re-scrape (donnÃ©es peuvent avoir changÃ©)

**Exemple**:
```
URL scrapÃ©e le 2025-01-01
TTL configurÃ©: 30 jours
Aujourd'hui: 2025-02-13

â†’ 43 jours Ã©coulÃ©s â†’ Re-scrape autorisÃ©
```

**Configuration**:
```bash
# TTL en jours (0 = jamais re-scraper)
DEDUP_URL_TTL_DAYS=30
```

**Use cases**:
- `0`: Production (jamais re-scraper)
- `30`: Maintenance mensuelle (refresh data)
- `7`: Monitoring hebdomadaire

---

## âš™ï¸ CONFIGURATION

### Variables d'Environnement

Toutes les variables sont dans `.env.production`:

```bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DEDUPLICATION SETTINGS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# TTL URLs (jours, 0 = jamais re-scraper)
DEDUP_URL_TTL_DAYS=30

# Email global (true = unique dans toute la base)
DEDUP_EMAIL_GLOBAL=true

# Content hash (true = dÃ©tecter pages similaires)
DEDUP_CONTENT_HASH_ENABLED=true

# URL normalization (true = http/https, www, etc.)
DEDUP_URL_NORMALIZE=true
```

### Recommandations par Environnement

#### Development / Testing
```bash
DEDUP_URL_TTL_DAYS=0          # Pas de TTL (test)
DEDUP_EMAIL_GLOBAL=false      # Per-job
DEDUP_CONTENT_HASH_ENABLED=true
DEDUP_URL_NORMALIZE=true
```

#### Production (URLs Only)
```bash
DEDUP_URL_TTL_DAYS=30         # Refresh mensuel
DEDUP_EMAIL_GLOBAL=true       # Global
DEDUP_CONTENT_HASH_ENABLED=true
DEDUP_URL_NORMALIZE=true
```

#### Production (Full Mode)
```bash
DEDUP_URL_TTL_DAYS=0          # Jamais re-scraper
DEDUP_EMAIL_GLOBAL=true       # Global
DEDUP_CONTENT_HASH_ENABLED=true
DEDUP_URL_NORMALIZE=true
```

---

## ğŸ“Š UTILISATION

### Automatique (Scrapy Pipeline)

La dÃ©duplication est **automatique** dans le pipeline Scrapy:

```python
# scraper/settings_production.py

ITEM_PIPELINES = {
    "scraper.utils.pipelines.UltraProDeduplicationPipeline": 50,  # â† DÃ©duplication
    "scraper.utils.pipelines.ValidationPipeline": 200,
    "scraper.utils.pipelines.PostgresPipeline": 300,
    ...
}
```

### Programmatique (API Python)

```python
from scraper.utils.deduplication_pro import DeduplicationManager

# Initialize manager
manager = DeduplicationManager(job_id=123)

# Check URL
url = "https://example.com/page1"
if manager.is_url_seen_exact(url):
    print("URL already scraped!")
else:
    manager.mark_url_seen_exact(url)
    # ... scrape URL

# Check email
email = "john@example.com"
if manager.is_email_seen(email):
    print("Email already exists!")
else:
    manager.mark_email_seen(email)
    # ... store contact

# Get statistics
stats = manager.get_stats()
print(f"Deduplication rate: {stats['deduplication_rate']:.1f}%")
```

### Statistiques en Temps RÃ©el

#### Via Dashboard

1. Aller sur **Dashboard Premium**
2. Tab **"Scraping URLs"**
3. Section **"DÃ©duplication Ultra-Professionnelle"**

MÃ©triques affichÃ©es:
- URLs exactes dÃ©dupliquÃ©es
- URLs normalisÃ©es dÃ©dupliquÃ©es
- Emails uniques
- Contenus uniques
- Taux de dÃ©duplication global

#### Via PostgreSQL

```sql
-- Vue statistiques
SELECT * FROM deduplication_stats;

-- DÃ©tails URLs
SELECT dedup_type, COUNT(*) as count
FROM url_deduplication_cache
GROUP BY dedup_type;

-- DÃ©tails content hash
SELECT COUNT(*) as total,
       COUNT(DISTINCT content_hash) as unique_hashes
FROM content_hash_cache;
```

#### Via Redis CLI

```bash
# Voir les clÃ©s de dÃ©duplication
docker exec scraper-redis redis-cli -a YOUR_PASSWORD KEYS "dedup:*"

# Voir le nombre d'emails uniques
docker exec scraper-redis redis-cli -a YOUR_PASSWORD SCARD "dedup:email:global"

# Voir le nombre d'URLs exactes (job #123)
docker exec scraper-redis redis-cli -a YOUR_PASSWORD SCARD "dedup:url_exact:123"
```

---

## âš¡ PERFORMANCE

### Redis (Cache Primaire)

**Avantages**:
- Latence < 1ms
- AtomicitÃ© (SADD)
- ScalabilitÃ© horizontale

**CapacitÃ©**:
- 1GB RAM = ~10M URLs
- 2GB RAM = ~20M URLs
- 4GB RAM = ~40M URLs

### PostgreSQL (Fallback)

**Avantages**:
- Persistance garantie
- Indexation avancÃ©e
- RequÃªtes complexes

**Performance**:
- Index B-tree sur `url`
- Index B-tree sur `content_hash`
- Contraintes UNIQUE Ã©vitent les doublons

### Benchmarks

| OpÃ©ration | Redis | PostgreSQL |
|-----------|-------|------------|
| Check URL | 0.2ms | 5ms |
| Mark URL seen | 0.3ms | 10ms |
| Check email | 0.2ms | 8ms |
| Content hash | 0.5ms | 15ms |

**Recommandation**: Toujours utiliser Redis en production.

### Optimisations

1. **Batch operations**: Utiliser Redis pipelines
2. **TTL automatique**: Cleanup via Redis EXPIRE
3. **Partitioning**: SÃ©parer par job_id
4. **Indexes**: PostgreSQL pour fallback rapide

---

## ğŸ”§ MAINTENANCE

### Cleanup Manuel

#### Redis

```bash
# Vider TOUTES les clÃ©s de dÃ©duplication (DANGER!)
docker exec scraper-redis redis-cli -a YOUR_PASSWORD DEL $(docker exec scraper-redis redis-cli -a YOUR_PASSWORD KEYS "dedup:*")

# Vider un job spÃ©cifique
docker exec scraper-redis redis-cli -a YOUR_PASSWORD DEL "dedup:url_exact:123"
```

#### PostgreSQL

```sql
-- Cleanup des entrÃ©es expirÃ©es
SELECT cleanup_expired_deduplication_cache();

-- RÃ©sultat:
-- (url_deleted: 1234, content_deleted: 567)

-- Vider TOUT (DANGER!)
TRUNCATE url_deduplication_cache, content_hash_cache CASCADE;

-- Vider un job spÃ©cifique
DELETE FROM url_deduplication_cache WHERE job_id = 123;
DELETE FROM content_hash_cache WHERE job_id = 123;
```

### Cleanup Automatique (Cron)

**Option 1: PostgreSQL Function (recommandÃ©)**

```bash
# Cron job (tous les jours Ã  3h du matin)
crontab -e
```

```cron
0 3 * * * docker exec scraper-postgres psql -U scraper_admin -d scraper_db -c "SELECT cleanup_expired_deduplication_cache();"
```

**Option 2: Script Python**

```python
# scripts/cleanup_deduplication.py

from scraper.database import get_db_session
from sqlalchemy import text

with get_db_session() as session:
    result = session.execute(text("SELECT * FROM cleanup_expired_deduplication_cache()")).fetchone()
    print(f"Cleanup: {result.url_deleted} URLs, {result.content_deleted} content hashes")
```

```bash
# Cron job
0 3 * * * cd /home/scraper/scraper-pro && docker exec scraper-app python scripts/cleanup_deduplication.py
```

### Monitoring

#### Alertes Prometheus

```yaml
# monitoring/prometheus/alerts/deduplication.yml

groups:
  - name: deduplication
    rules:
      - alert: DeduplicationCacheExpired
        expr: deduplication_expired_entries > 10000
        for: 5m
        annotations:
          summary: "Too many expired deduplication entries"
```

#### Grafana Dashboard

MÃ©triques Ã  surveiller:
- Taux de dÃ©duplication (%)
- Nombre d'entrÃ©es dans Redis
- Nombre d'entrÃ©es dans PostgreSQL
- Latence moyenne des checks

---

## ğŸ› TROUBLESHOOTING

### ProblÃ¨me: Taux de dÃ©duplication trop Ã©levÃ© (>80%)

**Cause**: Configuration trop agressive ou bug

**Solution**:
1. VÃ©rifier `DEDUP_URL_TTL_DAYS` (0 = jamais re-scraper)
2. DÃ©sactiver temporairement `DEDUP_CONTENT_HASH_ENABLED`
3. VÃ©rifier les logs: `docker logs scraper-app | grep "deduplicated"`

### ProblÃ¨me: Redis out of memory

**Cause**: Trop d'entrÃ©es en cache

**Solution**:
1. Augmenter la RAM Redis: `maxmemory 2gb` dans `docker-compose.production.yml`
2. Activer TTL: `DEDUP_URL_TTL_DAYS=30`
3. Cleanup manuel: `DEL dedup:*`

### ProblÃ¨me: PostgreSQL lent

**Cause**: Indexes manquants ou table trop grande

**Solution**:
1. VÃ©rifier les indexes: `\d+ url_deduplication_cache`
2. Cleanup: `SELECT cleanup_expired_deduplication_cache();`
3. VACUUM: `VACUUM ANALYZE url_deduplication_cache;`

### ProblÃ¨me: Faux positifs (URLs uniques marquÃ©es comme duplicates)

**Cause**: Normalisation trop agressive

**Solution**:
1. DÃ©sactiver normalisation: `DEDUP_URL_NORMALIZE=false`
2. VÃ©rifier les tracking params dans `deduplication_pro.py`
3. Analyser les logs: `docker logs scraper-app | grep "normalized"`

### ProblÃ¨me: Emails valides rejetÃ©s

**Cause**: Email dÃ©jÃ  dans la base (global)

**Solution**:
1. VÃ©rifier: `SELECT email FROM scraped_contacts WHERE email = 'john@example.com';`
2. Si lÃ©gitime: passer en per-job: `DEDUP_EMAIL_GLOBAL=false`
3. Ou supprimer l'ancien: `DELETE FROM scraped_contacts WHERE email = 'john@example.com';`

---

## ğŸ“š RÃ‰FÃ‰RENCES

### Fichiers ClÃ©s

- **Pipeline**: `scraper/utils/pipelines.py` â†’ `UltraProDeduplicationPipeline`
- **Manager**: `scraper/utils/deduplication_pro.py` â†’ `DeduplicationManager`
- **Settings**: `scraper/settings_production.py`
- **Migration SQL**: `db/migrations/001_add_deduplication_tables.sql`
- **Dashboard**: `dashboard/app_premium.py` (stats visualization)

### Documentation Connexe

- [DEPLOYMENT_PRODUCTION.md](../DEPLOYMENT_PRODUCTION.md) - Guide de dÃ©ploiement
- [README.md](../README.md) - Vue d'ensemble du projet
- [config/scraping_modes.json](../config/scraping_modes.json) - Modes de scraping

---

**FÃ©licitations! Vous maÃ®trisez maintenant le systÃ¨me de dÃ©duplication ultra-professionnel.** ğŸ‰

Pour toute question, consultez les logs ou le dashboard Grafana.
