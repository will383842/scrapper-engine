# üì¶ Guide d'Installation - Scraper-Pro

Guide complet pour installer et configurer **Scraper-Pro** en production.

---

## üìã Pr√©requis Syst√®me

### Mat√©riel minimum

| Ressource | Minimum | Recommand√© |
|-----------|---------|------------|
| **CPU** | 2 cores | 4 cores |
| **RAM** | 4 GB | 8 GB |
| **Disque** | 20 GB SSD | 50 GB SSD |
| **R√©seau** | 10 Mbps | 100 Mbps |

### Logiciels requis

```bash
# Docker Engine 20.10+
docker --version
# Docker version 20.10.21, build baeda1f

# Docker Compose 2.0+
docker-compose --version
# Docker Compose version v2.15.1

# Git
git --version
# git version 2.39.0
```

---

## üöÄ Installation Pas-√†-Pas

### √âtape 1 : Cloner le repository

```bash
# Clone avec SSH (recommand√©)
git clone git@github.com:votre-org/scraper-pro.git
cd scraper-pro

# OU avec HTTPS
git clone https://github.com/votre-org/scraper-pro.git
cd scraper-pro
```

### √âtape 2 : Configuration de l'environnement

```bash
# Copier le template
cp .env.example .env

# √âditer avec votre √©diteur pr√©f√©r√©
nano .env  # ou vim, code, etc.
```

#### 2.1 - Configurer PostgreSQL

```bash
# G√©n√©rer un mot de passe s√©curis√©
POSTGRES_PASSWORD=$(python3 -c "import secrets; print(secrets.token_urlsafe(32))")
echo "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"

# Ajouter dans .env
POSTGRES_DB=scraper_db
POSTGRES_USER=scraper_admin
POSTGRES_PASSWORD=VOTRE_MDP_GENERE
```

#### 2.2 - Configurer Redis

```bash
# G√©n√©rer un mot de passe s√©curis√©
REDIS_PASSWORD=$(python3 -c "import secrets; print(secrets.token_urlsafe(32))")
echo "REDIS_PASSWORD=${REDIS_PASSWORD}"

# Ajouter dans .env
REDIS_PASSWORD=VOTRE_MDP_REDIS_GENERE
```

#### 2.3 - Configurer les Proxies

##### Option A : Oxylabs (Recommand√©)

```bash
# Dans .env
PROXY_PROVIDER=oxylabs
PROXY_USER=customer-YOUR_USERNAME
PROXY_PASS=YOUR_PASSWORD
```

Obtenir vos credentials :
1. Aller sur [https://oxylabs.io](https://oxylabs.io)
2. Dashboard ‚Üí Proxies ‚Üí Credentials
3. Copier Username et Password

##### Option B : BrightData

```bash
PROXY_PROVIDER=brightdata
PROXY_USER=lum-customer-YOUR_CUSTOMER-zone-YOUR_ZONE
PROXY_PASS=YOUR_PASSWORD
```

##### Option C : SmartProxy

```bash
PROXY_PROVIDER=smartproxy
PROXY_USER=YOUR_USERNAME
PROXY_PASS=YOUR_PASSWORD
```

#### 2.4 - Configurer MailWizz

##### SOS-Expat

```bash
# Dans .env
MAILWIZZ_SOS_EXPAT_API_URL=https://mail.sos-expat.com/api
MAILWIZZ_SOS_EXPAT_API_KEY=YOUR_API_KEY
```

Obtenir la cl√© API :
1. Connexion MailWizz SOS-Expat
2. Param√®tres ‚Üí API Keys ‚Üí Create new
3. Copier la cl√© publique

##### Ulixai

```bash
# Dans .env
MAILWIZZ_ULIXAI_API_URL=https://mail.ulixai.com/api
MAILWIZZ_ULIXAI_API_KEY=YOUR_API_KEY
```

#### 2.5 - Configurer les Webhooks

```bash
# G√©n√©rer les secrets HMAC
WEBHOOK_SOS_EXPAT_SECRET=$(python3 -c "import secrets; print(secrets.token_urlsafe(32))")
WEBHOOK_ULIXAI_SECRET=$(python3 -c "import secrets; print(secrets.token_urlsafe(32))")

# Dans .env
WEBHOOK_SOS_EXPAT_URL=https://us-central1-sos-urgently-ac307.cloudfunctions.net/emailEventsWebhook
WEBHOOK_SOS_EXPAT_SECRET=${WEBHOOK_SOS_EXPAT_SECRET}

WEBHOOK_ULIXAI_URL=https://api.ulixai.com/webhooks/email-events
WEBHOOK_ULIXAI_SECRET=${WEBHOOK_ULIXAI_SECRET}
```

‚ö†Ô∏è **Important** : Configurer ces m√™mes secrets dans vos Cloud Functions !

#### 2.6 - Configurer l'authentification API

```bash
# G√©n√©rer un secret HMAC s√©curis√© (64 caract√®res)
API_HMAC_SECRET=$(python3 -c "import secrets; print(secrets.token_urlsafe(48))")
echo "API_HMAC_SECRET=${API_HMAC_SECRET}"

# Ajouter dans .env
API_HMAC_SECRET=VOTRE_SECRET_API_GENERE
```

#### 2.7 - Configurer le Dashboard

```bash
# Choisir un mot de passe admin s√©curis√©
DASHBOARD_PASSWORD=VotreMotDePasseAdmin2024!

# Ajouter dans .env
DASHBOARD_PASSWORD=${DASHBOARD_PASSWORD}
```

#### 2.8 - Configurer SerpAPI (Optionnel mais recommand√©)

Pour contourner les CAPTCHA Google :

```bash
# 1. Cr√©er un compte sur https://serpapi.com
# 2. Dashboard ‚Üí API Key
# 3. Copier la cl√©

# Dans .env
SERPAPI_KEY=YOUR_SERPAPI_KEY_HERE
```

### √âtape 3 : Configurer les fichiers JSON

#### 3.1 - Configuration Proxies

√âditer `config/proxy_config.json` :

```json
{
  "providers": {
    "oxylabs": {
      "endpoint": "pr.oxylabs.io:7777",
      "auth_type": "user_pass",
      "pool_size": 20,
      "type": "datacenter"
    }
  },
  "rotation": {
    "mode": "weighted_random",
    "sticky_session_seconds": 180,
    "max_consecutive_failures": 5,
    "cooldown_minutes": [5, 10, 20, 30],
    "max_cooldowns_before_blacklist": 5
  },
  "rate_limiting": {
    "default_delay_seconds": 2.0,
    "per_domain_limits": {
      "google.com": 5.0,
      "google.fr": 5.0,
      "maps.google.com": 5.0
    }
  }
}
```

#### 3.2 - Configuration MailWizz Routing

V√©rifier `config/mailwizz_routing.json` :

```json
{
  "platforms": {
    "sos-expat": {
      "lists": {
        "avocat": {
          "list_id": 1,
          "list_name": "Avocats Internationaux",
          "auto_tags": ["avocat", "professionnel", "juridique"],
          "template_default": "partenariat_avocat"
        }
        // ... autres cat√©gories
      }
    },
    "ulixai": {
      "lists": {
        "blogueur": {
          "list_id": 1,
          "list_name": "Blogueurs Voyage",
          "auto_tags": ["blogueur", "content_creator", "voyage"],
          "template_default": "affiliation_75pct"
        }
        // ... autres cat√©gories
      }
    }
  }
}
```

‚ö†Ô∏è **Important** : Adapter les `list_id` √† vos vraies listes MailWizz !

### √âtape 4 : Initialiser la base de donn√©es

```bash
# D√©marrer seulement PostgreSQL
docker-compose up -d postgres

# Attendre que PostgreSQL soit pr√™t (10-15 secondes)
sleep 15

# V√©rifier le statut
docker-compose logs postgres | grep "database system is ready"

# Initialiser le sch√©ma
docker-compose exec postgres psql -U scraper_admin -d scraper_db -f /docker-entrypoint-initdb.d/01-init.sql

# Appliquer les migrations
docker-compose exec postgres psql -U scraper_admin -d scraper_db < db/migrations/002_add_checkpoint_resume.sql
docker-compose exec postgres psql -U scraper_admin -d scraper_db < db/migrations/003_add_scraped_articles.sql
docker-compose exec postgres psql -U scraper_admin -d scraper_db < db/migrations/004_add_compound_indexes.sql
```

### √âtape 5 : D√©marrer tous les services

```bash
# D√©marrer en mode d√©tach√©
docker-compose up -d

# V√©rifier que tous les services sont up
docker-compose ps

# R√©sultat attendu :
# NAME                  STATUS              PORTS
# scraper-postgres      Up (healthy)        127.0.0.1:5432->5432/tcp
# scraper-redis         Up (healthy)        127.0.0.1:6379->6379/tcp
# scraper-app           Up                  127.0.0.1:8000->8000/tcp
# scraper-dashboard     Up                  127.0.0.1:8501->8501/tcp
# scraper-prometheus    Up                  127.0.0.1:9090->9090/tcp
# scraper-grafana       Up                  127.0.0.1:3000->3000/tcp
# scraper-loki          Up                  127.0.0.1:3100->3100/tcp
```

### √âtape 6 : V√©rifier l'installation

#### 6.1 - Sant√© de l'API

```bash
curl http://localhost:8000/health

# R√©ponse attendue :
{
  "status": "ok",
  "service": "scraper-pro",
  "postgres": true,
  "redis": true
}
```

#### 6.2 - Acc√®s Dashboard

Ouvrir : http://localhost:8501

Mot de passe : `DASHBOARD_PASSWORD` d√©fini dans `.env`

#### 6.3 - V√©rifier les logs

```bash
# Logs du scraper
docker-compose logs -f scraper

# Logs PostgreSQL
docker-compose logs postgres

# Logs de tous les services
docker-compose logs
```

---

## üîß Configuration Post-Installation

### 1. Configurer les Backups

```bash
# Cr√©er le r√©pertoire de backup
sudo mkdir -p /var/backups/scraper-pro
sudo chown $USER:$USER /var/backups/scraper-pro

# Ajouter le cron de backup (ex√©cut√© tous les jours √† 3h00)
crontab -e

# Ajouter cette ligne :
0 3 * * * /c/Users/willi/Documents/Projets/VS_CODE/scraper-pro/scripts/backup-postgres.sh
```

### 2. Configurer Grafana

```bash
# Acc√©der √† Grafana
open http://localhost:3000

# Login : admin / GRAFANA_PASSWORD (d√©fini dans .env)

# Importer les dashboards :
# 1. Settings ‚Üí Data Sources ‚Üí Add Prometheus (http://prometheus:9090)
# 2. Dashboards ‚Üí Import ‚Üí Upload dashboards/scraper-pro.json
```

### 3. Tester le syst√®me end-to-end

```bash
# Cr√©er un job de test
curl -X POST http://localhost:8000/api/v1/scraping/jobs \
  -H "Content-Type: application/json" \
  -H "X-Timestamp: $(date +%s)" \
  -H "X-Signature: $(echo -n "$(date +%s).{}" | openssl dgst -sha256 -hmac "$(grep API_HMAC_SECRET .env | cut -d= -f2)" | awk '{print $2}')" \
  -d '{
    "source_type": "custom_urls",
    "name": "Test Installation",
    "config": {
      "urls": ["https://example.com"]
    },
    "category": null,
    "platform": null,
    "tags": ["test"],
    "auto_inject_mailwizz": false
  }'

# V√©rifier dans le dashboard que le job est cr√©√©
```

---

## üêõ Troubleshooting Installation

### Probl√®me 1 : PostgreSQL ne d√©marre pas

```bash
# V√©rifier les logs
docker-compose logs postgres

# Erreur commune : "permission denied"
# Solution : R√©initialiser les volumes
docker-compose down -v
docker-compose up -d postgres
```

### Probl√®me 2 : Redis connection refused

```bash
# V√©rifier que Redis tourne
docker-compose ps redis

# Tester la connexion
docker-compose exec redis redis-cli -a "$(grep REDIS_PASSWORD .env | cut -d= -f2)" ping

# R√©sultat attendu : PONG
```

### Probl√®me 3 : API retourne 503 "Database not configured"

```bash
# V√©rifier les variables d'environnement
docker-compose exec scraper env | grep POSTGRES

# Red√©marrer le scraper
docker-compose restart scraper
```

### Probl√®me 4 : Dashboard Streamlit ne charge pas

```bash
# V√©rifier les logs
docker-compose logs dashboard

# V√©rifier la connexion DB
docker-compose exec dashboard python -c "
from sqlalchemy import create_engine
import os
url = f'postgresql://{os.getenv(\"POSTGRES_USER\")}:{os.getenv(\"POSTGRES_PASSWORD\")}@postgres:5432/{os.getenv(\"POSTGRES_DB\")}'
engine = create_engine(url)
conn = engine.connect()
print('‚úì Database connected')
"
```

### Probl√®me 5 : Les proxies ne fonctionnent pas

```bash
# Tester manuellement
docker-compose exec scraper python -c "
import os
import httpx
proxy = f'http://{os.getenv(\"PROXY_USER\")}:{os.getenv(\"PROXY_PASS\")}@pr.oxylabs.io:7777'
try:
    r = httpx.get('http://ip-api.com/json', proxies={'http://': proxy}, timeout=10)
    print('‚úì Proxy works:', r.json()['query'])
except Exception as e:
    print('‚úó Proxy error:', e)
"
```

---

## üìö Prochaines √©tapes

Apr√®s l'installation :

1. üìñ Lire [ARCHITECTURE.md](ARCHITECTURE.md) pour comprendre le syst√®me
2. üîå Lire [API.md](API.md) pour l'utilisation de l'API
3. üöÄ Lire [DEPLOYMENT.md](DEPLOYMENT.md) pour le d√©ploiement production
4. üìä Configurer les dashboards Grafana
5. üîî Configurer les alertes (email/Slack)

---

## ‚úÖ Checklist d'Installation

```
Installation Checklist
======================

Pr√©requis
‚ñ° Docker 20.10+ install√©
‚ñ° Docker Compose 2.0+ install√©
‚ñ° Git install√©
‚ñ° Compte proxy (Oxylabs/BrightData/SmartProxy)
‚ñ° Acc√®s API MailWizz (SOS-Expat + Ulixai)
‚ñ° Compte SerpAPI (optionnel)

Configuration
‚ñ° Repository clon√©
‚ñ° .env cr√©√© et configur√©
‚ñ° POSTGRES_PASSWORD g√©n√©r√© (32+ chars)
‚ñ° REDIS_PASSWORD g√©n√©r√© (32+ chars)
‚ñ° API_HMAC_SECRET g√©n√©r√© (48+ chars)
‚ñ° WEBHOOK secrets g√©n√©r√©s (32+ chars)
‚ñ° Proxies configur√©s dans .env
‚ñ° MailWizz API keys configur√©s
‚ñ° SerpAPI key configur√© (optionnel)
‚ñ° config/proxy_config.json v√©rifi√©
‚ñ° config/mailwizz_routing.json adapt√©

D√©marrage
‚ñ° PostgreSQL d√©marr√© et healthy
‚ñ° Redis d√©marr√© et healthy
‚ñ° Base de donn√©es initialis√©e
‚ñ° Migrations appliqu√©es
‚ñ° Scraper d√©marr√©
‚ñ° Dashboard d√©marr√©
‚ñ° Prometheus d√©marr√©
‚ñ° Grafana d√©marr√©

V√©rification
‚ñ° API /health retourne "ok"
‚ñ° Dashboard accessible (http://localhost:8501)
‚ñ° Grafana accessible (http://localhost:3000)
‚ñ° Backups configur√©s
‚ñ° Job de test cr√©√© et r√©ussi
‚ñ° Logs visibles et propres

Post-Installation
‚ñ° Dashboards Grafana import√©s
‚ñ° Alertes configur√©es
‚ñ° Monitoring v√©rifi√©
‚ñ° Documentation lue
‚ñ° √âquipe form√©e
```

---

**Installation r√©ussie ? üéâ** Passez √† [l'utilisation du syst√®me](../README.md#-utilisation) !
