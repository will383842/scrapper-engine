# ğŸ—ï¸ Architecture Technique - Scraper-Pro

Documentation complÃ¨te de l'architecture du systÃ¨me **Scraper-Pro**.

---

## ğŸ“Š Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SCRAPER-PRO SYSTEM                        â”‚
â”‚                     (Microservices Architecture)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   External   â”‚         â”‚   Frontend   â”‚         â”‚   Backend    â”‚
â”‚   Services   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Services   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Services   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                         â”‚                         â”‚
      â”‚                         â”‚                         â”‚
      â–¼                         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Oxylabs    â”‚         â”‚  Streamlit   â”‚        â”‚   FastAPI     â”‚
â”‚  BrightData â”‚         â”‚  Dashboard   â”‚        â”‚   REST API    â”‚
â”‚  SmartProxy â”‚         â”‚  (Port 8501) â”‚        â”‚  (Port 8000)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     Data Layer              â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚  PostgreSQL 16 (Primary DB) â”‚
                        â”‚  Redis 7 (Cache + Queue)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    Scrapy Engine            â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚  4 Spiders (Google, Maps,   â”‚
                        â”‚  Custom URLs, Blog Content) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Processing Pipeline        â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚  Validation â†’ Categorizationâ”‚
                        â”‚  â†’ Routing â†’ MailWizz Sync  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   External Integrations     â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚  MailWizz API (SOS-Expat,   â”‚
                        â”‚  Ulixai) + Webhooks         â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Composants Principaux

### 1. **Scraper Service** (Container: `scraper-app`)

**RÃ´le** : Orchestrateur principal, API REST, exÃ©cution des spiders

**Technologies** :
- FastAPI 0.115 (Web framework)
- Uvicorn (ASGI server)
- Scrapy 2.11 (Scraping framework)
- SQLAlchemy 2.0 (ORM)
- Redis-py 5.2 (Cache client)

**Ports** :
- `8000` : API REST

**Endpoints principaux** :
```
GET  /health                              # Health check
POST /api/v1/scraping/jobs                # CrÃ©er un job
POST /api/v1/scraping/jobs/{id}/resume    # Reprendre un job
GET  /api/v1/scraping/jobs/{id}/status    # Statut du job
POST /api/v1/scraping/jobs/{id}/pause     # Pause un job
POST /api/v1/scraping/jobs/{id}/cancel    # Annuler un job
GET  /api/v1/contacts                     # Lister les contacts
POST /api/v1/whois/lookup                 # Lookup WHOIS
```

**Processus** :
```
1. API FastAPI (port 8000)
2. Cron daemon (process_contacts + sync_to_mailwizz)
3. Scrapy spiders (subprocess via runner.py)
```

---

### 2. **PostgreSQL 16** (Container: `scraper-postgres`)

**RÃ´le** : Base de donnÃ©es relationnelle principale

**SchÃ©ma** : 9 tables

| Table | RÃ´le | Taille estimÃ©e |
|-------|------|----------------|
| `scraping_jobs` | Jobs de scraping | ~1K rows |
| `scraped_contacts` | Contacts bruts | ~1M rows |
| `validated_contacts` | Contacts validÃ©s | ~500K rows |
| `mailwizz_sync_log` | Logs sync MailWizz | ~500K rows |
| `proxy_stats` | Stats proxies | ~100 rows |
| `url_fingerprints` | Cache anti-doublons | ~2M rows |
| `email_domain_blacklist` | Domaines blacklistÃ©s | ~1K rows |
| `error_logs` | Logs d'erreurs | ~10K rows |
| `whois_cache` | Cache WHOIS | ~100K rows |
| `scraped_articles` | Articles de blog | ~50K rows |

**Indexes** : 18 indexes (simples + composÃ©s)

**Connection Pool** :
- Pool size: 10
- Max overflow: 20
- Pre-ping: enabled

**Volumes** :
- `postgres_data:/var/lib/postgresql/data`

---

### 3. **Redis 7** (Container: `scraper-redis`)

**RÃ´le** : Cache, dÃ©duplication, queues

**Utilisation** :

```python
# Cache dÃ©duplication (par job)
scraper:seen_emails:{job_id} â†’ SET [emails]

# Cache MX DNS lookup
scraper:mx_cache:{domain} â†’ BOOLEAN

# Cache WHOIS (optionnel)
scraper:whois:{domain} â†’ JSON
```

**Configuration** :
- Max memory: 512 MB (configurable)
- Eviction policy: `allkeys-lru`
- Persistence: RDB (snapshot)

---

### 4. **Scrapy Engine**

**Architecture** :

```
ScrapyEngine
â”œâ”€â”€ Spiders (4)
â”‚   â”œâ”€â”€ google_search_spider.py
â”‚   â”œâ”€â”€ google_maps_spider.py
â”‚   â”œâ”€â”€ generic_url_spider.py
â”‚   â””â”€â”€ blog_content_spider.py
â”œâ”€â”€ Middlewares
â”‚   â”œâ”€â”€ RandomUserAgentMiddleware
â”‚   â””â”€â”€ ProxyMiddleware
â”œâ”€â”€ Pipelines
â”‚   â”œâ”€â”€ DeduplicationPipeline (Redis)
â”‚   â”œâ”€â”€ ValidationPipeline
â”‚   â”œâ”€â”€ PostgresPipeline
â”‚   â”œâ”€â”€ ArticlePipeline
â”‚   â””â”€â”€ ProgressTrackingPipeline
â””â”€â”€ Items
    â”œâ”€â”€ ContactItem
    â””â”€â”€ ArticleItem
```

**Flow d'un spider** :

```
1. start_requests()
   â”œâ”€ Charger checkpoint si resume=true
   â”œâ”€ GÃ©nÃ©rer URLs Ã  scraper
   â””â”€ yield scrapy.Request()

2. parse() / parse_detail()
   â”œâ”€ Extraire donnÃ©es (XPath/CSS selectors)
   â”œâ”€ Yield ContactItem ou ArticleItem
   â””â”€ Sauvegarder checkpoint

3. Pipelines
   â”œâ”€ DeduplicationPipeline : Drop si duplicate
   â”œâ”€ ValidationPipeline : Drop si invalide
   â”œâ”€ PostgresPipeline : Insert dans DB
   â””â”€ ProgressTrackingPipeline : Update job stats

4. Fin
   â””â”€ Update job status (completed/failed)
```

---

### 5. **Dashboard Streamlit** (Container: `scraper-dashboard`)

**RÃ´le** : Interface d'administration web

**Pages** :

```
Dashboard (app.py)
â”œâ”€â”€ Tab 1: Jobs
â”‚   â”œâ”€â”€ Liste des jobs (statut, progrÃ¨s)
â”‚   â”œâ”€â”€ CrÃ©er un nouveau job
â”‚   â””â”€â”€ Actions (resume, pause, cancel)
â”œâ”€â”€ Tab 2: Contacts
â”‚   â”œâ”€â”€ Stats (scraped, validated, sent)
â”‚   â”œâ”€â”€ Recherche et filtres
â”‚   â”œâ”€â”€ DÃ©tails contact
â”‚   â””â”€â”€ Export CSV
â”œâ”€â”€ Tab 3: Articles
â”‚   â”œâ”€â”€ Liste des articles
â”‚   â”œâ”€â”€ Filtres (domain, langue)
â”‚   â”œâ”€â”€ DÃ©tails article
â”‚   â””â”€â”€ Export CSV/JSON
â”œâ”€â”€ Tab 4: Stats
â”‚   â”œâ”€â”€ Volume scraping (30j)
â”‚   â”œâ”€â”€ Sync MailWizz (30j)
â”‚   â”œâ”€â”€ Domain blacklist
â”‚   â””â”€â”€ WHOIS stats
â”œâ”€â”€ Tab 5: WHOIS Lookup
â”‚   â”œâ”€â”€ Recherche manuelle
â”‚   â””â”€ Historique lookups
â””â”€â”€ Tab 6: Configuration
    â”œâ”€â”€ System health
    â”œâ”€â”€ Proxy provider
    â””â”€â”€ MailWizz routing
```

**Authentification** :
- Password-based (HMAC compare)
- Session state Streamlit

**Port** : `8501`

---

### 6. **Monitoring Stack**

#### Prometheus (Container: `scraper-prometheus`)

**RÃ´le** : Collecte et stockage des mÃ©triques

**MÃ©triques exposÃ©es** :

```python
# FastAPI mÃ©triques (via prometheus-client)
scraper_requests_total                    # Total requests
scraper_requests_duration_seconds         # Request latency
scraper_jobs_running                      # Jobs en cours
scraper_jobs_completed_total              # Jobs terminÃ©s
scraper_contacts_scraped_total            # Contacts scrapÃ©s
scraper_contacts_validated_total          # Contacts validÃ©s
scraper_mailwizz_sync_success_total       # Sync rÃ©ussis
scraper_mailwizz_sync_failed_total        # Sync Ã©chouÃ©s
scraper_proxy_requests_total              # RequÃªtes proxy
scraper_proxy_failures_total              # Ã‰checs proxy
```

**Scrape config** :
```yaml
scrape_configs:
  - job_name: 'scraper-api'
    scrape_interval: 15s
    static_configs:
      - targets: ['scraper:8000']
```

**Port** : `9090`

#### Grafana (Container: `scraper-grafana`)

**RÃ´le** : Visualisation des mÃ©triques

**Dashboards** :
1. **Scraper Overview** : Vue d'ensemble du systÃ¨me
2. **Jobs Monitoring** : Suivi des jobs de scraping
3. **MailWizz Sync** : Performance de la sync
4. **Proxies Health** : SantÃ© des proxies
5. **Database Performance** : PostgreSQL metrics

**Port** : `3000`

#### Loki + Promtail (Containers: `scraper-loki`, `scraper-promtail`)

**RÃ´le** : Centralisation des logs

**Sources** :
- `/app/logs/*.log` (scraper)
- Docker container logs (stdout/stderr)

**Port** : `3100`

---

## ğŸ”„ Pipeline de traitement

### Phase 1 : Scraping

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Request â”‚
â”‚  (create    â”‚
â”‚   job)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Insert into â”‚
â”‚ scraping_   â”‚
â”‚ jobs        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ run_spider()â”‚
â”‚ (subprocess)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€> Spider start_requests()
       â”‚    â”‚
       â”‚    â”œâ”€â”€> Proxy rotation
       â”‚    â”œâ”€â”€> User-agent rotation
       â”‚    â””â”€â”€> Rate limiting
       â”‚
       â”œâ”€â”€> Parse pages
       â”‚    â”‚
       â”‚    â”œâ”€â”€> Extract emails
       â”‚    â”œâ”€â”€> Extract phones
       â”‚    â”œâ”€â”€> Extract social media
       â”‚    â””â”€â”€> Extract WHOIS
       â”‚
       â”œâ”€â”€> Pipelines
       â”‚    â”‚
       â”‚    â”œâ”€â”€> DeduplicationPipeline
       â”‚    â”œâ”€â”€> ValidationPipeline
       â”‚    â””â”€â”€> PostgresPipeline
       â”‚
       â””â”€â”€> Insert into scraped_contacts
            (status: pending_validation)
```

### Phase 2 : Validation (Cron job - toutes les heures)

```
process_contacts.py
â”‚
â”œâ”€â”€> SELECT * FROM scraped_contacts
â”‚    WHERE status = 'pending_validation'
â”‚    LIMIT 1000 FOR UPDATE SKIP LOCKED
â”‚
â”œâ”€â”€> Pour chaque contact :
â”‚    â”‚
â”‚    â”œâ”€â”€> validate_email(email)
â”‚    â”‚    â”œâ”€ Regex check
â”‚    â”‚    â”œâ”€ MX DNS lookup (cached)
â”‚    â”‚    â”œâ”€ Blacklist prefixes
â”‚    â”‚    â””â”€ Disposable domains check
â”‚    â”‚
â”‚    â”œâ”€â”€> Check duplicate in validated_contacts
â”‚    â”‚
â”‚    â”œâ”€â”€> Check domain blacklist (bounce rate > 10%)
â”‚    â”‚
â”‚    â”œâ”€â”€> categorize(contact)
â”‚    â”‚    â”œâ”€ Keywords scoring
â”‚    â”‚    â”œâ”€ Source type scoring
â”‚    â”‚    â””â”€ Return best category
â”‚    â”‚
â”‚    â”œâ”€â”€> determine_platform(category)
â”‚    â”‚    â”œâ”€ SOS_EXPAT_CATEGORIES â†’ "sos-expat"
â”‚    â”‚    â””â”€ ULIXAI_CATEGORIES â†’ "ulixai"
â”‚    â”‚
â”‚    â”œâ”€â”€> get_routing_info(category, platform)
â”‚    â”‚    â””â”€ Return {list_id, template, tags}
â”‚    â”‚
â”‚    â””â”€â”€> INSERT INTO validated_contacts
â”‚         (status: ready_for_mailwizz)
â”‚
â””â”€â”€> UPDATE scraped_contacts
     SET status = 'validated'
```

### Phase 3 : Sync MailWizz (Cron job - toutes les heures, +30min)

```
sync_to_mailwizz.py
â”‚
â”œâ”€â”€> Check warmup quota (optionnel)
â”‚    â””â”€ get_daily_quota_remaining()
â”‚
â”œâ”€â”€> SELECT * FROM validated_contacts
â”‚    WHERE status = 'ready_for_mailwizz'
â”‚    AND retry_count < 3
â”‚    LIMIT min(100, quota_remaining)
â”‚    FOR UPDATE SKIP LOCKED
â”‚
â”œâ”€â”€> Pour chaque contact :
â”‚    â”‚
â”‚    â”œâ”€â”€> Build subscriber_data
â”‚    â”‚    â”œâ”€ EMAIL, FNAME, LNAME
â”‚    â”‚    â”œâ”€ COUNTRY, PHONE, WEBSITE
â”‚    â”‚    â”œâ”€ CATEGORY, SOURCE
â”‚    â”‚    â””â”€ Social media fields
â”‚    â”‚
â”‚    â”œâ”€â”€> client.add_subscriber(list_id, data, tags)
â”‚    â”‚    â”‚
â”‚    â”‚    â”œâ”€ HTTP POST to MailWizz API
â”‚    â”‚    â””â”€ Return {success, subscriber_uid, error}
â”‚    â”‚
â”‚    â”œâ”€â”€> If success :
â”‚    â”‚    â”œâ”€ UPDATE validated_contacts
â”‚    â”‚    â”‚   SET status = 'sent_to_mailwizz',
â”‚    â”‚    â”‚       mailwizz_subscriber_id = uid
â”‚    â”‚    â””â”€ INSERT INTO mailwizz_sync_log
â”‚    â”‚        (status: success)
â”‚    â”‚
â”‚    â””â”€â”€> If error :
â”‚         â”œâ”€ UPDATE validated_contacts
â”‚         â”‚   SET retry_count = retry_count + 1
â”‚         â”‚   SET last_error = error_msg
â”‚         â”‚   SET status = 'failed' IF retry_count >= 3
â”‚         â””â”€ INSERT INTO mailwizz_sync_log
â”‚             (status: failed)
â”‚
â””â”€â”€> Return stats {success, failed, retries}
```

---

## ğŸ” SÃ©curitÃ©

### Authentification API (HMAC-SHA256)

```python
# Client gÃ©nÃ¨re la signature
timestamp = str(int(time.time()))
body = json.dumps(payload)
message = f"{timestamp}.{body}"
signature = hmac.sha256(API_HMAC_SECRET, message).hexdigest()

# Headers
X-Timestamp: 1707832800
X-Signature: a1b2c3d4e5f6...

# Serveur vÃ©rifie
if abs(time.time() - int(timestamp)) > 300:
    return 401  # Expired (5 min TTL)

expected_sig = hmac.sha256(API_HMAC_SECRET, message).hexdigest()
if not hmac.compare_digest(signature, expected_sig):
    return 401  # Invalid signature
```

### Network Isolation

```yaml
# docker-compose.yml
networks:
  scraper-network:
    driver: bridge

# Tous les services sur ce rÃ©seau privÃ©
# Seuls ports exposÃ©s : 127.0.0.1:XXXX (localhost uniquement)
```

### Secrets Management

```bash
# .env (gitignored)
API_HMAC_SECRET=...
POSTGRES_PASSWORD=...
REDIS_PASSWORD=...
MAILWIZZ_*_API_KEY=...
WEBHOOK_*_SECRET=...

# Rotation recommandÃ©e : 90 jours
```

---

## ğŸ“ˆ Performance & ScalabilitÃ©

### Optimisations Database

```sql
-- Indexes composÃ©s pour les requÃªtes frÃ©quentes
CREATE INDEX idx_scraped_status_scraped_at
    ON scraped_contacts(status, scraped_at ASC);

CREATE INDEX idx_validated_status_retry_created
    ON validated_contacts(status, retry_count, created_at ASC);

-- Connection pooling
pool_size=10, max_overflow=20, pool_pre_ping=True
```

### Rate Limiting

```python
# config/proxy_config.json
{
  "rate_limiting": {
    "default_delay_seconds": 2.0,
    "per_domain_limits": {
      "google.com": 5.0,      # 5s entre chaque requÃªte Google
      "maps.google.com": 5.0
    }
  }
}

# settings.py (Scrapy)
CONCURRENT_REQUESTS = 8
CONCURRENT_REQUESTS_PER_DOMAIN = 2
DOWNLOAD_DELAY = 2.0
AUTOTHROTTLE_ENABLED = True
```

### Cache Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MX Cache   â”‚  TTL: 7 days (emails domaines)
â”‚ Email Seen â”‚  TTL: 24h (dÃ©dup par job)
â”‚ WHOIS      â”‚  TTL: 30 days (domaines)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ URL FP     â”‚  Permanent (dÃ©dup global)
â”‚ WHOIS      â”‚  Permanent (cache longue durÃ©e)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Horizontal Scaling

**Actuellement** : Monolithe (1 scraper container)

**Pour scaler** :
```yaml
# docker-compose.yml
services:
  scraper:
    deploy:
      replicas: 3  # 3 instances de scraper-app

  # Load balancer (Nginx)
  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
```

**Limites** :
- PostgreSQL : Max 100 connections (pool_size * replicas < 100)
- Redis : Non limitant (milliers de connections)
- Proxies : LimitÃ© par quota provider

---

## ğŸ”„ Flux de donnÃ©es

### Diagramme de sÃ©quence - CrÃ©ation d'un job

```
User/API â†’ FastAPI â†’ Database â†’ ScrapyEngine â†’ Proxy â†’ Google â†’ Database â†’ MailWizz

1. POST /api/v1/scraping/jobs
   â”œâ”€ FastAPI valide payload
   â”œâ”€ Insert dans scraping_jobs (status: pending)
   â””â”€ run_spider(job_id, source_type, config)

2. run_spider() lance subprocess
   â”œâ”€ scrapy crawl google_search -a job_id=123
   â”œâ”€ Spider.start_requests()
   â”‚   â”œâ”€ Load checkpoint si resume=true
   â”‚   â””â”€ Generate Google URLs
   â”œâ”€ Middleware : Proxy rotation
   â”œâ”€ Middleware : User-agent rotation
   â”œâ”€ Parse Google results
   â”‚   â”œâ”€ Extract result URLs
   â”‚   â””â”€ Follow to detail pages
   â”œâ”€ Parse detail pages
   â”‚   â”œâ”€ Extract email/phone/social
   â”‚   â””â”€ Yield ContactItem
   â””â”€ Pipelines
       â”œâ”€ DeduplicationPipeline (Redis check)
       â”œâ”€ ValidationPipeline (format check)
       â”œâ”€ PostgresPipeline (INSERT scraped_contacts)
       â””â”€ ProgressTrackingPipeline (UPDATE job stats)

3. Cron: process_contacts (toutes les heures)
   â”œâ”€ Fetch pending_validation
   â”œâ”€ validate_email() + categorize()
   â””â”€ INSERT validated_contacts

4. Cron: sync_to_mailwizz (+30min)
   â”œâ”€ Fetch ready_for_mailwizz
   â”œâ”€ POST to MailWizz API
   â””â”€ UPDATE status sent_to_mailwizz
```

---

## ğŸ“Š SchÃ©ma de base de donnÃ©es

Voir `db/init.sql` pour le schÃ©ma complet.

**Relations** :

```
scraping_jobs (1) â”€â”€< (N) scraped_contacts
                            â”‚
                            â”‚ (1:1)
                            â–¼
                      validated_contacts (1) â”€â”€< (N) mailwizz_sync_log
```

---

## ğŸ§ª Testing Strategy

```
tests/
â”œâ”€â”€ test_api.py              # API endpoints
â”œâ”€â”€ test_validator.py        # Email/phone validation
â”œâ”€â”€ test_categorizer.py      # CatÃ©gorisation
â”œâ”€â”€ test_mailwizz_client.py  # Client MailWizz
â”œâ”€â”€ test_spiders.py          # Spiders Scrapy
â”œâ”€â”€ test_pipelines.py        # Pipelines
â”œâ”€â”€ test_proxy_manager.py    # Proxy rotation
â”œâ”€â”€ test_checkpoint.py       # Checkpoint/resume
â””â”€â”€ ... (8 autres fichiers)

# Run tests
docker-compose exec scraper pytest --cov=scraper --cov-report=html
```

---

## ğŸš€ DÃ©ploiement

Voir [DEPLOYMENT.md](DEPLOYMENT.md) pour :
- StratÃ©gie de dÃ©ploiement
- Configuration production
- Scaling horizontal
- Disaster recovery

---

**Besoin de clarifications ?** Consultez le code source ou contactez l'Ã©quipe technique.
