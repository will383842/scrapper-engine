# ========================================
# DOCKER COMPOSE - SCRAPER-PRO UNIQUEMENT
# ========================================
# Configuration optimisée pour serveur 2 vCPU / 4 GB RAM
# Application : Scraper-Pro seulement (sans Backlink Engine)
# ========================================

version: '3.8'

# ========================================
# RÉSEAUX
# ========================================
networks:
  scraper-network:
    driver: bridge
    name: scraper-network

# ========================================
# VOLUMES
# ========================================
volumes:
  postgres_data:
    name: scraper-postgres-data
  redis_data:
    name: scraper-redis-data

# ========================================
# SERVICES
# ========================================
services:
  # ==========================================
  # PostgreSQL (Scraper-Pro uniquement)
  # ==========================================
  postgres:
    image: postgres:16-alpine
    container_name: scraper-postgres
    restart: unless-stopped

    environment:
      POSTGRES_DB: scraper_db
      POSTGRES_USER: ${POSTGRES_USER:-scraper_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}

    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db/migrations:/docker-entrypoint-initdb.d:ro

    ports:
      - "5432:5432"

    networks:
      - scraper-network

    # Limites RAM (max 400 MB pour 1 seule app)
    deploy:
      resources:
        limits:
          memory: 400M
        reservations:
          memory: 150M

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-scraper_user}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================
  # Redis (Scraper-Pro uniquement)
  # ==========================================
  redis:
    image: redis:7-alpine
    container_name: scraper-redis
    restart: unless-stopped

    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 100mb
      --maxmemory-policy allkeys-lru
      --save 60 1000

    volumes:
      - redis_data:/data

    ports:
      - "6379:6379"

    networks:
      - scraper-network

    # Limites RAM (max 100 MB)
    deploy:
      resources:
        limits:
          memory: 100M
        reservations:
          memory: 50M

    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # ==========================================
  # Scraper API (FastAPI)
  # ==========================================
  scraper-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraper-api
    restart: unless-stopped

    environment:
      # Database
      DATABASE_URL: postgresql://${POSTGRES_USER:-scraper_user}:${POSTGRES_PASSWORD}@postgres:5432/scraper_db

      # Redis
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0

      # API
      API_HOST: 0.0.0.0
      API_PORT: 8000
      API_HMAC_SECRET: ${API_HMAC_SECRET}

      # Scraping (OPTIMISÉ pour 2 vCPU)
      PROXY_ENABLED: "false"
      DOWNLOAD_DELAY: "2.5"
      CONCURRENT_REQUESTS: "3"
      CONCURRENT_REQUESTS_PER_DOMAIN: "1"

      # Throttling intelligent
      AUTOTHROTTLE_ENABLED: "true"
      AUTOTHROTTLE_START_DELAY: "2"
      AUTOTHROTTLE_MAX_DELAY: "30"
      SMART_THROTTLE_MIN_DELAY: "1.0"
      SMART_THROTTLE_MAX_DELAY: "60.0"

      # Logs
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    ports:
      - "8000:8000"

    networks:
      - scraper-network

    # Limites RAM (max 500 MB)
    deploy:
      resources:
        limits:
          memory: 500M
          cpus: '0.8'
        reservations:
          memory: 250M
          cpus: '0.5'

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ==========================================
  # Scraper Worker (Scrapy)
  # ==========================================
  scraper-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraper-worker
    restart: unless-stopped

    command: ["python", "-m", "scraper.worker"]

    environment:
      # Database
      DATABASE_URL: postgresql://${POSTGRES_USER:-scraper_user}:${POSTGRES_PASSWORD}@postgres:5432/scraper_db

      # Redis
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0

      # Scraping (OPTIMISÉ pour 2 vCPU)
      PROXY_ENABLED: "false"
      DOWNLOAD_DELAY: "2.5"
      CONCURRENT_REQUESTS: "3"
      CONCURRENT_REQUESTS_PER_DOMAIN: "1"

      # Throttling
      AUTOTHROTTLE_ENABLED: "true"
      SMART_THROTTLE_MIN_DELAY: "1.0"
      SMART_THROTTLE_MAX_DELAY: "60.0"

      LOG_LEVEL: ${LOG_LEVEL:-INFO}

    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    networks:
      - scraper-network

    # Limites RAM (max 800 MB - composant principal)
    deploy:
      resources:
        limits:
          memory: 800M
          cpus: '1.0'
        reservations:
          memory: 450M
          cpus: '0.7'

  # ==========================================
  # Dashboard Streamlit
  # ==========================================
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: scraper-dashboard
    restart: unless-stopped

    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-scraper_user}:${POSTGRES_PASSWORD}@postgres:5432/scraper_db
      DASHBOARD_PASSWORD: ${DASHBOARD_PASSWORD}

    depends_on:
      postgres:
        condition: service_healthy

    ports:
      - "8501:8501"

    networks:
      - scraper-network

    # Limites RAM (max 400 MB)
    deploy:
      resources:
        limits:
          memory: 400M
          cpus: '0.5'
        reservations:
          memory: 200M
          cpus: '0.3'

# ========================================
# SERVICES OPTIONNELS (Commentés par défaut)
# Décommenter si vous upgradez vers 8 GB RAM
# ========================================

  # ==========================================
  # Prometheus (OPTIONNEL - Monitoring)
  # ==========================================
  # prometheus:
  #   image: prom/prometheus:latest
  #   container_name: scraper-prometheus
  #   restart: unless-stopped
  #
  #   volumes:
  #     - ./monitoring/prometheus:/etc/prometheus
  #     - prometheus_data:/prometheus
  #
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'
  #     - '--storage.tsdb.retention.time=15d'
  #
  #   ports:
  #     - "9090:9090"
  #
  #   networks:
  #     - scraper-network
  #
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 300M

  # ==========================================
  # Grafana (OPTIONNEL - Dashboards)
  # ==========================================
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: scraper-grafana
  #   restart: unless-stopped
  #
  #   environment:
  #     - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
  #     - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
  #
  #   volumes:
  #     - ./monitoring/grafana:/etc/grafana/provisioning
  #     - grafana_data:/var/lib/grafana
  #
  #   ports:
  #     - "3001:3000"
  #
  #   depends_on:
  #     - prometheus
  #
  #   networks:
  #     - scraper-network
  #
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 200M

# ========================================
# RÉSUMÉ RAM
# ========================================
# PostgreSQL    : 400 MB max
# Redis         : 100 MB max
# API           : 500 MB max
# Worker        : 800 MB max
# Dashboard     : 400 MB max
# Système (OS)  : ~500 MB
# ---------------------------------
# TOTAL         : ~2.7 GB / 4 GB
# MARGE         : ~1.3 GB (32%)
# ========================================
